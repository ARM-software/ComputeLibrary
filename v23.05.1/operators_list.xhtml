<!-- HTML header for doxygen 1.8.15-->
<!-- Remember to use version doxygen 1.8.15 +-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="robots" content="NOINDEX, NOFOLLOW" /> <!-- Prevent indexing by search engines -->
<title>Compute Library: Supported Operators</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <img alt="Compute Library" src="https://raw.githubusercontent.com/ARM-software/ComputeLibrary/gh-pages/ACL_logo.png" style="max-width: 100%;margin-top: 15px;margin-left: 10px"/>
  <td style="padding-left: 0.5em;">
   <div id="projectname">
   &#160;<span id="projectnumber">23.05.1</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('operators_list.xhtml',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="PageDoc"><div class="header">
  <div class="headertitle">
<div class="title">Supported Operators </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#S9_1_operators_list">Supported Operators</a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="S9_1_operators_list"></a>
Supported Operators</h1>
<p>Compute Library supports operators that are listed in below table.</p>
<p>Compute Library supports a wide list of data-types, information can been directly found in the documentation of each kernel/function. The main data-types that the Machine Learning functions support are the following: </p><ul>
<li>
BFLOAT16: 16-bit non-standard brain floating point </li>
<li>
QASYMM8: 8-bit unsigned asymmetric quantized </li>
<li>
QASYMM8_SIGNED: 8-bit signed asymmetric quantized </li>
<li>
QSYMM8_PER_CHANNEL: 8-bit signed symmetric quantized (Used for the weights) </li>
<li>
QSYMM8: 8-bit unsigned symmetric quantized </li>
<li>
QSYMM16: 16-bit unsigned symmetric quantized </li>
<li>
F32: 32-bit single precision floating point </li>
<li>
F16: 16-bit half precision floating point </li>
<li>
S32: 32-bit signed integer </li>
<li>
U8: 8-bit unsigned char </li>
<li>
All: Agnostic to any specific data type </li>
</ul>
<p>Compute Library supports the following data layouts (fast changing dimension from right to left): </p><ul>
<li>
NHWC: The native layout of Compute Library that delivers the best performance where channels are in the fastest changing dimension </li>
<li>
NCHW: Legacy layout where width is in the fastest changing dimension </li>
<li>
NDHWC: New data layout for supporting 3D operators </li>
<li>
All: Agnostic to any specific data layout </li>
</ul>
<p>where N = batches, C = channels, H = height, W = width, D = depth</p>
<a class="anchor" id="multi_row"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>Function </th><th>Description </th><th>Equivalent Android NNAPI Op </th><th>Backends </th><th>Data Layouts </th><th>Data Types </th></tr>
<tr>
<td rowspan="2">ActivationLayer </td><td rowspan="2" style="width:200px;">Function to simulate an activation layer with the specified activation function. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_ELU </li>
<li>
ANEURALNETWORKS_HARD_SWISH </li>
<li>
ANEURALNETWORKS_LOGISTIC </li>
<li>
ANEURALNETWORKS_RELU </li>
<li>
ANEURALNETWORKS_RELU1 </li>
<li>
ANEURALNETWORKS_RELU6 </li>
<li>
ANEURALNETWORKS_TANH </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_activation_layer.xhtml" title="Basic function to run cpu::kernels::CpuActivationKernel.">NEActivationLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_activation_layer.xhtml" title="Basic function to run opencl::kernels::ClActivationKernel.">CLActivationLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ArgMinMaxLayer </td><td rowspan="2" style="width:200px;">Function to calculate the index of the minimum or maximum values in a tensor based on an axis. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_ARGMAX </li>
<li>
ANEURALNETWORKS_ARGMIN </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_arg_min_max_layer.xhtml" title="Function to calculate the index of the minimum or maximum values in a tensor based on an axis.">NEArgMinMaxLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>U32, S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>U32, S32 </td></tr>
<tr>
<td>S32</td><td>U32, S32 </td></tr>
<tr>
<td>F16</td><td>U32, S32 </td></tr>
<tr>
<td>F32</td><td>U32, S32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_arg_min_max_layer.xhtml" title="Function to calculate the index of the minimum or maximum values in a tensor based on an axis.">CLArgMinMaxLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>U32, S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>U32, S32 </td></tr>
<tr>
<td>S32</td><td>U32, S32 </td></tr>
<tr>
<td>F16</td><td>U32, S32 </td></tr>
<tr>
<td>F32</td><td>U32, S32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">ArithmeticAddition </td><td rowspan="1" style="width:200px;">Function to add 2 tensors. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_ADD </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_arithmetic_addition.xhtml" title="Basic function to run cpu::kernels::CpuAddKernel.">NEArithmeticAddition</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>S32 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">ArithmeticSubtraction </td><td rowspan="1" style="width:200px;">Function to substract 2 tensors. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_SUB </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_arithmetic_subtraction.xhtml" title="Basic function to run cpu::kernels::CpuSubKernel.">NEArithmeticSubtraction</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>S32 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BatchNormalizationLayer </td><td rowspan="2" style="width:200px;">Function to perform batch normalization. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_batch_normalization_layer.xhtml" title="Basic function to run NENormalizationLayerKernel and simulate a batch normalization layer.">NEBatchNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_batch_normalization_layer.xhtml" title="Basic function to run CLNormalizationLayerKernel and simulate a batch normalization layer.">CLBatchNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BatchToSpaceLayer </td><td rowspan="2" style="width:200px;">Batch to space transformation. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_BATCH_TO_SPACE_ND </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_batch_to_space_layer.xhtml" title="Basic function to run NEBatchToSpaceLayerKernel.">NEBatchToSpaceLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>All</td><td>s32</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_batch_to_space_layer.xhtml" title="Basic function to run CLBatchToSpaceLayerKernel.">CLBatchToSpaceLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>All</td><td>s32</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BitwiseAnd </td><td rowspan="2" style="width:200px;">Function to perform bitwise AND between 2 tensors. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_LOGICAL_AND </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_bitwise_and.xhtml" title="Basic function to run NEBitwiseAndKernel.">NEBitwiseAnd</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_bitwise_and.xhtml" title="Basic function to perform bitwise AND by running CLBitwiseKernel.">CLBitwiseAnd</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BitwiseNot </td><td rowspan="2" style="width:200px;">Function to perform bitwise NOT. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_LOGICAL_NOT </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_bitwise_not.xhtml" title="Basic function to run NEBitwiseNotKernel.">NEBitwiseNot</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_bitwise_not.xhtml" title="Basic function to perform bitwise NOT by running CLBitwiseKernel.">CLBitwiseNot</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BitwiseOr </td><td rowspan="2" style="width:200px;">Function to perform bitwise OR between 2 tensors. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_LOGICAL_OR </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_bitwise_or.xhtml" title="Basic function to run NEBitwiseOrKernel.">NEBitwiseOr</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_bitwise_or.xhtml" title="Basic function to perform bitwise OR by running CLBitwiseKernel.">CLBitwiseOr</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BitwiseXor </td><td rowspan="2" style="width:200px;">Function to perform bitwise XOR between 2 tensors. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_bitwise_xor.xhtml" title="Basic function to run NEBitwiseXorKernel.">NEBitwiseXor</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_bitwise_xor.xhtml" title="Basic function to perform bitwise XOR by running CLBitwiseKernel.">CLBitwiseXor</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BoundingBoxTransform </td><td rowspan="2" style="width:200px;">Transform proposal bounding boxes to target bounding box using bounding box deltas. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_bounding_box_transform.xhtml" title="Basic function to run NEBoundingBoxTransformKernel.">NEBoundingBoxTransform</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM16</td><td>QASYMM8</td><td>QASYMM16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_bounding_box_transform.xhtml" title="Basic function to run CLBoundingBoxTransformKernel.">CLBoundingBoxTransform</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM16</td><td>QASYMM8</td><td>QASYMM16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Cast </td><td rowspan="2" style="width:200px;">Function to cast a tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CAST </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_cast.xhtml" title="Basic function to run cpu::kernels::CpuCastKernel.">NECast</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>S16, S32, F32, F16 </td></tr>
<tr>
<td>QASYMM8</td><td>U16, S16, S32, F32, F16 </td></tr>
<tr>
<td>U8</td><td>U16, S16, S32, F32, F16 </td></tr>
<tr>
<td>U16</td><td>U8, U32 </td></tr>
<tr>
<td>S16</td><td>QASYMM8_SIGNED, U8, S32 </td></tr>
<tr>
<td>F16</td><td>QASYMM8_SIGNED, QASYMM8, F32, S32, U8 </td></tr>
<tr>
<td>S32</td><td>QASYMM8_SIGNED, QASYMM8, F16, F32, U8 </td></tr>
<tr>
<td>F32</td><td>QASYMM8_SIGNED, QASYMM8, BFLOAT16, F16, S32, U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_cast.xhtml" title="Basic function to run opencl::kernels::ClCastKernel.">CLCast</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>S8, U16, S16, U32, S32, F16, F32 </td></tr>
<tr>
<td>U16</td><td>U8, S8, S16, U32, S32, F16, F32 </td></tr>
<tr>
<td>S16</td><td>U8, S8, U16, U32, S32, F16, F32 </td></tr>
<tr>
<td>U32</td><td>U8, S8, U16, S16, S32, F16, F32 </td></tr>
<tr>
<td>S32</td><td>U8, S8, U16, S16, U32, F16, F32 </td></tr>
<tr>
<td>F16</td><td>U8, S8, U16, S16, U32, F32 </td></tr>
<tr>
<td>F32</td><td>U8, S8, U16, S16, U32, F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ChannelShuffleLayer </td><td rowspan="2" style="width:200px;">Function to shuffle the channels of the input tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CHANNEL_SHUFFLE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_channel_shuffle_layer.xhtml" title="Basic function to run NEChannelShuffleLayerKernel.">NEChannelShuffleLayer</a> </td><td><ul>
<li>
NCHW </li>
<li>
NHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_channel_shuffle_layer.xhtml" title="Basic function to run CLChannelShuffleLayerKernel.">CLChannelShuffleLayer</a> </td><td><ul>
<li>
NCHW </li>
<li>
NHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">Comparison </td><td rowspan="1" style="width:200px;">Function to compare 2 tensors. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_EQUAL </li>
<li>
ANEURALNETWORKS_GREATER </li>
<li>
ANEURALNETWORKS_GREATER_EQUAL </li>
<li>
ANEURALNETWORKS_LESS </li>
<li>
ANEURALNETWORKS_LESS_EQUAL </li>
<li>
ANEURALNETWORKS_NOT_EQUAL </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_comparison.xhtml" title="Basic function to run CLComparisonKernel.">CLComparison</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>All</td><td>All</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ConcatenateLayer </td><td rowspan="2" style="width:200px;">Function to concatenate tensors along a given axis. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONCATENATION </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_concatenate_layer.xhtml" title="Basic function to execute concatenate tensors along a given axis.">NEConcatenateLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_concatenate_layer.xhtml" title="Basic function to execute concatenate tensors along a given axis.">CLConcatenateLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ConvertFullyConnectedWeights </td><td rowspan="2" style="width:200px;">Function to transpose the weights for the fully connected layer. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_convert_fully_connected_weights.xhtml" title="Basic function to run cpu::kernels::CpuConvertFullyConnectedWeightsKernel.">NEConvertFullyConnectedWeights</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_convert_fully_connected_weights.xhtml" title="Basic function to run an opencl::kernels::ClConvertFullyConnectedWeightsKernel.">CLConvertFullyConnectedWeights</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ConvolutionLayer </td><td rowspan="2" style="width:200px;">Function to compute a convolution layer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_convolution_layer.xhtml" title="Basic function to simulate a convolution layer.">NEConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_convolution_layer.xhtml" title="Basic function to compute the convolution layer.">CLConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Conv3D </td><td rowspan="2" style="width:200px;">Function to compute a 3d convolution layer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONV_3D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_conv3_d.xhtml" title="Basic function to simulate a 3d convolution.">NEConv3D</a> </td><td><ul>
<li>
NDHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_conv3_d.xhtml" title="Basic function to compute the convolution3d layer.">CLConv3D</a> </td><td><ul>
<li>
NDHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Copy </td><td rowspan="2" style="width:200px;">Function to copy a tensor. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_copy.xhtml" title="Basic function to run cpu::kernels::CpuCopyKernel.">NECopy</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_copy.xhtml" title="Basic function to run opencl::kernels::ClCopyKernel.">CLCopy</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">Crop </td><td rowspan="1" style="width:200px;">Performs a copy of input tensor to the output tensor. </td><td rowspan="1"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_crop.xhtml" title="Basic function to run opencl::kernels::ClCropKernel.">CLCrop</a> </td><td><ul>
<li>
NHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">CropResize </td><td rowspan="2" style="width:200px;">Function to perform cropping and resizing. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_crop_resize.xhtml" title="Function to perform cropping and resizing.">NECropResize</a> </td><td><ul>
<li>
NHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>All</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_crop_resize.xhtml" title="Function to perform cropping and resizing.">CLCropResize</a> </td><td><ul>
<li>
NHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>All</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">DeconvolutionLayer </td><td rowspan="2" style="width:200px;">Function to compute a deconvolution or transpose convolution. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_TRANSPOSE_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_deconvolution_layer.xhtml" title="Function to run the deconvolution layer.">NEDeconvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_deconvolution_layer.xhtml" title="Basic function to compute the deconvolution layer.">CLDeconvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">DeconvolutionLayerUpsample </td><td rowspan="1" style="width:200px;">Function to execute deconvolution upsample on OpenCL. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_TRANSPOSE_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_deconvolution_layer_upsample.xhtml" title="Basic function to execute deconvolution upsample on OpenCL.">CLDeconvolutionLayerUpsample</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">DepthConvertLayer </td><td rowspan="2" style="width:200px;">Performs a down-scaling depth conversion. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_depth_convert_layer.xhtml" title="Basic function to run cpu::kernels::CpuCastKernel.">NEDepthConvertLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>F16, F32 </td></tr>
<tr>
<td>U8</td><td>U16, S16, S32 </td></tr>
<tr>
<td>U16</td><td>U8, U32 </td></tr>
<tr>
<td>S16</td><td>U8, S32 </td></tr>
<tr>
<td>BFLOAT16</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>QASYMM8, F32 </td></tr>
<tr>
<td>F32</td><td>QASYMM8, F16, BFLOAT16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_depth_convert_layer.xhtml" title="Basic function to run opencl::kernels::ClCastKernel.">CLDepthConvertLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>S8, U16, S16, U32, S32, F16, F32 </td></tr>
<tr>
<td>U16</td><td>U8, S8, S16, U32, S32, F16, F32 </td></tr>
<tr>
<td>S16</td><td>U8, S8, U16, U32, S32, F16, F32 </td></tr>
<tr>
<td>U32</td><td>U8, S8, U16, S16, S32, F16, F32 </td></tr>
<tr>
<td>S32</td><td>U8, S8, U16, S16, U32, F16, F32 </td></tr>
<tr>
<td>F16</td><td>U8, S8, U16, S16, U32, F32 </td></tr>
<tr>
<td>F32</td><td>U8, S8, U16, S16, U32, F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">DepthToSpaceLayer </td><td rowspan="2" style="width:200px;">Depth to Space transformation. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_DEPTH_TO_SPACE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_depth_to_space_layer.xhtml" title="Basic function to run NEDepthToSpaceLayerKernel.">NEDepthToSpaceLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_depth_to_space_layer.xhtml" title="Basic function to run CLDepthToSpaceLayerKernel.">CLDepthToSpaceLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">DepthwiseConvolutionLayer </td><td rowspan="2" style="width:200px;">Function to perform depthwise separable convolution. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_DEPTHWISE_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_depthwise_convolution_layer.xhtml" title="Function to execute a depthwise convolution.">NEDepthwiseConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_depthwise_convolution_layer.xhtml" title="Function to execute a depthwise convolution.">CLDepthwiseConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">DequantizationLayer </td><td rowspan="2" style="width:200px;">Function to dequantize the values in a tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_DEQUANTIZE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_dequantization_layer.xhtml" title="Basic function to run cpu::CpuDequantize that dequantizes an input tensor.">NEDequantizationLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>F16, F32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>F16, F32 </td></tr>
<tr>
<td>QSYMM8_PER_CHANNEL</td><td>F16, F32 </td></tr>
<tr>
<td>QSYMM8</td><td>F16, F32 </td></tr>
<tr>
<td>QSYMM16</td><td>F16, F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_dequantization_layer.xhtml" title="Basic function to run opencl::ClDequantize that dequantizes an input tensor.">CLDequantizationLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>F16, F32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>F16, F32 </td></tr>
<tr>
<td>QSYMM8_PER_CHANNEL</td><td>F16, F32 </td></tr>
<tr>
<td>QSYMM8</td><td>F16, F32 </td></tr>
<tr>
<td>QSYMM16</td><td>F16, F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">DetectionPostProcessLayer </td><td rowspan="1" style="width:200px;">Function to generate the detection output based on center size encoded boxes, class prediction and anchors by doing non maximum suppression (NMS). </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_DETECTION_POSTPROCESSING </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_detection_post_process_layer.xhtml" title="NE Function to generate the detection output based on center size encoded boxes, class prediction and...">NEDetectionPostProcessLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0 - src2</th><th>dst0 - dst3 </th></tr>
<tr>
<td>QASYMM8</td><td>F32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>F32 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">DirectConvolutionLayer </td><td rowspan="2" style="width:200px;">Function to compute direct convolution. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_direct_convolution_layer.xhtml" title="Function to run the direct convolution.">NEDirectConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_direct_convolution_layer.xhtml" title="Basic function to execute direct convolution function:">CLDirectConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">DirectDeconvolutionLayer </td><td rowspan="1" style="width:200px;">Function to run the deconvolution layer. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_TRANSPOSE_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_direct_deconvolution_layer.xhtml" title="Function to run the deconvolution layer.">CLDirectDeconvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="13">ElementwiseOperations </td><td rowspan="13" style="width:200px;">Function to perform in Cpu: - Div - Max - Min - Pow - SquaredDiff - Comparisons (Equal, greater, greater_equal, less, less_equal, not_equal) Function to perform in CL: - Add - Sub - Div - Max - Min - Pow - SquaredDiff </td><td rowspan="13"><ul>
<li>
ANEURALNETWORKS_MAXIMUM </li>
<li>
ANEURALNETWORKS_MINIMUM </li>
<li>
ANEURALNETWORKS_POW </li>
<li>
ANEURALNETWORKS_DIV </li>
<li>
ANEURALNETWORKS_ADD </li>
<li>
ANEURALNETWORKS_SUB </li>
<li>
ANEURALNETWORKS_EQUAL </li>
<li>
ANEURALNETWORKS_GREATER </li>
<li>
ANEURALNETWORKS_GREATER_EQUAL </li>
<li>
ANEURALNETWORKS_LESS </li>
<li>
ANEURALNETWORKS_LESS_EQUAL </li>
<li>
ANEURALNETWORKS_NOT_EQUAL </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_elementwise_max.xhtml" title="Basic function to run cpu::kernels::CpuArithmeticKernel for max.">NEElementwiseMax</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_elementwise_min.xhtml" title="Basic function to run cpu::kernels::CpuArithmeticKernel for min.">NEElementwiseMin</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_elementwise_squared_diff.xhtml" title="Basic function to run cpu::kernels::CpuArithmeticKernel for squared difference.">NEElementwiseSquaredDiff</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_elementwise_division.xhtml" title="Basic function to run cpu::kernels::CpuArithmeticKernel for division.">NEElementwiseDivision</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_elementwise_power.xhtml" title="Basic function to run cpu::kernels::CpuArithmeticKernel for power.">NEElementwisePower</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_elementwise_comparison.xhtml" title="Basic function to run cpu::kernels::CpuComparisonKernel.">NEElementwiseComparison</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>U8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>U8 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>U8 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>U8 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>U8 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_arithmetic_addition.xhtml" title="Basic function to run opencl::kernels::ClSaturatedArithmeticKernel for addition.">CLArithmeticAddition</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>U8</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_arithmetic_subtraction.xhtml" title="Basic function to run opencl::kernels::ClSaturatedArithmeticKernel for subtraction.">CLArithmeticSubtraction</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>U8</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_arithmetic_division.xhtml" title="Basic function to run opencl::kernels::ClSaturatedArithmeticKernel for division.">CLArithmeticDivision</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_elementwise_max.xhtml" title="Basic function to run opencl::kernels::ClArithmeticKernel for max.">CLElementwiseMax</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>U32</td><td>U32</td><td>U32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_elementwise_min.xhtml" title="Basic function to run opencl::kernels::ClArithmeticKernel for min.">CLElementwiseMin</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>U32</td><td>U32</td><td>U32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_elementwise_squared_diff.xhtml" title="Basic function to run opencl::kernels::ClArithmeticKernel for squared difference.">CLElementwiseSquaredDiff</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_elementwise_power.xhtml" title="Basic function to run opencl::kernels::ClArithmeticKernel for power.">CLElementwisePower</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="8">ElementwiseUnaryLayer </td><td rowspan="8" style="width:200px;">Function to perform: - Rsqrt - Exp - Neg - Log - Abs - Round - Sin </td><td rowspan="8"><ul>
<li>
ANEURALNETWORKS_ABS </li>
<li>
ANEURALNETWORKS_EXP </li>
<li>
ANEURALNETWORKS_LOG </li>
<li>
ANEURALNETWORKS_NEG </li>
<li>
ANEURALNETWORKS_RSQRT </li>
<li>
ANEURALNETWORKS_SIN </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_elementwise_unary_layer.xhtml" title="Basic function to perform unary elementwise operations.">NEElementwiseUnaryLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_rsqrt_layer.xhtml" title="Basic function to perform inverse square root on an input tensor.">CLRsqrtLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_exp_layer.xhtml" title="Basic function to perform exponential on an input tensor.">CLExpLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_neg_layer.xhtml" title="Basic function to negate an input tensor.">CLNegLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_sin_layer.xhtml" title="Basic function to calculate sine of an input tensor.">CLSinLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_log_layer.xhtml" title="Basic function to perform elementwise log on an input tensor.">CLLogLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_abs_layer.xhtml" title="Basic function to get the absolute value of an input tensor.">CLAbsLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_round_layer.xhtml" title="Basic function to get the round (to the nearest even) value of an input tensor.">CLRoundLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">FFT1D </td><td rowspan="2" style="width:200px;">Fast Fourier Transform 1D. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_f_f_t1_d.xhtml" title="Basic function to execute one dimensional FFT.">NEFFT1D</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_f_f_t1_d.xhtml" title="Basic function to execute one dimensional FFT.">CLFFT1D</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">FFT2D </td><td rowspan="2" style="width:200px;">Fast Fourier Transform 2D. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_f_f_t2_d.xhtml" title="Basic function to execute two dimensional FFT.">NEFFT2D</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_f_f_t2_d.xhtml" title="Basic function to execute two dimensional FFT.">CLFFT2D</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">FFTConvolutionLayer </td><td rowspan="2" style="width:200px;">Fast Fourier Transform Convolution. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_f_f_t_convolution_layer.xhtml" title="Basic function to execute FFT-based convolution on CPU.">NEFFTConvolutionLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_f_f_t_convolution_layer.xhtml" title="Basic function to execute FFT-based convolution on OpenCL.">CLFFTConvolutionLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Fill </td><td rowspan="2" style="width:200px;">Set the values of a tensor with a given value. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_FILL </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_fill.xhtml" title="Basic function to run cpu::kernels::CpuFillKernel.">NEFill</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_fill.xhtml" title="Basic function to run opencl::kernels::ClFillKernel.">CLFill</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">FillBorder </td><td rowspan="1" style="width:200px;">Function to fill the borders within the XY-planes. </td><td rowspan="1"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_fill_border.xhtml" title="Basic function to run NEFillBorderKernel.">NEFillBorder</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">FlattenLayer </td><td rowspan="2" style="width:200px;">Reshape a tensor to be 1D </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_RESHAPE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_flatten_layer.xhtml" title="Basic function to execute flatten layer kernel.">NEFlattenLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_flatten_layer.xhtml" title="Basic function to execute flatten.">CLFlattenLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Floor </td><td rowspan="2" style="width:200px;">Round the value to the lowest number. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_FLOOR </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_floor.xhtml" title="Basic function to run cpu::kernels::CpuFloorKernel.">NEFloor</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_floor.xhtml" title="Basic function to run opencl::kernels::ClFloorKernel.">CLFloor</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">FullyConnectedLayer </td><td rowspan="2" style="width:200px;">Function to perform a fully connected / dense layer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_FULLY_CONNECTED </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_fully_connected_layer.xhtml" title="Basic function to compute a Fully Connected layer.">NEFullyConnectedLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_fully_connected_layer.xhtml" title="Basic function to compute a Fully Connected layer on OpenCL.">CLFullyConnectedLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">FuseBatchNormalization </td><td rowspan="2" style="width:200px;">Function to fuse the batch normalization node to a preceding convolution node. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_fuse_batch_normalization.xhtml" title="Basic function to fuse the batch normalization node to a preceding convolution node.">NEFuseBatchNormalization</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_fuse_batch_normalization.xhtml" title="Basic function to fuse the batch normalization node to a preceding convolution node.">CLFuseBatchNormalization</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Gather </td><td rowspan="2" style="width:200px;">Performs the Gather operation along the chosen axis. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_GATHER </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_gather.xhtml" title="Basic function to run NEGatherKernel.">NEGather</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_gather.xhtml" title="Basic function to run CLGatherKernel.">CLGather</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">GEMM </td><td rowspan="2" style="width:200px;">General Matrix Multiplication. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_g_e_m_m.xhtml" title="Basic function to execute GEMM.">NEGEMM</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>BFLOAT16</td><td>BFLOAT16</td><td>BFLOAT16</td><td>BFLOAT16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_g_e_m_m.xhtml" title="Basic function to execute GEMM on OpenCL.">CLGEMM</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">GEMMConv2d </td><td rowspan="1" style="width:200px;">General Matrix Multiplication. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_g_e_m_m_conv2d.xhtml" title="Basic function to compute the convolution layer.">NEGEMMConv2d</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>BFLOAT16</td><td>BFLOAT16</td><td>BFLOAT16</td><td>BFLOAT16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">GEMMConvolutionLayer </td><td rowspan="2" style="width:200px;">General Matrix Multiplication. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_g_e_m_m_convolution_layer.xhtml" title="Basic function to compute the convolution layer.">NEGEMMConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>BFLOAT16</td><td>BFLOAT16</td><td>BFLOAT16</td><td>BFLOAT16 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_g_e_m_m_convolution_layer.xhtml" title="Basic function to compute the convolution layer.">CLGEMMConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">GEMMDeconvolutionLayer </td><td rowspan="1" style="width:200px;">General Matrix Multiplication. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_TRANSPOSE_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_g_e_m_m_deconvolution_layer.xhtml" title="Function to run the deconvolution layer through a call to GEMM.">CLGEMMDeconvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">GEMMLowpMatrixMultiplyCore </td><td rowspan="2" style="width:200px;">General Matrix Multiplication. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_g_e_m_m_lowp_matrix_multiply_core.xhtml" title="Function to run Gemm on quantized types.">NEGEMMLowpMatrixMultiplyCore</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8</td><td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_g_e_m_m_lowp_matrix_multiply_core.xhtml" title="Basic function to execute GEMMLowpMatrixMultiplyCore on OpenCL.">CLGEMMLowpMatrixMultiplyCore</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8</td><td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">GEMMLowpOutputStage </td><td rowspan="2" style="width:200px;">General Matrix Multiplication. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_g_e_m_m_lowp_output_stage.xhtml" title="Basic function to execute GEMMLowpQuantizeDown kernels.">NEGEMMLowpOutputStage</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>S32</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>S32</td><td>S32</td><td>QSYMM16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_g_e_m_m_lowp_output_stage.xhtml" title="Basic function to execute GEMMLowpQuantizeDown kernels on CL.">CLGEMMLowpOutputStage</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>S32</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>S32</td><td>S32</td><td>QSYMM16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">GenerateProposalsLayer </td><td rowspan="2" style="width:200px;">Function to generate proposals for a RPN (Region Proposal Network). </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_GENERATE_PROPOSALS </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_generate_proposals_layer.xhtml" title="Basic function to generate proposals for a RPN (Region Proposal Network)">NEGenerateProposalsLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8</td><td>QSYMM16</td><td>QASYMM8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_generate_proposals_layer.xhtml" title="Basic function to generate proposals for a RPN (Region Proposal Network)">CLGenerateProposalsLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8</td><td>QSYMM16</td><td>QASYMM8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">InstanceNormalizationLayer </td><td rowspan="2" style="width:200px;">Function to perform a Instance normalization on a given axis. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_INSTANCE_NORMALIZATION </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_instance_normalization_layer.xhtml" title="Basic function to perform a Instance normalization.">NEInstanceNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_instance_normalization_layer.xhtml" title="Basic function to perform a Instance normalization.">CLInstanceNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">L2NormalizeLayer </td><td rowspan="2" style="width:200px;">Function to perform a L2 normalization on a given axis. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_L2_NORMALIZATION </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_l2_normalize_layer.xhtml" title="Basic function to perform a L2 normalization on a given axis.">NEL2NormalizeLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_l2_normalize_layer.xhtml" title="Basic function to perform a L2 normalization on a given axis.">CLL2NormalizeLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="3">Logical </td><td rowspan="3" style="width:200px;">Function to perform: - Logical AND - Logical OR - Logical NOT </td><td rowspan="3"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_logical_and.xhtml" title="Basic function to perform logical AND.">NELogicalAnd</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_logical_or.xhtml" title="Basic function to perform logical OR.">NELogicalOr</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_logical_not.xhtml" title="Basic function to perform logical NOT.">NELogicalNot</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">LogicalAnd </td><td rowspan="1" style="width:200px;">Function to perform Logical AND. </td><td rowspan="1"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_logical_and.xhtml" title="Basic function to run arm_compute::opencl::kernels::ClLogicalBinaryKernel.">CLLogicalAnd</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">LogicalOr </td><td rowspan="1" style="width:200px;">Function to perform Logical OR. </td><td rowspan="1"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_logical_or.xhtml" title="Basic function to run arm_compute::opencl::kernels::ClLogicalBinaryKernel.">CLLogicalOr</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">LogicalNot </td><td rowspan="1" style="width:200px;">Function to perform Logical NOT. </td><td rowspan="1"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_logical_not.xhtml" title="Basic function to do logical NOT operation.">CLLogicalNot</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">LSTMLayer </td><td rowspan="2" style="width:200px;">Function to perform a single time step in a Long Short-Term <a class="el" href="classarm__compute_1_1_memory.xhtml" title="CPU implementation of memory object.">Memory</a> (LSTM) layer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_LSTM </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_l_s_t_m_layer.xhtml" title="Basic function to run NELSTMLayer.">NELSTMLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0 - src13</th><th>dst0 - dst3 </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_l_s_t_m_layer.xhtml" title="This function performs a single time step in a Long Short-Term Memory (LSTM) layer.">CLLSTMLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0 - src13</th><th>dst0 - dst3 </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">LSTMLayerQuantized </td><td rowspan="2" style="width:200px;">Function to perform quantized LSTM (Long Short-Term <a class="el" href="classarm__compute_1_1_memory.xhtml" title="CPU implementation of memory object.">Memory</a>) </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_QUANTIZED_LSTM </li>
<li>
ANEURALNETWORKS_QUANTIZED_16BIT_LSTM </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_l_s_t_m_layer_quantized.xhtml" title="Basic function to run NELSTMLayerQuantized.">NELSTMLayerQuantized</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0 - src8</th><th>src9 - src12</th><th>src13</th><th>src14</th><th>dst0</th><th>dst1 </th></tr>
<tr>
<td>QASYMM8</td><td>S32</td><td>QSYMM16</td><td>QASYMM8</td><td>QSYMM16</td><td>QASYMM8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_l_s_t_m_layer_quantized.xhtml" title="Basic function to run CLLSTMLayerQuantized.">CLLSTMLayerQuantized</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0 - src8</th><th>src9 - src12</th><th>src13</th><th>src14</th><th>dst0</th><th>dst1 </th></tr>
<tr>
<td>QASYMM8</td><td>S32</td><td>QSYMM16</td><td>QASYMM8</td><td>QSYMM16</td><td>QASYMM8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">MaxUnpoolingLayer </td><td rowspan="2" style="width:200px;">Function to perform MaxUnpooling. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_max_unpooling_layer.xhtml" title="Function to perform MaxUnpooling.">NEMaxUnpoolingLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_max_unpooling_layer.xhtml" title="Function to perform MaxUnpooling.">CLMaxUnpoolingLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">MeanStdDevNormalizationLayer </td><td rowspan="2" style="width:200px;">Function to execute mean and standard deviation normalization. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_mean_std_dev_normalization_layer.xhtml" title="Basic function to execute mean and standard deviation normalization by calling NEMeanStdDevNormalizat...">NEMeanStdDevNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_mean_std_dev_normalization_layer.xhtml" title="Basic function to execute mean and standard deviation normalization by calling CLMeanStdDevNormalizat...">CLMeanStdDevNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">NormalizationLayer </td><td rowspan="2" style="width:200px;">Function to compute normalization layer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_LOCAL_RESPONSE_NORMALIZATION </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_normalization_layer.xhtml" title="Basic function to compute a normalization layer.">NENormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_normalization_layer.xhtml" title="Basic function to compute a normalization layer.">CLNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">PadLayer </td><td rowspan="2" style="width:200px;">Function to pad a tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_PAD </li>
<li>
ANEURALNETWORKS_PAD_V2 </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_pad_layer.xhtml" title="Basic function to pad a tensor.">NEPadLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_pad_layer.xhtml" title="Basic function to pad a tensor.">CLPadLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Permute </td><td rowspan="2" style="width:200px;">Function to transpose an ND tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_TRANSPOSE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_permute.xhtml" title="Basic function to run cpu::kernels::CpuPermuteKernel.">NEPermute</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_permute.xhtml" title="Basic function to execute an opencl::kernels::ClPermuteKernel.">CLPermute</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">PixelWiseMultiplication </td><td rowspan="2" style="width:200px;">Function to perform a multiplication. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_MUL </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_pixel_wise_multiplication.xhtml" title="Basic function to run cpu::CpuMul.">NEPixelWiseMultiplication</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>S32 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>U8</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>S32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_pixel_wise_multiplication.xhtml" title="Basic function to run opencl::ClMul.">CLPixelWiseMultiplication</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>S32 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>U8</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">PoolingLayer </td><td rowspan="2" style="width:200px;">Function to perform pooling with the specified pooling operation. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_AVERAGE_POOL_2D </li>
<li>
ANEURALNETWORKS_L2_POOL_2D </li>
<li>
ANEURALNETWORKS_MAX_POOL_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_pooling_layer.xhtml" title="Basic function to simulate a pooling layer with the specified pooling operation.">NEPoolingLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_pooling_layer.xhtml" title="Basic function to run opencl::ClPool2d.">CLPoolingLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Pooling3dLayer </td><td rowspan="2" style="width:200px;">Function to perform pooling 3D with the specified pooling operation. </td><td rowspan="2"><ul>
<li>
N/A </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_pooling3d_layer.xhtml" title="Basic function to simulate a pooling 3d layer with the specified pooling operation.">NEPooling3dLayer</a> </td><td><ul>
<li>
NDHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_pooling3d_layer.xhtml" title="Basic function to run opencl::ClPool3d.">CLPooling3dLayer</a> </td><td><ul>
<li>
NDHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">PReluLayer </td><td rowspan="2" style="width:200px;">Function to compute the activation layer with the PRELU activation function. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_PRELU </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_p_relu_layer.xhtml" title="Basic function to run cpu::kernels::CpuArithmeticKernel for PRELU.">NEPReluLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_p_relu_layer.xhtml" title="Basic function to run opencl::kernels::ClArithmeticKernel for PRELU.">CLPReluLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">PriorBoxLayer </td><td rowspan="2" style="width:200px;">Function to compute prior boxes and clip. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_prior_box_layer.xhtml" title="Basic function to run NEPriorBoxLayerKernel.">NEPriorBoxLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_prior_box_layer.xhtml" title="Basic function to run CLPriorBoxLayerKernel.">CLPriorBoxLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">QLSTMLayer </td><td rowspan="2" style="width:200px;">Function to perform quantized LSTM (Long Short-Term <a class="el" href="classarm__compute_1_1_memory.xhtml" title="CPU implementation of memory object.">Memory</a>). </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_QUANTIZED_LSTM </li>
<li>
ANEURALNETWORKS_QUANTIZED_16BIT_LSTM </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_q_l_s_t_m_layer.xhtml" title="Basic function to run NEQLSTMLayer.">NEQLSTMLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1 - src6</th><th>src7 -src9</th><th>src10</th><th>src11</th><th>dst0</th><th>dst1 - dst2 </th></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8</td><td>S32</td><td>QSYMM16</td><td>QASYMM8_SIGNED</td><td>QSYMM16</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_q_l_s_t_m_layer.xhtml" title="Basic function to run CLQLSTMLayer.">CLQLSTMLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1 - src6</th><th>src7 -src9</th><th>src10</th><th>src11</th><th>dst0</th><th>dst1 - dst2 </th></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8</td><td>S32</td><td>QSYMM16</td><td>QASYMM8_SIGNED</td><td>QSYMM16</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">QuantizationLayer </td><td rowspan="2" style="width:200px;">Function to perform quantization layer </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_QUANTIZE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_quantization_layer.xhtml" title="Basic function to run a quantization layer using cpu::CpuQuantize.">NEQuantizationLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
<tr>
<td>F16</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
<tr>
<td>F32</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_quantization_layer.xhtml" title="Basic function to simulate a quantization layer.">CLQuantizationLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
<tr>
<td>F16</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
<tr>
<td>F32</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Range </td><td rowspan="2" style="width:200px;">Function to generates a sequence of numbers starting from START and extends by increments of 'STEP' up to but not including 'END'. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_range.xhtml" title="Basic function to run NERangeKernel.">NERange</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>dst </th></tr>
<tr>
<td>U8 </td></tr>
<tr>
<td>S8 </td></tr>
<tr>
<td>U16 </td></tr>
<tr>
<td>S16 </td></tr>
<tr>
<td>U32 </td></tr>
<tr>
<td>S32 </td></tr>
<tr>
<td>F16 </td></tr>
<tr>
<td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_range.xhtml" title="Basic function to run CLRangeKernel.">CLRange</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>dst </th></tr>
<tr>
<td>U8 </td></tr>
<tr>
<td>S8 </td></tr>
<tr>
<td>QASYMM8 </td></tr>
<tr>
<td>U16 </td></tr>
<tr>
<td>S16 </td></tr>
<tr>
<td>U32 </td></tr>
<tr>
<td>S32 </td></tr>
<tr>
<td>F16 </td></tr>
<tr>
<td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ReduceMean </td><td rowspan="2" style="width:200px;">Function to perform reduce mean operation. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_MEAN </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_reduce_mean.xhtml" title="Basic function to perform reduce operation.">NEReduceMean</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_reduce_mean.xhtml" title="Basic function to perform reduce operation.">CLReduceMean</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ReductionOperation </td><td rowspan="2" style="width:200px;">Function to perform reduce with the following operations - ARG_IDX_MAX: Index of the max value - ARG_IDX_MIN: Index of the min value - MEAN_SUM: Mean of sum - PROD: Product - SUM_SQUARE: Sum of squares - SUM: Sum - MIN: Min - MAX: Max </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_REDUCE_ALL </li>
<li>
ANEURALNETWORKS_REDUCE_ANY </li>
<li>
ANEURALNETWORKS_REDUCE_MAX </li>
<li>
ANEURALNETWORKS_REDUCE_MIN </li>
<li>
ANEURALNETWORKS_REDUCE_PROD </li>
<li>
ANEURALNETWORKS_REDUCE_SUM </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_reduction_operation.xhtml" title="Basic function to simulate a reduction operation.">NEReductionOperation</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_reduction_operation.xhtml" title="Perform reduction operation.">CLReductionOperation</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ReorgLayer </td><td rowspan="2" style="width:200px;">Performs a reorganization layer of input tensor to the output tensor. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_reorg_layer.xhtml" title="Basic function to run NEReorgLayerKernel.">NEReorgLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_reorg_layer.xhtml">CLReorgLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ReshapeLayer </td><td rowspan="2" style="width:200px;">Function to reshape a tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_RESHAPE </li>
<li>
ANEURALNETWORKS_SQUEEZE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_reshape_layer.xhtml" title="Basic function to run cpu::kernels::CpuReshapeKernel.">NEReshapeLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_reshape_layer.xhtml" title="Basic function to run opencl::kernels::ClReshapeKernel.">CLReshapeLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Reverse </td><td rowspan="2" style="width:200px;">Function to reverse tensor according to axis. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_reverse.xhtml" title="Basic function to run NEReverseKernel.">NEReverse</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>All</td><td>U32</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_reverse.xhtml" title="Basic function to run CLReverseKernel.">CLReverse</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>All</td><td>U32</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">RNNLayer </td><td rowspan="2" style="width:200px;">Function to perform recurrent neural network layer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_RNN </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_r_n_n_layer.xhtml" title="Basic function to run NERNNLayer.">NERNNLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>src3</th><th>dst0</th><th>dst1 </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_r_n_n_layer.xhtml" title="Basic function to run CLRNNLayer.">CLRNNLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>src3</th><th>dst0</th><th>dst1 </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ROIAlignLayer </td><td rowspan="2" style="width:200px;">Function to perform ROI alignment. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_ROI_ALIGN </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_r_o_i_align_layer.xhtml" title="Basic function to run NEROIAlignLayerKernel.">NEROIAlignLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM16</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM16</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_r_o_i_align_layer.xhtml" title="Basic function to run CLROIAlignLayerKernel.">CLROIAlignLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM16</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM16</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ROIPoolingLayer </td><td rowspan="2" style="width:200px;">Function to perform ROI pooling. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_ROI_POOLING </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_r_o_i_pooling_layer.xhtml" title="Basic function to run NEROIPoolingLayerKernel.">NEROIPoolingLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F32</td><td>U16</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>U16</td><td>QASYMM8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_r_o_i_pooling_layer.xhtml" title="Basic function to run CLROIPoolingLayerKernel.">CLROIPoolingLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>U16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>U16</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>U16</td><td>QASYMM8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Scale </td><td rowspan="2" style="width:200px;">Function to perform resize a tensor using to interpolate: - Bilinear - Nearest neighbor </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_RESIZE_BILINEAR </li>
<li>
ANEURALNETWORKS_RESIZE_NEAREST_NEIGHBOR </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_scale.xhtml" title="Basic function to compute Scale.">NEScale</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
<tr>
<td>S8</td><td>S8 </td></tr>
<tr>
<td>S16</td><td>S16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_scale.xhtml" title="Basic function to run opencl::ClScale.">CLScale</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Select </td><td rowspan="2" style="width:200px;">Function to select values from 2 tensors depending on an input tensor of booleans. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_SELECT </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_select.xhtml" title="Basic function to run NESelect.">NESelect</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>U8</td><td>All</td><td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_select.xhtml" title="Basic function to run CLSelect.">CLSelect</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>U8</td><td>All</td><td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Slice </td><td rowspan="2" style="width:200px;">Function to perform tensor slicing. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_SLICE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_slice.xhtml" title="Basic function to perform tensor slicing.">NESlice</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_slice.xhtml" title="Basic function to perform tensor slicing.">CLSlice</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">SoftmaxLayer </td><td rowspan="2" style="width:200px;">Function to compute a SoftmaxLayer and a Log SoftmaxLayer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_LOG_SOFTMAX </li>
<li>
ANEURALNETWORKS_SOFTMAX </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_softmax_layer_generic.xhtml" title="Basic function to compute a SoftmaxLayer and a Log SoftmaxLayer.">NESoftmaxLayerGeneric</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_softmax_layer_generic.xhtml" title="Basic function to compute a SoftmaxLayer.">CLSoftmaxLayerGeneric</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">SpaceToBatchLayer </td><td rowspan="2" style="width:200px;">Function to divide a tensor spatially. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_SPACE_TO_BATCH_ND </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_space_to_batch_layer.xhtml" title="Basic function to spatial divide a tensor.">NESpaceToBatchLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>All</td><td>S32</td><td>S32</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_space_to_batch_layer.xhtml" title="Basic function to spatial divide a tensor.">CLSpaceToBatchLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>All</td><td>S32</td><td>S32</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">SpaceToDepthLayer </td><td rowspan="2" style="width:200px;">Function to rearrange blocks of spatial data into depth. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_SPACE_TO_DEPTH </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_space_to_depth_layer.xhtml" title="Basic function to run NESpaceToDepthLayerKernel.">NESpaceToDepthLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_space_to_depth_layer.xhtml" title="Basic function to run CLSpaceToDepthLayerKernel.">CLSpaceToDepthLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Split </td><td rowspan="2" style="width:200px;">Function to split a tensor along a given axis. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_SPLIT </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_split.xhtml" title="Basic function to split a tensor along a given axis.">NESplit</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_split.xhtml" title="Basic function to split a tensor along a given axis.">CLSplit</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">StackLayer </td><td rowspan="2" style="width:200px;">Function to stack tensors along an axis. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_stack_layer.xhtml" title="Basic function to stack tensors along an axis.">NEStackLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_stack_layer.xhtml" title="Basic function to stack tensors along an axis.">CLStackLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">StridedSlice </td><td rowspan="2" style="width:200px;">Function to extract a strided slice of a tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_STRIDED_SLICE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_strided_slice.xhtml" title="Basic function to run NEStridedSliceKernel.">NEStridedSlice</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_strided_slice.xhtml" title="Basic function to run CLStridedSliceKernel.">CLStridedSlice</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Tile </td><td rowspan="2" style="width:200px;">Function to construct a tensor by tiling a given tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_TILE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_tile.xhtml" title="Basic function to run NETileKernel.">NETile</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_tile.xhtml" title="Basic function to run CLTileKernel.">CLTile</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Transpose </td><td rowspan="2" style="width:200px;">Function to transpose a 2D tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_TRANSPOSE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_transpose.xhtml" title="Basic function to run cpu::kernels::CpuTransposeKernel.">NETranspose</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_transpose.xhtml" title="Basic function to execute an opencl::kernels::ClTransposeKernel.">CLTranspose</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Unstack </td><td rowspan="2" style="width:200px;">Function to unpack a rank-R tensor into rank-(R-1) tensors. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_unstack.xhtml" title="Basic function to unpack a rank-R tensor into rank-(R-1) tensors.">NEUnstack</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_unstack.xhtml" title="Basic function to unpack a rank-R tensor into rank-(R-1) tensors.">CLUnstack</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">WinogradConvolutionLayer </td><td rowspan="2" style="width:200px;">Function to do Winograd Convolution. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_winograd_convolution_layer.xhtml" title="Basic function to simulate a convolution layer.">NEWinogradConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_winograd_convolution_layer.xhtml" title="Basic function to execute Winograd-based convolution on OpenCL.">CLWinogradConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
</table>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Wed Jul 5 2023 11:29:23 for Compute Library by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
