<!-- HTML header for doxygen 1.8.15-->
<!-- Remember to use version doxygen 1.8.15 +-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="robots" content="NOINDEX, NOFOLLOW" /> <!-- Prevent indexing by search engines -->
<title>Compute Library: Supported Operators</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
</script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <img alt="Compute Library" src="https://raw.githubusercontent.com/ARM-software/ComputeLibrary/gh-pages/ACL_logo.png" style="max-width: 100%;margin-top: 15px;margin-left: 10px"/>
  <td style="padding-left: 0.5em;">
   <div id="projectname">
   &#160;<span id="projectnumber">22.02</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('operators_list.xhtml','');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Supported Operators </div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#S9_1_operators_list">Supported Operators</a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="S9_1_operators_list"></a>
Supported Operators</h1>
<p>Compute Library supports operators that are listed in below table.</p>
<p>Compute Library supports a wide list of data-types, information can been directly found in the documentation of each kernel/function. The main data-types that the Machine Learning functions support are the following: </p><ul>
<li>
BFLOAT16: 16-bit non-standard brain floating point </li>
<li>
QASYMM8: 8-bit unsigned asymmetric quantized </li>
<li>
QASYMM8_SIGNED: 8-bit signed asymmetric quantized </li>
<li>
QSYMM8_PER_CHANNEL: 8-bit signed symmetric quantized (Used for the weights) </li>
<li>
QSYMM8: 8-bit unsigned symmetric quantized </li>
<li>
QSYMM16: 16-bit unsigned symmetric quantized </li>
<li>
F32: 32-bit single precision floating point </li>
<li>
F16: 16-bit half precision floating point </li>
<li>
S32: 32-bit signed integer </li>
<li>
U8: 8-bit unsigned char </li>
<li>
All: Agnostic to any specific data type </li>
</ul>
<p>Compute Library supports the following data layouts (fast changing dimension from right to left): </p><ul>
<li>
NHWC: The native layout of Compute Library that delivers the best performance where channels are in the fastest changing dimension </li>
<li>
NCHW: Legacy layout where width is in the fastest changing dimension </li>
<li>
NDHWC: New data layout for supporting 3D operators </li>
<li>
All: Agnostic to any specific data layout </li>
</ul>
<p>where N = batches, C = channels, H = height, W = width, D = depth</p>
<a class="anchor" id="multi_row"></a>
<table class="doxtable">
<caption></caption>
<tr>
<th>Function </th><th>Description </th><th>Equivalent Android NNAPI Op </th><th>Backends </th><th>Data Layouts </th><th>Data Types </th></tr>
<tr>
<td rowspan="2">ActivationLayer </td><td rowspan="2" style="width:200px;">Function to simulate an activation layer with the specified activation function. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_ELU </li>
<li>
ANEURALNETWORKS_HARD_SWISH </li>
<li>
ANEURALNETWORKS_LOGISTIC </li>
<li>
ANEURALNETWORKS_RELU </li>
<li>
ANEURALNETWORKS_RELU1 </li>
<li>
ANEURALNETWORKS_RELU6 </li>
<li>
ANEURALNETWORKS_TANH </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_activation_layer.xhtml" title="Basic function to run cpu::kernels::CpuActivationKernel. ">NEActivationLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_activation_layer.xhtml" title="Basic function to run opencl::kernels::ClActivationKernel. ">CLActivationLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ArgMinMaxLayer </td><td rowspan="2" style="width:200px;">Function to calculate the index of the minimum or maximum values in a tensor based on an axis. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_ARGMAX </li>
<li>
ANEURALNETWORKS_ARGMIN </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_arg_min_max_layer.xhtml" title="Function to calculate the index of the minimum or maximum values in a tensor based on an axis...">NEArgMinMaxLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>U32, S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>U32, S32 </td></tr>
<tr>
<td>S32</td><td>U32, S32 </td></tr>
<tr>
<td>F16</td><td>U32, S32 </td></tr>
<tr>
<td>F32</td><td>U32, S32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_arg_min_max_layer.xhtml" title="Function to calculate the index of the minimum or maximum values in a tensor based on an axis...">CLArgMinMaxLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>U32, S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>U32, S32 </td></tr>
<tr>
<td>S32</td><td>U32, S32 </td></tr>
<tr>
<td>F16</td><td>U32, S32 </td></tr>
<tr>
<td>F32</td><td>U32, S32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">ArithmeticAddition </td><td rowspan="1" style="width:200px;">Function to add 2 tensors. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_ADD </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_arithmetic_addition.xhtml" title="Basic function to run cpu::kernels::CpuAddKernel. ">NEArithmeticAddition</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>S32 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">ArithmeticSubtraction </td><td rowspan="1" style="width:200px;">Function to substract 2 tensors. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_SUB </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_arithmetic_subtraction.xhtml" title="Basic function to run cpu::kernels::CpuSubKernel. ">NEArithmeticSubtraction</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>S32 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BatchNormalizationLayer </td><td rowspan="2" style="width:200px;">Function to perform batch normalization. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_batch_normalization_layer.xhtml" title="Basic function to run NENormalizationLayerKernel and simulate a batch normalization layer...">NEBatchNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_batch_normalization_layer.xhtml" title="Basic function to run CLNormalizationLayerKernel and simulate a batch normalization layer...">CLBatchNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BatchToSpaceLayer </td><td rowspan="2" style="width:200px;">Batch to space transformation. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_BATCH_TO_SPACE_ND </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_batch_to_space_layer.xhtml" title="Basic function to run NEBatchToSpaceLayerKernel. ">NEBatchToSpaceLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>All</td><td>s32</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_batch_to_space_layer.xhtml" title="Basic function to run CLBatchToSpaceLayerKernel. ">CLBatchToSpaceLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>All</td><td>s32</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BitwiseAnd </td><td rowspan="2" style="width:200px;">Function to perform bitwise AND between 2 tensors. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_LOGICAL_AND </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_bitwise_and.xhtml" title="Basic function to run NEBitwiseAndKernel. ">NEBitwiseAnd</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_bitwise_and.xhtml" title="Basic function to perform bitwise AND by running CLBitwiseKernel. ">CLBitwiseAnd</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BitwiseNot </td><td rowspan="2" style="width:200px;">Function to perform bitwise NOT. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_LOGICAL_NOT </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_bitwise_not.xhtml" title="Basic function to run NEBitwiseNotKernel. ">NEBitwiseNot</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_bitwise_not.xhtml" title="Basic function to perform bitwise NOT by running CLBitwiseKernel. ">CLBitwiseNot</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BitwiseOr </td><td rowspan="2" style="width:200px;">Function to perform bitwise OR between 2 tensors. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_LOGICAL_OR </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_bitwise_or.xhtml" title="Basic function to run NEBitwiseOrKernel. ">NEBitwiseOr</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_bitwise_or.xhtml" title="Basic function to perform bitwise OR by running CLBitwiseKernel. ">CLBitwiseOr</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BitwiseXor </td><td rowspan="2" style="width:200px;">Function to perform bitwise XOR between 2 tensors. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_bitwise_xor.xhtml" title="Basic function to run NEBitwiseXorKernel. ">NEBitwiseXor</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_bitwise_xor.xhtml" title="Basic function to perform bitwise XOR by running CLBitwiseKernel. ">CLBitwiseXor</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">BoundingBoxTransform </td><td rowspan="2" style="width:200px;">Transform proposal bounding boxes to target bounding box using bounding box deltas. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_bounding_box_transform.xhtml" title="Basic function to run NEBoundingBoxTransformKernel. ">NEBoundingBoxTransform</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM16</td><td>QASYMM8</td><td>QASYMM16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_bounding_box_transform.xhtml" title="Basic function to run CLBoundingBoxTransformKernel. ">CLBoundingBoxTransform</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM16</td><td>QASYMM8</td><td>QASYMM16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Cast </td><td rowspan="2" style="width:200px;">Function to cast a tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CAST </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_cast.xhtml" title="Basic function to run cpu::kernels::CpuCastKernel. ">NECast</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>S16, S32, F32, F16 </td></tr>
<tr>
<td>QASYMM8</td><td>U16, S16, S32, F32, F16 </td></tr>
<tr>
<td>U8</td><td>U16, S16, S32, F32, F16 </td></tr>
<tr>
<td>U16</td><td>U8, U32 </td></tr>
<tr>
<td>S16</td><td>QASYMM8_SIGNED, U8, S32 </td></tr>
<tr>
<td>F16</td><td>QASYMM8_SIGNED, QASYMM8, F32, S32, U8 </td></tr>
<tr>
<td>S32</td><td>QASYMM8_SIGNED, QASYMM8, F16, F32, U8 </td></tr>
<tr>
<td>F32</td><td>QASYMM8_SIGNED, QASYMM8, BFLOAT16, F16, S32, U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_cast.xhtml" title="Basic function to run opencl::kernels::ClCastKernel. ">CLCast</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>S8, U16, S16, U32, S32, F16, F32 </td></tr>
<tr>
<td>U16</td><td>U8, S8, S16, U32, S32, F16, F32 </td></tr>
<tr>
<td>S16</td><td>U8, S8, U16, U32, S32, F16, F32 </td></tr>
<tr>
<td>U32</td><td>U8, S8, U16, S16, S32, F16, F32 </td></tr>
<tr>
<td>S32</td><td>U8, S8, U16, S16, U32, F16, F32 </td></tr>
<tr>
<td>F16</td><td>U8, S8, U16, S16, U32, F32 </td></tr>
<tr>
<td>F32</td><td>U8, S8, U16, S16, U32, F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ChannelShuffleLayer </td><td rowspan="2" style="width:200px;">Function to shuffle the channels of the input tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CHANNEL_SHUFFLE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_channel_shuffle_layer.xhtml" title="Basic function to run NEChannelShuffleLayerKernel. ">NEChannelShuffleLayer</a> </td><td><ul>
<li>
NCHW </li>
<li>
NHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_channel_shuffle_layer.xhtml" title="Basic function to run CLChannelShuffleLayerKernel. ">CLChannelShuffleLayer</a> </td><td><ul>
<li>
NCHW </li>
<li>
NHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">Comparison </td><td rowspan="1" style="width:200px;">Function to compare 2 tensors. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_EQUAL </li>
<li>
ANEURALNETWORKS_GREATER </li>
<li>
ANEURALNETWORKS_GREATER_EQUAL </li>
<li>
ANEURALNETWORKS_LESS </li>
<li>
ANEURALNETWORKS_LESS_EQUAL </li>
<li>
ANEURALNETWORKS_NOT_EQUAL </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_comparison.xhtml" title="Basic function to run CLComparisonKernel. ">CLComparison</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>All</td><td>All</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ConcatenateLayer </td><td rowspan="2" style="width:200px;">Function to concatenate tensors along a given axis. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONCATENATION </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_concatenate_layer.xhtml" title="Basic function to execute concatenate tensors along a given axis. ">NEConcatenateLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_concatenate_layer.xhtml" title="Basic function to execute concatenate tensors along a given axis. ">CLConcatenateLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ConvertFullyConnectedWeights </td><td rowspan="2" style="width:200px;">Function to transpose the weights for the fully connected layer. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_convert_fully_connected_weights.xhtml" title="Basic function to run cpu::kernels::CpuConvertFullyConnectedWeightsKernel. ">NEConvertFullyConnectedWeights</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_convert_fully_connected_weights.xhtml" title="Basic function to run an opencl::kernels::ClConvertFullyConnectedWeightsKernel. ">CLConvertFullyConnectedWeights</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ConvolutionLayer </td><td rowspan="2" style="width:200px;">Function to compute a convolution layer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_convolution_layer.xhtml" title="Basic function to simulate a convolution layer. ">NEConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_convolution_layer.xhtml" title="Basic function to compute the convolution layer. ">CLConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Conv3D </td><td rowspan="2" style="width:200px;">Function to compute a 3d convolution layer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONV_3D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_conv3_d.xhtml" title="Basic function to simulate a 3d convolution. ">NEConv3D</a> </td><td><ul>
<li>
NDHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_conv3_d.xhtml" title="Basic function to compute the convolution3d layer. ">CLConv3D</a> </td><td><ul>
<li>
NDHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Copy </td><td rowspan="2" style="width:200px;">Function to copy a tensor. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_copy.xhtml" title="Basic function to run cpu::kernels::CpuCopyKernel. ">NECopy</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_copy.xhtml" title="Basic function to run opencl::kernels::ClCopyKernel. ">CLCopy</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">Crop </td><td rowspan="1" style="width:200px;">Performs a copy of input tensor to the output tensor. </td><td rowspan="1"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_crop.xhtml" title="Basic function to run opencl::kernels::ClCropKernel. ">CLCrop</a> </td><td><ul>
<li>
NHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">CropResize </td><td rowspan="2" style="width:200px;">Function to perform cropping and resizing. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_crop_resize.xhtml" title="Function to perform cropping and resizing. ">NECropResize</a> </td><td><ul>
<li>
NHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>All</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_crop_resize.xhtml" title="Function to perform cropping and resizing. ">CLCropResize</a> </td><td><ul>
<li>
NHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>All</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">DeconvolutionLayer </td><td rowspan="2" style="width:200px;">Function to compute a deconvolution or transpose convolution. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_TRANSPOSE_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_deconvolution_layer.xhtml" title="Function to run the deconvolution layer. ">NEDeconvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_deconvolution_layer.xhtml" title="Basic function to compute the deconvolution layer. ">CLDeconvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">DeconvolutionLayerUpsample </td><td rowspan="1" style="width:200px;">Function to execute deconvolution upsample on OpenCL. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_TRANSPOSE_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_deconvolution_layer_upsample.xhtml" title="Basic function to execute deconvolution upsample on OpenCL. ">CLDeconvolutionLayerUpsample</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">DepthConvertLayer </td><td rowspan="2" style="width:200px;">Performs a down-scaling depth conversion. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_depth_convert_layer.xhtml" title="Basic function to run cpu::kernels::CpuCastKernel. ">NEDepthConvertLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>F16, F32 </td></tr>
<tr>
<td>U8</td><td>U16, S16, S32 </td></tr>
<tr>
<td>U16</td><td>U8, U32 </td></tr>
<tr>
<td>S16</td><td>U8, S32 </td></tr>
<tr>
<td>BFLOAT16</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>QASYMM8, F32 </td></tr>
<tr>
<td>F32</td><td>QASYMM8, F16, BFLOAT16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_depth_convert_layer.xhtml" title="Basic function to run opencl::kernels::ClCastKernel. ">CLDepthConvertLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>S8, U16, S16, U32, S32, F16, F32 </td></tr>
<tr>
<td>U16</td><td>U8, S8, S16, U32, S32, F16, F32 </td></tr>
<tr>
<td>S16</td><td>U8, S8, U16, U32, S32, F16, F32 </td></tr>
<tr>
<td>U32</td><td>U8, S8, U16, S16, S32, F16, F32 </td></tr>
<tr>
<td>S32</td><td>U8, S8, U16, S16, U32, F16, F32 </td></tr>
<tr>
<td>F16</td><td>U8, S8, U16, S16, U32, F32 </td></tr>
<tr>
<td>F32</td><td>U8, S8, U16, S16, U32, F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">DepthToSpaceLayer </td><td rowspan="2" style="width:200px;">Depth to Space transformation. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_DEPTH_TO_SPACE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_depth_to_space_layer.xhtml" title="Basic function to run NEDepthToSpaceLayerKernel. ">NEDepthToSpaceLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_depth_to_space_layer.xhtml" title="Basic function to run CLDepthToSpaceLayerKernel. ">CLDepthToSpaceLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">DepthwiseConvolutionLayer </td><td rowspan="2" style="width:200px;">Function to perform depthwise separable convolution. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_DEPTHWISE_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_depthwise_convolution_layer.xhtml" title="Function to execute a depthwise convolution. ">NEDepthwiseConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_depthwise_convolution_layer.xhtml" title="Function to execute a depthwise convolution. ">CLDepthwiseConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">DequantizationLayer </td><td rowspan="2" style="width:200px;">Function to dequantize the values in a tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_DEQUANTIZE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_dequantization_layer.xhtml" title="Basic function to run cpu::CpuDequantize that dequantizes an input tensor. ">NEDequantizationLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>F16, F32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>F16, F32 </td></tr>
<tr>
<td>QSYMM8_PER_CHANNEL</td><td>F16, F32 </td></tr>
<tr>
<td>QSYMM8</td><td>F16, F32 </td></tr>
<tr>
<td>QSYMM16</td><td>F16, F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_dequantization_layer.xhtml" title="Basic function to run opencl::ClDequantize that dequantizes an input tensor. ">CLDequantizationLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>F16, F32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>F16, F32 </td></tr>
<tr>
<td>QSYMM8_PER_CHANNEL</td><td>F16, F32 </td></tr>
<tr>
<td>QSYMM8</td><td>F16, F32 </td></tr>
<tr>
<td>QSYMM16</td><td>F16, F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">DetectionPostProcessLayer </td><td rowspan="1" style="width:200px;">Function to generate the detection output based on center size encoded boxes, class prediction and anchors by doing non maximum suppression (NMS). </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_DETECTION_POSTPROCESSING </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_detection_post_process_layer.xhtml" title="NE Function to generate the detection output based on center size encoded boxes, class prediction and...">NEDetectionPostProcessLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0 - src2</th><th>dst0 - dst3 </th></tr>
<tr>
<td>QASYMM8</td><td>F32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>F32 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">DirectConvolutionLayer </td><td rowspan="2" style="width:200px;">Function to compute direct convolution. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_direct_convolution_layer.xhtml" title="Function to run the direct convolution. ">NEDirectConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_direct_convolution_layer.xhtml" title="Basic function to execute direct convolution function: ">CLDirectConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">DirectDeconvolutionLayer </td><td rowspan="1" style="width:200px;">Function to run the deconvolution layer. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_TRANSPOSE_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_direct_deconvolution_layer.xhtml" title="Function to run the deconvolution layer. ">CLDirectDeconvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="13">ElementwiseOperations </td><td rowspan="13" style="width:200px;">Function to perform in Cpu: - Div - Max - Min - Pow - SquaredDiff - Comparisons (Equal, greater, greater_equal, less, less_equal, not_equal) Function to perform in CL: - Add - Sub - Div - Max - Min - Pow - SquaredDiff </td><td rowspan="13"><ul>
<li>
ANEURALNETWORKS_MAXIMUM </li>
<li>
ANEURALNETWORKS_MINIMUM </li>
<li>
ANEURALNETWORKS_POW </li>
<li>
ANEURALNETWORKS_DIV </li>
<li>
ANEURALNETWORKS_ADD </li>
<li>
ANEURALNETWORKS_SUB </li>
<li>
ANEURALNETWORKS_EQUAL </li>
<li>
ANEURALNETWORKS_GREATER </li>
<li>
ANEURALNETWORKS_GREATER_EQUAL </li>
<li>
ANEURALNETWORKS_LESS </li>
<li>
ANEURALNETWORKS_LESS_EQUAL </li>
<li>
ANEURALNETWORKS_NOT_EQUAL </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_elementwise_max.xhtml" title="Basic function to run cpu::kernels::CpuArithmeticKernel for max. ">NEElementwiseMax</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_elementwise_min.xhtml" title="Basic function to run cpu::kernels::CpuArithmeticKernel for min. ">NEElementwiseMin</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_elementwise_squared_diff.xhtml" title="Basic function to run cpu::kernels::CpuArithmeticKernel for squared difference. ">NEElementwiseSquaredDiff</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_elementwise_division.xhtml" title="Basic function to run cpu::kernels::CpuArithmeticKernel for division. ">NEElementwiseDivision</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_elementwise_power.xhtml" title="Basic function to run cpu::kernels::CpuArithmeticKernel for power. ">NEElementwisePower</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_elementwise_comparison.xhtml" title="Basic function to run cpu::kernels::CpuComparisonKernel. ">NEElementwiseComparison</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>U8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>U8 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>U8 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>U8 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>U8 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_arithmetic_addition.xhtml" title="Basic function to run opencl::kernels::ClSaturatedArithmeticKernel for addition. ">CLArithmeticAddition</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>U8</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_arithmetic_subtraction.xhtml" title="Basic function to run opencl::kernels::ClSaturatedArithmeticKernel for subtraction. ">CLArithmeticSubtraction</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>U8</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_arithmetic_division.xhtml" title="Basic function to run opencl::kernels::ClSaturatedArithmeticKernel for division. ">CLArithmeticDivision</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_elementwise_max.xhtml" title="Basic function to run opencl::kernels::ClArithmeticKernel for max. ">CLElementwiseMax</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>U32</td><td>U32</td><td>U32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_elementwise_min.xhtml" title="Basic function to run opencl::kernels::ClArithmeticKernel for min. ">CLElementwiseMin</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>U32</td><td>U32</td><td>U32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_elementwise_squared_diff.xhtml" title="Basic function to run opencl::kernels::ClArithmeticKernel for squared difference. ...">CLElementwiseSquaredDiff</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_elementwise_power.xhtml" title="Basic function to run opencl::kernels::ClArithmeticKernel for power. ">CLElementwisePower</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="8">ElementwiseUnaryLayer </td><td rowspan="8" style="width:200px;">Function to perform: - Rsqrt - Exp - Neg - Log - Abs - Round - Sin </td><td rowspan="8"><ul>
<li>
ANEURALNETWORKS_ABS </li>
<li>
ANEURALNETWORKS_EXP </li>
<li>
ANEURALNETWORKS_LOG </li>
<li>
ANEURALNETWORKS_NEG </li>
<li>
ANEURALNETWORKS_RSQRT </li>
<li>
ANEURALNETWORKS_SIN </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_elementwise_unary_layer.xhtml" title="Basic function to perform unary elementwise operations. ">NEElementwiseUnaryLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_rsqrt_layer.xhtml" title="Basic function to perform inverse square root on an input tensor. ">CLRsqrtLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_exp_layer.xhtml" title="Basic function to perform exponential on an input tensor. ">CLExpLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_neg_layer.xhtml" title="Basic function to negate an input tensor. ">CLNegLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_sin_layer.xhtml" title="Basic function to calculate sine of an input tensor. ">CLSinLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_log_layer.xhtml" title="Basic function to perform elementwise log on an input tensor. ">CLLogLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_abs_layer.xhtml" title="Basic function to get the absolute value of an input tensor. ">CLAbsLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_round_layer.xhtml" title="Basic function to get the round (to the nearest even) value of an input tensor. ">CLRoundLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">FFT1D </td><td rowspan="2" style="width:200px;">Fast Fourier Transform 1D. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_f_f_t1_d.xhtml" title="Basic function to execute one dimensional FFT. ">NEFFT1D</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_f_f_t1_d.xhtml" title="Basic function to execute one dimensional FFT. ">CLFFT1D</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">FFT2D </td><td rowspan="2" style="width:200px;">Fast Fourier Transform 2D. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_f_f_t2_d.xhtml" title="Basic function to execute two dimensional FFT. ">NEFFT2D</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_f_f_t2_d.xhtml" title="Basic function to execute two dimensional FFT. ">CLFFT2D</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">FFTConvolutionLayer </td><td rowspan="2" style="width:200px;">Fast Fourier Transform Convolution. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_f_f_t_convolution_layer.xhtml" title="Basic function to execute FFT-based convolution on CPU. ">NEFFTConvolutionLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_f_f_t_convolution_layer.xhtml" title="Basic function to execute FFT-based convolution on OpenCL. ">CLFFTConvolutionLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Fill </td><td rowspan="2" style="width:200px;">Set the values of a tensor with a given value. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_FILL </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_fill.xhtml" title="Basic function to run cpu::kernels::CpuFillKernel. ">NEFill</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_fill.xhtml" title="Basic function to run opencl::kernels::ClFillKernel. ">CLFill</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">FillBorder </td><td rowspan="1" style="width:200px;">Function to fill the borders within the XY-planes. </td><td rowspan="1"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_fill_border.xhtml" title="Basic function to run NEFillBorderKernel. ">NEFillBorder</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">FlattenLayer </td><td rowspan="2" style="width:200px;">Reshape a tensor to be 1D </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_RESHAPE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_flatten_layer.xhtml" title="Basic function to execute flatten layer kernel. ">NEFlattenLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_flatten_layer.xhtml" title="Basic function to execute flatten. ">CLFlattenLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Floor </td><td rowspan="2" style="width:200px;">Round the value to the lowest number. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_FLOOR </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_floor.xhtml" title="Basic function to run cpu::kernels::CpuFloorKernel. ">NEFloor</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_floor.xhtml" title="Basic function to run opencl::kernels::ClFloorKernel. ">CLFloor</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">FullyConnectedLayer </td><td rowspan="2" style="width:200px;">Function to perform a fully connected / dense layer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_FULLY_CONNECTED </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_fully_connected_layer.xhtml" title="Basic function to compute a Fully Connected layer. ">NEFullyConnectedLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_fully_connected_layer.xhtml" title="Basic function to compute a Fully Connected layer on OpenCL. ">CLFullyConnectedLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">FuseBatchNormalization </td><td rowspan="2" style="width:200px;">Function to fuse the batch normalization node to a preceding convolution node. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_fuse_batch_normalization.xhtml" title="Basic function to fuse the batch normalization node to a preceding convolution node. ">NEFuseBatchNormalization</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_fuse_batch_normalization.xhtml" title="Basic function to fuse the batch normalization node to a preceding convolution node. ">CLFuseBatchNormalization</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Gather </td><td rowspan="2" style="width:200px;">Performs the Gather operation along the chosen axis. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_GATHER </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_gather.xhtml" title="Basic function to run NEGatherKernel. ">NEGather</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_gather.xhtml" title="Basic function to run CLGatherKernel. ">CLGather</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">GEMM </td><td rowspan="2" style="width:200px;">General Matrix Multiplication. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_g_e_m_m.xhtml" title="Basic function to execute GEMM. ">NEGEMM</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>BFLOAT16</td><td>BFLOAT16</td><td>BFLOAT16</td><td>BFLOAT16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_g_e_m_m.xhtml" title="Basic function to execute GEMM on OpenCL. ">CLGEMM</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">GEMMConv2d </td><td rowspan="1" style="width:200px;">General Matrix Multiplication. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_g_e_m_m_conv2d.xhtml" title="Basic function to compute the convolution layer. ">NEGEMMConv2d</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>BFLOAT16</td><td>BFLOAT16</td><td>BFLOAT16</td><td>BFLOAT16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">GEMMConvolutionLayer </td><td rowspan="2" style="width:200px;">General Matrix Multiplication. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_g_e_m_m_convolution_layer.xhtml" title="Basic function to compute the convolution layer. ">NEGEMMConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>BFLOAT16</td><td>BFLOAT16</td><td>BFLOAT16</td><td>BFLOAT16 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_g_e_m_m_convolution_layer.xhtml" title="Basic function to compute the convolution layer. ">CLGEMMConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">GEMMDeconvolutionLayer </td><td rowspan="1" style="width:200px;">General Matrix Multiplication. </td><td rowspan="1"><ul>
<li>
ANEURALNETWORKS_TRANSPOSE_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_g_e_m_m_deconvolution_layer.xhtml" title="Function to run the deconvolution layer through a call to GEMM. ">CLGEMMDeconvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">GEMMLowpMatrixMultiplyCore </td><td rowspan="2" style="width:200px;">General Matrix Multiplication. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_g_e_m_m_lowp_matrix_multiply_core.xhtml" title="Function to run Gemm on quantized types. ">NEGEMMLowpMatrixMultiplyCore</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8</td><td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_g_e_m_m_lowp_matrix_multiply_core.xhtml" title="Basic function to execute GEMMLowpMatrixMultiplyCore on OpenCL. ">CLGEMMLowpMatrixMultiplyCore</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8_PER_CHANNEL</td><td>S32</td><td>S32 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QSYMM8</td><td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">GEMMLowpOutputStage </td><td rowspan="2" style="width:200px;">General Matrix Multiplication. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_g_e_m_m_lowp_output_stage.xhtml" title="Basic function to execute GEMMLowpQuantizeDown kernels. ">NEGEMMLowpOutputStage</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>S32</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>S32</td><td>S32</td><td>QSYMM16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_g_e_m_m_lowp_output_stage.xhtml" title="Basic function to execute GEMMLowpQuantizeDown kernels on CL. ">CLGEMMLowpOutputStage</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>S32</td><td>S32</td><td>QASYMM8 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>S32</td><td>S32</td><td>QSYMM16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">GenerateProposalsLayer </td><td rowspan="2" style="width:200px;">Function to generate proposals for a RPN (Region Proposal Network). </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_GENERATE_PROPOSALS </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_generate_proposals_layer.xhtml" title="Basic function to generate proposals for a RPN (Region Proposal Network) ">NEGenerateProposalsLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8</td><td>QSYMM16</td><td>QASYMM8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_generate_proposals_layer.xhtml" title="Basic function to generate proposals for a RPN (Region Proposal Network) ">CLGenerateProposalsLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QSYMM8</td><td>QSYMM16</td><td>QASYMM8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">InstanceNormalizationLayer </td><td rowspan="2" style="width:200px;">Function to perform a Instance normalization on a given axis. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_INSTANCE_NORMALIZATION </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_instance_normalization_layer.xhtml" title="Basic function to perform a Instance normalization. ">NEInstanceNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_instance_normalization_layer.xhtml" title="Basic function to perform a Instance normalization. ">CLInstanceNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">L2NormalizeLayer </td><td rowspan="2" style="width:200px;">Function to perform a L2 normalization on a given axis. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_L2_NORMALIZATION </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_l2_normalize_layer.xhtml" title="Basic function to perform a L2 normalization on a given axis. ">NEL2NormalizeLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_l2_normalize_layer.xhtml" title="Basic function to perform a L2 normalization on a given axis. ">CLL2NormalizeLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="3">Logical </td><td rowspan="3" style="width:200px;">Function to perform: - Logical AND - Logical OR - Logical NOT </td><td rowspan="3"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_logical_and.xhtml" title="Basic function to perform logical AND. ">NELogicalAnd</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_logical_or.xhtml" title="Basic function to perform logical OR. ">NELogicalOr</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_n_e_logical_not.xhtml" title="Basic function to perform logical NOT. ">NELogicalNot</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">LogicalAnd </td><td rowspan="1" style="width:200px;">Function to perform Logical AND. </td><td rowspan="1"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_logical_and.xhtml" title="Basic function to run arm_compute::opencl::kernels::ClLogicalBinaryKernel. ">CLLogicalAnd</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">LogicalOr </td><td rowspan="1" style="width:200px;">Function to perform Logical OR. </td><td rowspan="1"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_logical_or.xhtml" title="Basic function to run arm_compute::opencl::kernels::ClLogicalBinaryKernel. ">CLLogicalOr</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="1">LogicalNot </td><td rowspan="1" style="width:200px;">Function to perform Logical NOT. </td><td rowspan="1"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_c_l_logical_not.xhtml" title="Basic function to do logical NOT operation. ">CLLogicalNot</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">LSTMLayer </td><td rowspan="2" style="width:200px;">Function to perform a single time step in a Long Short-Term <a class="el" href="classarm__compute_1_1_memory.xhtml" title="CPU implementation of memory object. ">Memory</a> (LSTM) layer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_LSTM </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_l_s_t_m_layer.xhtml" title="Basic function to run NELSTMLayer. ">NELSTMLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0 - src13</th><th>dst0 - dst3 </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_l_s_t_m_layer.xhtml" title="This function performs a single time step in a Long Short-Term Memory (LSTM) layer. ">CLLSTMLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0 - src13</th><th>dst0 - dst3 </th></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">LSTMLayerQuantized </td><td rowspan="2" style="width:200px;">Function to perform quantized LSTM (Long Short-Term <a class="el" href="classarm__compute_1_1_memory.xhtml" title="CPU implementation of memory object. ">Memory</a>) </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_QUANTIZED_LSTM </li>
<li>
ANEURALNETWORKS_QUANTIZED_16BIT_LSTM </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_l_s_t_m_layer_quantized.xhtml" title="Basic function to run NELSTMLayerQuantized. ">NELSTMLayerQuantized</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0 - src8</th><th>src9 - src12</th><th>src13</th><th>src14</th><th>dst0</th><th>dst1 </th></tr>
<tr>
<td>QASYMM8</td><td>S32</td><td>QSYMM16</td><td>QASYMM8</td><td>QSYMM16</td><td>QASYMM8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_l_s_t_m_layer_quantized.xhtml" title="Basic function to run CLLSTMLayerQuantized. ">CLLSTMLayerQuantized</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0 - src8</th><th>src9 - src12</th><th>src13</th><th>src14</th><th>dst0</th><th>dst1 </th></tr>
<tr>
<td>QASYMM8</td><td>S32</td><td>QSYMM16</td><td>QASYMM8</td><td>QSYMM16</td><td>QASYMM8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">MaxUnpoolingLayer </td><td rowspan="2" style="width:200px;">Function to perform MaxUnpooling. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_max_unpooling_layer.xhtml" title="Function to perform MaxUnpooling. ">NEMaxUnpoolingLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_max_unpooling_layer.xhtml" title="Function to perform MaxUnpooling. ">CLMaxUnpoolingLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">MeanStdDevNormalizationLayer </td><td rowspan="2" style="width:200px;">Function to execute mean and standard deviation normalization. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_mean_std_dev_normalization_layer.xhtml" title="Basic function to execute mean and standard deviation normalization by calling NEMeanStdDevNormalizat...">NEMeanStdDevNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_mean_std_dev_normalization_layer.xhtml" title="Basic function to execute mean and standard deviation normalization by calling CLMeanStdDevNormalizat...">CLMeanStdDevNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">NormalizationLayer </td><td rowspan="2" style="width:200px;">Function to compute normalization layer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_LOCAL_RESPONSE_NORMALIZATION </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_normalization_layer.xhtml" title="Basic function to compute a normalization layer. ">NENormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_normalization_layer.xhtml" title="Basic function to compute a normalization layer. ">CLNormalizationLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">PadLayer </td><td rowspan="2" style="width:200px;">Function to pad a tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_PAD </li>
<li>
ANEURALNETWORKS_PAD_V2 </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_pad_layer.xhtml" title="Basic function to pad a tensor. ">NEPadLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_pad_layer.xhtml" title="Basic function to pad a tensor. ">CLPadLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Permute </td><td rowspan="2" style="width:200px;">Function to transpose an ND tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_TRANSPOSE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_permute.xhtml" title="Basic function to run cpu::kernels::CpuPermuteKernel. ">NEPermute</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_permute.xhtml" title="Basic function to execute an opencl::kernels::ClPermuteKernel. ">CLPermute</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">PixelWiseMultiplication </td><td rowspan="2" style="width:200px;">Function to perform a multiplication. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_MUL </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_pixel_wise_multiplication.xhtml" title="Basic function to run cpu::CpuMul. ">NEPixelWiseMultiplication</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>S32 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>U8</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>S32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_pixel_wise_multiplication.xhtml" title="Basic function to run opencl::ClMul. ">CLPixelWiseMultiplication</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>QASYMM16 </td></tr>
<tr>
<td>QSYMM16</td><td>QSYMM16</td><td>S32 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>U8 </td></tr>
<tr>
<td>U8</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>U8</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>U8</td><td>S16 </td></tr>
<tr>
<td>S16</td><td>S16</td><td>S16 </td></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>S32</td><td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">PoolingLayer </td><td rowspan="2" style="width:200px;">Function to perform pooling with the specified pooling operation. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_AVERAGE_POOL_2D </li>
<li>
ANEURALNETWORKS_L2_POOL_2D </li>
<li>
ANEURALNETWORKS_MAX_POOL_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_pooling_layer.xhtml" title="Basic function to simulate a pooling layer with the specified pooling operation. ">NEPoolingLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_pooling_layer.xhtml" title="Basic function to run opencl::ClPool2d. ">CLPoolingLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">PReluLayer </td><td rowspan="2" style="width:200px;">Function to compute the activation layer with the PRELU activation function. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_PRELU </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_p_relu_layer.xhtml" title="Basic function to run cpu::kernels::CpuArithmeticKernel for PRELU. ">NEPReluLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_p_relu_layer.xhtml" title="Basic function to run opencl::kernels::ClArithmeticKernel for PRELU. ">CLPReluLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">PriorBoxLayer </td><td rowspan="2" style="width:200px;">Function to compute prior boxes and clip. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_prior_box_layer.xhtml" title="Basic function to run NEPriorBoxLayerKernel. ">NEPriorBoxLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_prior_box_layer.xhtml" title="Basic function to run CLPriorBoxLayerKernel. ">CLPriorBoxLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">QLSTMLayer </td><td rowspan="2" style="width:200px;">Function to perform quantized LSTM (Long Short-Term <a class="el" href="classarm__compute_1_1_memory.xhtml" title="CPU implementation of memory object. ">Memory</a>). </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_QUANTIZED_LSTM </li>
<li>
ANEURALNETWORKS_QUANTIZED_16BIT_LSTM </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_q_l_s_t_m_layer.xhtml" title="Basic function to run NEQLSTMLayer. ">NEQLSTMLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1 - src6</th><th>src7 -src9</th><th>src10</th><th>src11</th><th>dst0</th><th>dst1 - dst2 </th></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8</td><td>S32</td><td>QSYMM16</td><td>QASYMM8_SIGNED</td><td>QSYMM16</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_q_l_s_t_m_layer.xhtml" title="Basic function to run CLQLSTMLayer. ">CLQLSTMLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1 - src6</th><th>src7 -src9</th><th>src10</th><th>src11</th><th>dst0</th><th>dst1 - dst2 </th></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8</td><td>S32</td><td>QSYMM16</td><td>QASYMM8_SIGNED</td><td>QSYMM16</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">QuantizationLayer </td><td rowspan="2" style="width:200px;">Function to perform quantization layer </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_QUANTIZE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_quantization_layer.xhtml" title="Basic function to run a quantization layer using cpu::CpuQuantize. ">NEQuantizationLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
<tr>
<td>F16</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
<tr>
<td>F32</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_quantization_layer.xhtml" title="Basic function to simulate a quantization layer. ">CLQuantizationLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
<tr>
<td>F16</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
<tr>
<td>F32</td><td>QASYMM8, QASYMM8_SIGNED, QASYMM16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Range </td><td rowspan="2" style="width:200px;">Function to generates a sequence of numbers starting from START and extends by increments of 'STEP' up to but not including 'END'. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_range.xhtml" title="Basic function to run NERangeKernel. ">NERange</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>dst </th></tr>
<tr>
<td>U8 </td></tr>
<tr>
<td>S8 </td></tr>
<tr>
<td>U16 </td></tr>
<tr>
<td>S16 </td></tr>
<tr>
<td>U32 </td></tr>
<tr>
<td>S32 </td></tr>
<tr>
<td>F16 </td></tr>
<tr>
<td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_range.xhtml" title="Basic function to run CLRangeKernel. ">CLRange</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>dst </th></tr>
<tr>
<td>U8 </td></tr>
<tr>
<td>S8 </td></tr>
<tr>
<td>QASYMM8 </td></tr>
<tr>
<td>U16 </td></tr>
<tr>
<td>S16 </td></tr>
<tr>
<td>U32 </td></tr>
<tr>
<td>S32 </td></tr>
<tr>
<td>F16 </td></tr>
<tr>
<td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ReduceMean </td><td rowspan="2" style="width:200px;">Function to perform reduce mean operation. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_MEAN </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_reduce_mean.xhtml" title="Basic function to perform reduce operation. ">NEReduceMean</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_reduce_mean.xhtml" title="Basic function to perform reduce operation. ">CLReduceMean</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ReductionOperation </td><td rowspan="2" style="width:200px;">Function to perform reduce with the following operations - ARG_IDX_MAX: Index of the max value - ARG_IDX_MIN: Index of the min value - MEAN_SUM: Mean of sum - PROD: Product - SUM_SQUARE: Sum of squares - SUM: Sum - MIN: Min - MAX: Max </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_REDUCE_ALL </li>
<li>
ANEURALNETWORKS_REDUCE_ANY </li>
<li>
ANEURALNETWORKS_REDUCE_MAX </li>
<li>
ANEURALNETWORKS_REDUCE_MIN </li>
<li>
ANEURALNETWORKS_REDUCE_PROD </li>
<li>
ANEURALNETWORKS_REDUCE_SUM </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_reduction_operation.xhtml" title="Basic function to simulate a reduction operation. ">NEReductionOperation</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_reduction_operation.xhtml" title="Perform reduction operation. ">CLReductionOperation</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>S32</td><td>S32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ReorgLayer </td><td rowspan="2" style="width:200px;">Performs a reorganization layer of input tensor to the output tensor. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_reorg_layer.xhtml" title="Basic function to run NEReorgLayerKernel. ">NEReorgLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_reorg_layer.xhtml">CLReorgLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ReshapeLayer </td><td rowspan="2" style="width:200px;">Function to reshape a tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_RESHAPE </li>
<li>
ANEURALNETWORKS_SQUEEZE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_reshape_layer.xhtml" title="Basic function to run cpu::kernels::CpuReshapeKernel. ">NEReshapeLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_reshape_layer.xhtml" title="Basic function to run opencl::kernels::ClReshapeKernel. ">CLReshapeLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Reverse </td><td rowspan="2" style="width:200px;">Function to reverse tensor according to axis. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_reverse.xhtml" title="Basic function to run NEReverseKernel. ">NEReverse</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>All</td><td>U32</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_reverse.xhtml" title="Basic function to run CLReverseKernel. ">CLReverse</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>All</td><td>U32</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">RNNLayer </td><td rowspan="2" style="width:200px;">Function to perform recurrent neural network layer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_RNN </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_r_n_n_layer.xhtml" title="Basic function to run NERNNLayer. ">NERNNLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>src3</th><th>dst0</th><th>dst1 </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_r_n_n_layer.xhtml" title="Basic function to run CLRNNLayer. ">CLRNNLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>src3</th><th>dst0</th><th>dst1 </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ROIAlignLayer </td><td rowspan="2" style="width:200px;">Function to perform ROI alignment. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_ROI_ALIGN </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_r_o_i_align_layer.xhtml" title="Basic function to run NEROIAlignLayerKernel. ">NEROIAlignLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM16</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM16</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_r_o_i_align_layer.xhtml" title="Basic function to run CLROIAlignLayerKernel. ">CLROIAlignLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>QASYMM16</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM16</td><td>QASYMM8_SIGNED </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">ROIPoolingLayer </td><td rowspan="2" style="width:200px;">Function to perform ROI pooling. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_ROI_POOLING </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_r_o_i_pooling_layer.xhtml" title="Basic function to run NEROIPoolingLayerKernel. ">NEROIPoolingLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F32</td><td>U16</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>U16</td><td>QASYMM8 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_r_o_i_pooling_layer.xhtml" title="Basic function to run CLROIPoolingLayerKernel. ">CLROIPoolingLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>dst </th></tr>
<tr>
<td>F16</td><td>U16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>U16</td><td>F32 </td></tr>
<tr>
<td>QASYMM8</td><td>U16</td><td>QASYMM8 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Scale </td><td rowspan="2" style="width:200px;">Function to perform resize a tensor using to interpolate: - Bilinear - Nearest neighbor </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_RESIZE_BILINEAR </li>
<li>
ANEURALNETWORKS_RESIZE_NEAREST_NEIGHBOR </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_scale.xhtml" title="Basic function to compute Scale. ">NEScale</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_scale.xhtml" title="Basic function to run opencl::ClScale. ">CLScale</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
<tr>
<td>U8</td><td>U8 </td></tr>
<tr>
<td>S16</td><td>S16 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Select </td><td rowspan="2" style="width:200px;">Function to select values from 2 tensors depending on an input tensor of booleans. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_SELECT </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_select.xhtml" title="Basic function to run NESelect. ">NESelect</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>U8</td><td>All</td><td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_select.xhtml" title="Basic function to run CLSelect. ">CLSelect</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>U8</td><td>All</td><td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Slice </td><td rowspan="2" style="width:200px;">Function to perform tensor slicing. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_SLICE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_slice.xhtml" title="Basic function to perform tensor slicing. ">NESlice</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_slice.xhtml" title="Basic function to perform tensor slicing. ">CLSlice</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">SoftmaxLayer </td><td rowspan="2" style="width:200px;">Function to compute a SoftmaxLayer and a Log SoftmaxLayer. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_LOG_SOFTMAX </li>
<li>
ANEURALNETWORKS_SOFTMAX </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_softmax_layer_generic.xhtml" title="Basic function to compute a SoftmaxLayer and a Log SoftmaxLayer. ">NESoftmaxLayerGeneric</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_softmax_layer_generic.xhtml" title="Basic function to compute a SoftmaxLayer. ">CLSoftmaxLayerGeneric</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>QASYMM8</td><td>QASYMM8 </td></tr>
<tr>
<td>QASYMM8_SIGNED</td><td>QASYMM8_SIGNED </td></tr>
<tr>
<td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">SpaceToBatchLayer </td><td rowspan="2" style="width:200px;">Function to divide a tensor spatially. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_SPACE_TO_BATCH_ND </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_space_to_batch_layer.xhtml" title="Basic function to spatial divide a tensor. ">NESpaceToBatchLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>All</td><td>S32</td><td>S32</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_space_to_batch_layer.xhtml" title="Basic function to spatial divide a tensor. ">CLSpaceToBatchLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>All</td><td>S32</td><td>S32</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">SpaceToDepthLayer </td><td rowspan="2" style="width:200px;">Function to rearrange blocks of spatial data into depth. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_SPACE_TO_DEPTH </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_space_to_depth_layer.xhtml" title="Basic function to run NESpaceToDepthLayerKernel. ">NESpaceToDepthLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_space_to_depth_layer.xhtml" title="Basic function to run CLSpaceToDepthLayerKernel. ">CLSpaceToDepthLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Split </td><td rowspan="2" style="width:200px;">Function to split a tensor along a given axis. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_SPLIT </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_split.xhtml" title="Basic function to split a tensor along a given axis. ">NESplit</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_split.xhtml" title="Basic function to split a tensor along a given axis. ">CLSplit</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">StackLayer </td><td rowspan="2" style="width:200px;">Function to stack tensors along an axis. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_stack_layer.xhtml" title="Basic function to stack tensors along an axis. ">NEStackLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_stack_layer.xhtml" title="Basic function to stack tensors along an axis. ">CLStackLayer</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">StridedSlice </td><td rowspan="2" style="width:200px;">Function to extract a strided slice of a tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_STRIDED_SLICE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_strided_slice.xhtml" title="Basic function to run NEStridedSliceKernel. ">NEStridedSlice</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_strided_slice.xhtml" title="Basic function to run CLStridedSliceKernel. ">CLStridedSlice</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Tile </td><td rowspan="2" style="width:200px;">Function to construct a tensor by tiling a given tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_TILE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_tile.xhtml" title="Basic function to run NETileKernel. ">NETile</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_tile.xhtml" title="Basic function to run CLTileKernel. ">CLTile</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Transpose </td><td rowspan="2" style="width:200px;">Function to transpose a 2D tensor. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_TRANSPOSE </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_transpose.xhtml" title="Basic function to run cpu::kernels::CpuTransposeKernel. ">NETranspose</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_transpose.xhtml" title="Basic function to execute an opencl::kernels::ClTransposeKernel. ">CLTranspose</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">Unstack </td><td rowspan="2" style="width:200px;">Function to unpack a rank-R tensor into rank-(R-1) tensors. </td><td rowspan="2"><ul>
<li>
n/a </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_unstack.xhtml" title="Basic function to unpack a rank-R tensor into rank-(R-1) tensors. ">NEUnstack</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_unstack.xhtml" title="Basic function to unpack a rank-R tensor into rank-(R-1) tensors. ">CLUnstack</a> </td><td><ul>
<li>
All </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src</th><th>dst </th></tr>
<tr>
<td>All</td><td>All </td></tr>
</table>
</td></tr>
<tr>
<td rowspan="2">WinogradConvolutionLayer </td><td rowspan="2" style="width:200px;">Function to do Winograd Convolution. </td><td rowspan="2"><ul>
<li>
ANEURALNETWORKS_CONV_2D </li>
</ul>
</td><td><a class="el" href="classarm__compute_1_1_n_e_winograd_convolution_layer.xhtml" title="Basic function to simulate a convolution layer. ">NEWinogradConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
<tr>
<td><a class="el" href="classarm__compute_1_1_c_l_winograd_convolution_layer.xhtml" title="Basic function to execute Winograd-based convolution on OpenCL. ">CLWinogradConvolutionLayer</a> </td><td><ul>
<li>
NHWC </li>
<li>
NCHW </li>
</ul>
</td><td><table class="doxtable">
<tr>
<th>src0</th><th>src1</th><th>src2</th><th>dst </th></tr>
<tr>
<td>F16</td><td>F16</td><td>F16</td><td>F16 </td></tr>
<tr>
<td>F32</td><td>F32</td><td>F32</td><td>F32 </td></tr>
</table>
</td></tr>
</table>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Sat Feb 26 2022 12:27:44 for Compute Library by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.13 </li>
  </ul>
</div>
</body>
</html>
