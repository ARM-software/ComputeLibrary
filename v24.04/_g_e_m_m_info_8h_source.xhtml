<!-- HTML header for doxygen 1.8.15-->
<!-- Remember to use version doxygen 1.8.15 +-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.17"/>
<meta name="robots" content="NOINDEX, NOFOLLOW" /> <!-- Prevent indexing by search engines -->
<title>Compute Library: arm_compute/function_info/GEMMInfo.h Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <img alt="Compute Library" src="https://raw.githubusercontent.com/ARM-software/ComputeLibrary/gh-pages/ACL_logo.png" style="max-width: 100%;margin-top: 15px;margin-left: 10px"/>
  <td style="padding-left: 0.5em;">
   <div id="projectname">
   &#160;<span id="projectnumber">24.04</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.17 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(document).ready(function(){initNavTree('_g_e_m_m_info_8h_source.xhtml',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">GEMMInfo.h</div>  </div>
</div><!--header-->
<div class="contents">
<a href="_g_e_m_m_info_8h.xhtml">Go to the documentation of this file.</a><div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;<span class="comment">/*</span></div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;<span class="comment"> * Copyright (c) 2016-2024 Arm Limited.</span></div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;<span class="comment"> *</span></div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;<span class="comment"> * SPDX-License-Identifier: MIT</span></div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;<span class="comment"> *</span></div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;<span class="comment"> * Permission is hereby granted, free of charge, to any person obtaining a copy</span></div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;<span class="comment"> * of this software and associated documentation files (the &quot;Software&quot;), to</span></div>
<div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;<span class="comment"> * deal in the Software without restriction, including without limitation the</span></div>
<div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;<span class="comment"> * rights to use, copy, modify, merge, publish, distribute, sublicense, and/or</span></div>
<div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;<span class="comment"> * sell copies of the Software, and to permit persons to whom the Software is</span></div>
<div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;<span class="comment"> * furnished to do so, subject to the following conditions:</span></div>
<div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;<span class="comment"> *</span></div>
<div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;<span class="comment"> * The above copyright notice and this permission notice shall be included in all</span></div>
<div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;<span class="comment"> * copies or substantial portions of the Software.</span></div>
<div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;<span class="comment"> *</span></div>
<div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;<span class="comment"> * THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span></div>
<div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;<span class="comment"> * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</span></div>
<div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;<span class="comment"> * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE</span></div>
<div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;<span class="comment"> * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</span></div>
<div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;<span class="comment"> * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,</span></div>
<div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;<span class="comment"> * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE</span></div>
<div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;<span class="comment"> * SOFTWARE.</span></div>
<div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;<span class="preprocessor">#ifndef ACL_ARM_COMPUTE_FUNCTION_INFO_GEMMINFO_H</span></div>
<div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;<span class="preprocessor">#define ACL_ARM_COMPUTE_FUNCTION_INFO_GEMMINFO_H</span></div>
<div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160; </div>
<div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="_core_types_8h.xhtml">arm_compute/core/CoreTypes.h</a>&quot;</span></div>
<div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;<span class="preprocessor">#include &quot;<a class="code" href="_activation_layer_info_8h.xhtml">arm_compute/function_info/ActivationLayerInfo.h</a>&quot;</span></div>
<div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160; </div>
<div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;<span class="preprocessor">#include &lt;vector&gt;</span></div>
<div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160; </div>
<div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;<span class="keyword">namespace </span><a class="code" href="namespacearm__compute.xhtml">arm_compute</a></div>
<div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;{</div>
<div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;<span class="keyword">class </span>ITensorInfo;<span class="comment"></span></div>
<div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;<span class="comment">/** GEMMLowp output stage type */</span></div>
<div class="line"><a name="l00036"></a><span class="lineno"><a class="line" href="namespacearm__compute.xhtml#a5558e2cc22f7f4771653d992c8ad8864">   36</a></span>&#160;<span class="keyword">enum class</span> <a class="code" href="namespacearm__compute.xhtml#a5558e2cc22f7f4771653d992c8ad8864">GEMMLowpOutputStageType</a></div>
<div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;{</div>
<div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;    <a class="code" href="namespacearm__compute.xhtml#a5558e2cc22f7f4771653d992c8ad8864ab50339a10e1de285ac99d4c3990b8693">NONE</a>,                     <span class="comment">/**&lt; No quantization */</span></div>
<div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;    <a class="code" href="namespacearm__compute.xhtml#a5558e2cc22f7f4771653d992c8ad8864a079e2ddc95b344b5cb0188bed9a80d8b">QUANTIZE_DOWN</a>,            <span class="comment">/**&lt; Quantize using an integer multiplication */</span></div>
<div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;    <a class="code" href="namespacearm__compute.xhtml#a5558e2cc22f7f4771653d992c8ad8864ab300cae200f67712c1eb9234e28158ca">QUANTIZE_DOWN_FIXEDPOINT</a>, <span class="comment">/**&lt; Quantize using a fixed point multiplication */</span></div>
<div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;    <a class="code" href="namespacearm__compute.xhtml#a5558e2cc22f7f4771653d992c8ad8864aad664ac5008f135e38afeb391e524f9c">QUANTIZE_DOWN_FLOAT</a>       <span class="comment">/**&lt; Quantize using a floating point multiplication */</span></div>
<div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;};</div>
<div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;<span class="comment"></span> </div>
<div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;<span class="comment">/** GEMMLowp output stage info */</span></div>
<div class="line"><a name="l00045"></a><span class="lineno"><a class="line" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml">   45</a></span>&#160;<span class="keyword">struct </span><a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml">GEMMLowpOutputStageInfo</a></div>
<div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;{</div>
<div class="line"><a name="l00047"></a><span class="lineno"><a class="line" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a6e019ad85979fd73c74f97e5483faf35">   47</a></span>&#160;    <a class="code" href="namespacearm__compute.xhtml#a5558e2cc22f7f4771653d992c8ad8864">GEMMLowpOutputStageType</a> <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a6e019ad85979fd73c74f97e5483faf35">type</a>{<a class="code" href="namespacearm__compute.xhtml#a5558e2cc22f7f4771653d992c8ad8864ab50339a10e1de285ac99d4c3990b8693">GEMMLowpOutputStageType::NONE</a>}; <span class="comment">/**&lt; GEMMLowp output stage type */</span></div>
<div class="line"><a name="l00048"></a><span class="lineno"><a class="line" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a01934c5087f5193aaf3ea9bf41d1a8dc">   48</a></span>&#160;    int32_t                 <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a01934c5087f5193aaf3ea9bf41d1a8dc">gemmlowp_offset</a>{0}; <span class="comment">/**&lt; GEMMLowp output stage offset used for quantizing to QASYMM8 */</span></div>
<div class="line"><a name="l00049"></a><span class="lineno"><a class="line" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a1cfb92f1c287bf099c3fca0ef0391a2b">   49</a></span>&#160;    int32_t <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a1cfb92f1c287bf099c3fca0ef0391a2b">gemmlowp_multiplier</a>{0};             <span class="comment">/**&lt; GEMMLowp output stage multiplier used for quantizing to QASYMM8 */</span></div>
<div class="line"><a name="l00050"></a><span class="lineno"><a class="line" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a3f0613aeb69c326e7d8ffb34b44fae94">   50</a></span>&#160;    int32_t <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a3f0613aeb69c326e7d8ffb34b44fae94">gemmlowp_shift</a>{0};                  <span class="comment">/**&lt; GEMMLowp output stage shift used for quantizing to uint8 */</span></div>
<div class="line"><a name="l00051"></a><span class="lineno"><a class="line" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a155d27c75f14a82a74e5039c9657c8eb">   51</a></span>&#160;    int32_t <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a155d27c75f14a82a74e5039c9657c8eb">gemmlowp_min_bound</a>{</div>
<div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;        <a class="code" href="namespacearm__compute_1_1support_1_1cpp11.xhtml#a73e352c61baaf9c1178da2d30105b04e">std::numeric_limits&lt;int32_t&gt;::</a></div>
<div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;<a class="code" href="namespacearm__compute_1_1support_1_1cpp11.xhtml#a73e352c61baaf9c1178da2d30105b04e">            lowest</a>()}; <span class="comment">/**&lt; GEMMLowp min value used to saturate down the output result before converting back to QASYMM8 */</span></div>
<div class="line"><a name="l00054"></a><span class="lineno"><a class="line" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a6db94040329f1dedcd348ec7de072e2a">   54</a></span>&#160;    int32_t <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a6db94040329f1dedcd348ec7de072e2a">gemmlowp_max_bound</a>{</div>
<div class="line"><a name="l00055"></a><span class="lineno">   55</span>&#160;        std::numeric_limits&lt;int32_t&gt;::</div>
<div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;            max()}; <span class="comment">/**&lt; GEMMLowp max value used to saturate down the output result before converting back to QASYMM8 */</span></div>
<div class="line"><a name="l00057"></a><span class="lineno"><a class="line" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#ae5bd6bebbc0c7ebd9e7dbfd47d939c2a">   57</a></span>&#160;    std::vector&lt;int32_t&gt; <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#ae5bd6bebbc0c7ebd9e7dbfd47d939c2a">gemmlowp_multipliers</a>{}; <span class="comment">/**&lt; GEMMLowp output stage multiplier used for quantizing to QASYMM8 */</span></div>
<div class="line"><a name="l00058"></a><span class="lineno"><a class="line" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#ab269b182588a158cd256f9d4bb2a00dd">   58</a></span>&#160;    std::vector&lt;int32_t&gt; <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#ab269b182588a158cd256f9d4bb2a00dd">gemmlowp_shifts</a>{};      <span class="comment">/**&lt; GEMMLowp output stage multiplier used for quantizing to QASYMM8 */</span></div>
<div class="line"><a name="l00059"></a><span class="lineno"><a class="line" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#adc26274f64ea797e55d06e6a813f30f9">   59</a></span>&#160;    <span class="keywordtype">float</span>    <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#adc26274f64ea797e55d06e6a813f30f9">gemmlowp_real_multiplier</a>{0}; <span class="comment">/**&lt; GEMMLowp output stage real multiplier used for quantizing to QASYMM8 */</span></div>
<div class="line"><a name="l00060"></a><span class="lineno"><a class="line" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a94e1801be6c3d9d6645c694d7e280cda">   60</a></span>&#160;    <span class="keywordtype">bool</span>     <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a94e1801be6c3d9d6645c694d7e280cda">is_quantized_per_channel</a>{<span class="keyword">false</span>}; <span class="comment">/**&lt; GEMMLowp quantized per-channel flag */</span></div>
<div class="line"><a name="l00061"></a><span class="lineno"><a class="line" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#ab233758aca2751c6e71a2f79baf7b92a">   61</a></span>&#160;    <a class="code" href="namespacearm__compute.xhtml#ad8ed01ff3ff33333d8e19db4d2818bb6">DataType</a> <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#ab233758aca2751c6e71a2f79baf7b92a">output_data_type</a>{</div>
<div class="line"><a name="l00062"></a><span class="lineno">   62</span>&#160;        <a class="code" href="namespacearm__compute.xhtml#ad8ed01ff3ff33333d8e19db4d2818bb6a696b031073e74bf2cb98e5ef201d4aa3">DataType::UNKNOWN</a>}; <span class="comment">/**&lt; Output tensor data type to use if the output is not initialized */</span></div>
<div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;};<span class="comment"></span></div>
<div class="line"><a name="l00064"></a><span class="lineno">   64</span>&#160;<span class="comment">/** GEMM information class. This class stores the necessary information to compute GEMM functions</span></div>
<div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;<span class="comment"> *</span></div>
<div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;<span class="comment"> * This object also contains the information about how matrix A and matrix B have been reshaped</span></div>
<div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;<span class="comment"> *</span></div>
<div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00069"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml">   69</a></span>&#160;<span class="keyword">class </span><a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml">GEMMInfo</a></div>
<div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;{</div>
<div class="line"><a name="l00071"></a><span class="lineno">   71</span>&#160;<span class="keyword">public</span>:<span class="comment"></span></div>
<div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;<span class="comment">    /** Default constructor */</span></div>
<div class="line"><a name="l00073"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ae0c4636f1099428785df0463f7151889">   73</a></span>&#160;    <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ae0c4636f1099428785df0463f7151889">GEMMInfo</a>() noexcept</div>
<div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;        : _is_a_reshaped(false),</div>
<div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;          _is_b_reshaped(false),</div>
<div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;          _reshape_b_only_on_first_run(true),</div>
<div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;          _depth_output_gemm3d(0),</div>
<div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;          _reinterpret_input_as_3d(false),</div>
<div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;          _retain_internal_weights(false),</div>
<div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;          _gemmlowp_output_stage(),</div>
<div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;          _fast_math(false),</div>
<div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;          _fp_mixed_precision(false),</div>
<div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;          _broadcast_bias(false),</div>
<div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;          _pretranspose_A(false),</div>
<div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;          _pretranspose_B(false),</div>
<div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;          _activation_info(),</div>
<div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;          _fixed_format(false),</div>
<div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;          _weight_format(<a class="code" href="namespacearm__compute.xhtml">arm_compute</a>::<a class="code" href="namespacearm__compute.xhtml#a23ab0e5c6b5d13e084628686c4f282d5">WeightFormat</a>::UNSPECIFIED),</div>
<div class="line"><a name="l00089"></a><span class="lineno">   89</span>&#160;          _accumulate(false)</div>
<div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;    {</div>
<div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;    }<span class="comment"></span></div>
<div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;<span class="comment">    /** Constructor</span></div>
<div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;<span class="comment">     * @param[in] is_a_reshaped               True if the matrix A has been reshaped</span></div>
<div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;<span class="comment">     * @param[in] is_b_reshaped               True if the matrix B has been reshaped</span></div>
<div class="line"><a name="l00096"></a><span class="lineno">   96</span>&#160;<span class="comment">     * @param[in] reshape_b_only_on_first_run Reshape matrix B only for the first run</span></div>
<div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;<span class="comment">     * @param[in] depth_output_gemm3d         (Optional) Depth (third dimension) of the output tensor to be used with the GEMM3D kernel</span></div>
<div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;<span class="comment">     *                                        If 0 the output will not be reinterpreted as 3D. Default 0</span></div>
<div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;<span class="comment">     * @param[in] reinterpret_input_as_3d     (Optional) Reinterpret the input as 3D tensor. (i.e. this flag should be set to true when GEMM is used</span></div>
<div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;<span class="comment">     *                                        to perform 1x1 convolutions with the NHWC data layout)</span></div>
<div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;<span class="comment">     * @param[in] retain_internal_weights     (Optional) Retain the weights tensor from previous run</span></div>
<div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;<span class="comment">     * @param[in] gemmlowp_output_stage       (Optional) GEMMLowp Output stage info</span></div>
<div class="line"><a name="l00103"></a><span class="lineno">  103</span>&#160;<span class="comment">     * @param[in] fp_mixed_precision          (Optional) Use wider accumulators (32 bit instead of 16 for FP16) to improve accuracy.</span></div>
<div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;<span class="comment">     * @param[in] fast_math                   (Optional) Use a data type of shorter width to improve performance</span></div>
<div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;<span class="comment">     * @param[in] broadcast_bias              (Optional) Broadcast the shape of the bias tensor from a vector to a matrix.</span></div>
<div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;<span class="comment">     * @param[in] activation_info             (Optional) Activation to apply after the matrix multiplication</span></div>
<div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;<span class="comment">     * @param[in] fixed_format                (Optional) Specify the selection of fixed format kernels for variable weights support in GEMM. These kernels expect the weights tensor to be in amemory format that is fixed by the kernel itself. For more information, see arm_compute::WeightFormat.</span></div>
<div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;<span class="comment">     * @param[in] weight_format               (Optional) arm_gemm:WeightFormat enumeration requested by the user. Default is arm_compute::WeightFormat::UNSPECIFIED.</span></div>
<div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;<span class="comment">     * @param[in] pretranspose_B              (Optional) Pretranspose matrix B (transposition of its lowest 2 dimensions), in addition to and before, any further transformations of B</span></div>
<div class="line"><a name="l00110"></a><span class="lineno">  110</span>&#160;<span class="comment">     * @param[in] accumulate                  (Optional) Whether to accumulate in destination or not</span></div>
<div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00112"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a455a66e83f3ef47942543321d04a0b01">  112</a></span>&#160;    <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a455a66e83f3ef47942543321d04a0b01">GEMMInfo</a>(<span class="keywordtype">bool</span>                       <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#aa7e9584d7080ca6442cec62afaff6cad">is_a_reshaped</a>,</div>
<div class="line"><a name="l00113"></a><span class="lineno">  113</span>&#160;             <span class="keywordtype">bool</span>                       <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a77964edb8d16bb8ec14ddd8985e03cb0">is_b_reshaped</a>,</div>
<div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;             <span class="keywordtype">bool</span>                       <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a4c8f9fa843de1086c27c86a6b8cf4582">reshape_b_only_on_first_run</a>,</div>
<div class="line"><a name="l00115"></a><span class="lineno">  115</span>&#160;             <span class="keywordtype">int</span>                        <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#abbd888f118c2209bf7578eb4f8942a07">depth_output_gemm3d</a>     = 0,</div>
<div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;             <span class="keywordtype">bool</span>                       <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a00330b8913cac3b07029ac0c3350e806">reinterpret_input_as_3d</a> = <span class="keyword">false</span>,</div>
<div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;             <span class="keywordtype">bool</span>                       <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ac9e7f8fc8a688fd119d3fb2892cce6b9">retain_internal_weights</a> = <span class="keyword">false</span>,</div>
<div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;             <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml">GEMMLowpOutputStageInfo</a>    <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a11d8f855e323a8396fe6944edcef4238">gemmlowp_output_stage</a>   = <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml">GEMMLowpOutputStageInfo</a>(),</div>
<div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;             <span class="keywordtype">bool</span>                       <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a501521734ce4f9d81d91d0c7e35dcfe9">fp_mixed_precision</a>      = <span class="keyword">false</span>,</div>
<div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;             <span class="keywordtype">bool</span>                       <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ae2d38726d9f14d748185f77798617c8a">fast_math</a>               = <span class="keyword">false</span>,</div>
<div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;             <span class="keywordtype">bool</span>                       <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a757197ffaf53ef6b284c6ceb24fdb688">broadcast_bias</a>          = <span class="keyword">false</span>,</div>
<div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;             <span class="keyword">const</span> <a class="code" href="classarm__compute_1_1_activation_layer_info.xhtml">ActivationLayerInfo</a> &amp;<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a39a8dd296461705ee5cb54eacb4b2818">activation_info</a>         = <a class="code" href="classarm__compute_1_1_activation_layer_info.xhtml">ActivationLayerInfo</a>(),</div>
<div class="line"><a name="l00123"></a><span class="lineno">  123</span>&#160;             <span class="keywordtype">bool</span>                       <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ab8326941d60f08c5d1f47ed65514f3ed">fixed_format</a>            = <span class="keyword">false</span>,</div>
<div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;             <a class="code" href="namespacearm__compute.xhtml#a23ab0e5c6b5d13e084628686c4f282d5">arm_compute::WeightFormat</a>  <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a6d5e7ad98fd0d58a84c837e931464de6">weight_format</a>           = <a class="code" href="namespacearm__compute.xhtml#a23ab0e5c6b5d13e084628686c4f282d5a1c04cc3823d476c3017238679a0fdf52">arm_compute::WeightFormat::UNSPECIFIED</a>,</div>
<div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;             <span class="keywordtype">bool</span>                       <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ade745f0d990160b2ad90ded0814a2498">pretranspose_B</a>          = <span class="keyword">false</span>,</div>
<div class="line"><a name="l00126"></a><span class="lineno">  126</span>&#160;             <span class="keywordtype">bool</span>                       <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a1e9fdeb262430b47630852f779699d43">accumulate</a>              = <span class="keyword">false</span>) noexcept</div>
<div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;        : _is_a_reshaped(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#aa7e9584d7080ca6442cec62afaff6cad">is_a_reshaped</a>),</div>
<div class="line"><a name="l00128"></a><span class="lineno">  128</span>&#160;          _is_b_reshaped(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a77964edb8d16bb8ec14ddd8985e03cb0">is_b_reshaped</a>),</div>
<div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;          _reshape_b_only_on_first_run(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a4c8f9fa843de1086c27c86a6b8cf4582">reshape_b_only_on_first_run</a>),</div>
<div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;          _depth_output_gemm3d(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#abbd888f118c2209bf7578eb4f8942a07">depth_output_gemm3d</a>),</div>
<div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;          _reinterpret_input_as_3d(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a00330b8913cac3b07029ac0c3350e806">reinterpret_input_as_3d</a>),</div>
<div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;          _retain_internal_weights(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ac9e7f8fc8a688fd119d3fb2892cce6b9">retain_internal_weights</a>),</div>
<div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;          _gemmlowp_output_stage(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a11d8f855e323a8396fe6944edcef4238">gemmlowp_output_stage</a>),</div>
<div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;          _fast_math(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ae2d38726d9f14d748185f77798617c8a">fast_math</a>),</div>
<div class="line"><a name="l00135"></a><span class="lineno">  135</span>&#160;          _fp_mixed_precision(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a501521734ce4f9d81d91d0c7e35dcfe9">fp_mixed_precision</a>),</div>
<div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;          _broadcast_bias(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a757197ffaf53ef6b284c6ceb24fdb688">broadcast_bias</a>),</div>
<div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;          _pretranspose_A(false),</div>
<div class="line"><a name="l00138"></a><span class="lineno">  138</span>&#160;          _pretranspose_B(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ade745f0d990160b2ad90ded0814a2498">pretranspose_B</a>),</div>
<div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;          _activation_info(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a39a8dd296461705ee5cb54eacb4b2818">activation_info</a>),</div>
<div class="line"><a name="l00140"></a><span class="lineno">  140</span>&#160;          _fixed_format(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ab8326941d60f08c5d1f47ed65514f3ed">fixed_format</a>),</div>
<div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;          _weight_format(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a6d5e7ad98fd0d58a84c837e931464de6">weight_format</a>),</div>
<div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;          _accumulate(<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a1e9fdeb262430b47630852f779699d43">accumulate</a>)</div>
<div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;    {</div>
<div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;    }<span class="comment"></span></div>
<div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;<span class="comment">    /** Flag which specifies if the matrix A has been reshaped</span></div>
<div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;<span class="comment">     * @return True if the matrix A has been reshaped</span></div>
<div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00149"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#aa7e9584d7080ca6442cec62afaff6cad">  149</a></span>&#160;    <span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#aa7e9584d7080ca6442cec62afaff6cad">is_a_reshaped</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;        <span class="keywordflow">return</span> _is_a_reshaped;</div>
<div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;    };<span class="comment"></span></div>
<div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;<span class="comment">    /** Flag which specifies if the matrix B has been reshaped</span></div>
<div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;<span class="comment">     * @return True if the matrix B has been reshaped</span></div>
<div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00157"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a77964edb8d16bb8ec14ddd8985e03cb0">  157</a></span>&#160;    <span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a77964edb8d16bb8ec14ddd8985e03cb0">is_b_reshaped</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;        <span class="keywordflow">return</span> _is_b_reshaped;</div>
<div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;    };<span class="comment"></span></div>
<div class="line"><a name="l00161"></a><span class="lineno">  161</span>&#160;<span class="comment">    /** Flag which specifies if the reshape of matrix B should executed only for the first</span></div>
<div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;<span class="comment">     * @note This flag could be set to TRUE when GEMM is used to accelerate convolution layer</span></div>
<div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;<span class="comment">     * @return True if the reshaped of matrix B happens only for the first run</span></div>
<div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00167"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a4c8f9fa843de1086c27c86a6b8cf4582">  167</a></span>&#160;    <span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a4c8f9fa843de1086c27c86a6b8cf4582">reshape_b_only_on_first_run</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;        <span class="keywordflow">return</span> _reshape_b_only_on_first_run;</div>
<div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;    };<span class="comment"></span></div>
<div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;<span class="comment">    /** Depth of the output when GEMM output is reinterpreted as 3D tensor</span></div>
<div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;<span class="comment">     * @return the depth of the output tensor</span></div>
<div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00175"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#abbd888f118c2209bf7578eb4f8942a07">  175</a></span>&#160;    <span class="keywordtype">int</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#abbd888f118c2209bf7578eb4f8942a07">depth_output_gemm3d</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;        <span class="keywordflow">return</span> _depth_output_gemm3d;</div>
<div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;    };<span class="comment"></span></div>
<div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;<span class="comment">    /** Flag which specifies if the input tensor has to be reinterpreted as 3D</span></div>
<div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;<span class="comment">     * @return True if the input tensor has to be reinterpreted as 3D tensor</span></div>
<div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00183"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a00330b8913cac3b07029ac0c3350e806">  183</a></span>&#160;    <span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a00330b8913cac3b07029ac0c3350e806">reinterpret_input_as_3d</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;        <span class="keywordflow">return</span> _reinterpret_input_as_3d;</div>
<div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;    };<span class="comment"></span></div>
<div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;<span class="comment">    /** Flag which specifies if the weights tensor has to be retained from previous run</span></div>
<div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;<span class="comment">     * @return True if the weights tensor has to be retained</span></div>
<div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00191"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ac9e7f8fc8a688fd119d3fb2892cce6b9">  191</a></span>&#160;    <span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ac9e7f8fc8a688fd119d3fb2892cce6b9">retain_internal_weights</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160;        <span class="keywordflow">return</span> _retain_internal_weights;</div>
<div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160;    };<span class="comment"></span></div>
<div class="line"><a name="l00195"></a><span class="lineno">  195</span>&#160;<span class="comment">    /** GEMMLowp output stage</span></div>
<div class="line"><a name="l00196"></a><span class="lineno">  196</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00197"></a><span class="lineno">  197</span>&#160;<span class="comment">     * @return the GEMMLowp output stage info</span></div>
<div class="line"><a name="l00198"></a><span class="lineno">  198</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00199"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a11d8f855e323a8396fe6944edcef4238">  199</a></span>&#160;    <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml">GEMMLowpOutputStageInfo</a> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a11d8f855e323a8396fe6944edcef4238">gemmlowp_output_stage</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00200"></a><span class="lineno">  200</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00201"></a><span class="lineno">  201</span>&#160;        <span class="keywordflow">return</span> _gemmlowp_output_stage;</div>
<div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;    };<span class="comment"></span></div>
<div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;<span class="comment">    /** Sets GEMMLowp output stage</span></div>
<div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00205"></a><span class="lineno">  205</span>&#160;<span class="comment">     * @param[in] output_stage Output stage to set</span></div>
<div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00207"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ae081b85ce44f2ec7cfe875afabcdfcc3">  207</a></span>&#160;    <span class="keywordtype">void</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ae081b85ce44f2ec7cfe875afabcdfcc3">set_gemmlowp_output_stage</a>(<a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml">GEMMLowpOutputStageInfo</a> &amp;<a class="code" href="working__space_8hpp.xhtml#aa2b9b52a4827eeb41f97f92a3781eee8">output_stage</a>)</div>
<div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;    {</div>
<div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;        _gemmlowp_output_stage = <a class="code" href="working__space_8hpp.xhtml#aa2b9b52a4827eeb41f97f92a3781eee8">output_stage</a>;</div>
<div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;    };<span class="comment"></span></div>
<div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;<span class="comment">    /** Flag which specifies if a wider accumulator should be used.</span></div>
<div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;<span class="comment">     * @return True if a wider accumulator has to be used</span></div>
<div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00215"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a501521734ce4f9d81d91d0c7e35dcfe9">  215</a></span>&#160;    <span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a501521734ce4f9d81d91d0c7e35dcfe9">fp_mixed_precision</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;        <span class="keywordflow">return</span> _fp_mixed_precision;</div>
<div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;    };<span class="comment"></span></div>
<div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;<span class="comment">    /** Flag which specifies if a shorter accumulator to be used.</span></div>
<div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;<span class="comment">     * @return True if a shorter accumulator has to be used</span></div>
<div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00223"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ae2d38726d9f14d748185f77798617c8a">  223</a></span>&#160;    <span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ae2d38726d9f14d748185f77798617c8a">fast_math</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00224"></a><span class="lineno">  224</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;        <span class="keywordflow">return</span> _fast_math;</div>
<div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;    };<span class="comment"></span></div>
<div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;<span class="comment">    /** Set fast math flag</span></div>
<div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;<span class="comment">     * @param[in] fast_math Flag to set</span></div>
<div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00231"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#af706055cbc19cf4ee2b6effe3beaccee">  231</a></span>&#160;    <span class="keywordtype">void</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#af706055cbc19cf4ee2b6effe3beaccee">set_fast_math</a>(<span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ae2d38726d9f14d748185f77798617c8a">fast_math</a>)</div>
<div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;    {</div>
<div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;        _fast_math = <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ae2d38726d9f14d748185f77798617c8a">fast_math</a>;</div>
<div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;    }<span class="comment"></span></div>
<div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;<span class="comment">    /** Flag which specifies whether to broadcast the shape of the bias tensor.</span></div>
<div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;<span class="comment">     * @return True if the shape of the bias tensor is to be broadcasted.</span></div>
<div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00239"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a757197ffaf53ef6b284c6ceb24fdb688">  239</a></span>&#160;    <span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a757197ffaf53ef6b284c6ceb24fdb688">broadcast_bias</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;        <span class="keywordflow">return</span> _broadcast_bias;</div>
<div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;    };<span class="comment"></span></div>
<div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;<span class="comment">    /** Flag which specifies whether A should be pre-transposed if supported.</span></div>
<div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;<span class="comment">     * @return True if A should be pre-transposed else false.</span></div>
<div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00247"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ac6663acffe7ad0fa26750779bec17d82">  247</a></span>&#160;    <span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ac6663acffe7ad0fa26750779bec17d82">pretranspose_A</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;        <span class="keywordflow">return</span> _pretranspose_A;</div>
<div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;    };<span class="comment"></span></div>
<div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;<span class="comment">    /** Set pre-transpose A flag</span></div>
<div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00253"></a><span class="lineno">  253</span>&#160;<span class="comment">     * @param[in] flag Flag to set</span></div>
<div class="line"><a name="l00254"></a><span class="lineno">  254</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00255"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a72940f70ecb21f935870ca4b61947ce9">  255</a></span>&#160;    <span class="keywordtype">void</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a72940f70ecb21f935870ca4b61947ce9">set_pretranspose_A</a>(<span class="keywordtype">bool</span> flag)</div>
<div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;    {</div>
<div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;        _pretranspose_A = flag;</div>
<div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;    }<span class="comment"></span></div>
<div class="line"><a name="l00259"></a><span class="lineno">  259</span>&#160;<span class="comment">    /** Flag which specifies whether b should be pre-transposed if supported.</span></div>
<div class="line"><a name="l00260"></a><span class="lineno">  260</span>&#160;<span class="comment">     * More concretely, the &quot;pre-transpose&quot; is the transposition of the b tensor&#39;s lowest 2 dimensions</span></div>
<div class="line"><a name="l00261"></a><span class="lineno">  261</span>&#160;<span class="comment">     * If specified true, this pre-transpose will occur in addition to and before, any further transformations of the b matrix</span></div>
<div class="line"><a name="l00262"></a><span class="lineno">  262</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00263"></a><span class="lineno">  263</span>&#160;<span class="comment">     * @return True if b should be pre-transposed else false.</span></div>
<div class="line"><a name="l00264"></a><span class="lineno">  264</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00265"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ade745f0d990160b2ad90ded0814a2498">  265</a></span>&#160;    <span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ade745f0d990160b2ad90ded0814a2498">pretranspose_B</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00266"></a><span class="lineno">  266</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00267"></a><span class="lineno">  267</span>&#160;        <span class="keywordflow">return</span> _pretranspose_B;</div>
<div class="line"><a name="l00268"></a><span class="lineno">  268</span>&#160;    };<span class="comment"></span></div>
<div class="line"><a name="l00269"></a><span class="lineno">  269</span>&#160;<span class="comment">    /** Set pre-transpose b flag</span></div>
<div class="line"><a name="l00270"></a><span class="lineno">  270</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00271"></a><span class="lineno">  271</span>&#160;<span class="comment">     * @param[in] flag Flag to set</span></div>
<div class="line"><a name="l00272"></a><span class="lineno">  272</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00273"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a7e1df6b05c99e52dc87aaa3b1aeab3e9">  273</a></span>&#160;    <span class="keywordtype">void</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a7e1df6b05c99e52dc87aaa3b1aeab3e9">set_pretranspose_B</a>(<span class="keywordtype">bool</span> flag)</div>
<div class="line"><a name="l00274"></a><span class="lineno">  274</span>&#160;    {</div>
<div class="line"><a name="l00275"></a><span class="lineno">  275</span>&#160;        _pretranspose_B = flag;</div>
<div class="line"><a name="l00276"></a><span class="lineno">  276</span>&#160;    }<span class="comment"></span></div>
<div class="line"><a name="l00277"></a><span class="lineno">  277</span>&#160;<span class="comment">    /** Activation layer to apply after the matrix multiplication</span></div>
<div class="line"><a name="l00278"></a><span class="lineno">  278</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00279"></a><span class="lineno">  279</span>&#160;<span class="comment">     * @return ActivationLayerInfo object</span></div>
<div class="line"><a name="l00280"></a><span class="lineno">  280</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00281"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a39a8dd296461705ee5cb54eacb4b2818">  281</a></span>&#160;    <a class="code" href="classarm__compute_1_1_activation_layer_info.xhtml">ActivationLayerInfo</a> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a39a8dd296461705ee5cb54eacb4b2818">activation_info</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00282"></a><span class="lineno">  282</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00283"></a><span class="lineno">  283</span>&#160;        <span class="keywordflow">return</span> _activation_info;</div>
<div class="line"><a name="l00284"></a><span class="lineno">  284</span>&#160;    }<span class="comment"></span></div>
<div class="line"><a name="l00285"></a><span class="lineno">  285</span>&#160;<span class="comment">    /** Set activation layer info</span></div>
<div class="line"><a name="l00286"></a><span class="lineno">  286</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00287"></a><span class="lineno">  287</span>&#160;<span class="comment">     * @param[in] activation_info ActivationLayerInfo object to set</span></div>
<div class="line"><a name="l00288"></a><span class="lineno">  288</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00289"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ad8fc03416b05b11b3b57e6cb17deea6f">  289</a></span>&#160;    <span class="keywordtype">void</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ad8fc03416b05b11b3b57e6cb17deea6f">set_activation_info</a>(<span class="keyword">const</span> <a class="code" href="classarm__compute_1_1_activation_layer_info.xhtml">ActivationLayerInfo</a> &amp;<a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a39a8dd296461705ee5cb54eacb4b2818">activation_info</a>)</div>
<div class="line"><a name="l00290"></a><span class="lineno">  290</span>&#160;    {</div>
<div class="line"><a name="l00291"></a><span class="lineno">  291</span>&#160;        _activation_info = <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a39a8dd296461705ee5cb54eacb4b2818">activation_info</a>;</div>
<div class="line"><a name="l00292"></a><span class="lineno">  292</span>&#160;    }<span class="comment"></span></div>
<div class="line"><a name="l00293"></a><span class="lineno">  293</span>&#160;<span class="comment">    /** Flag which specifies if the GEMM operation is running fixed-format kernels.</span></div>
<div class="line"><a name="l00294"></a><span class="lineno">  294</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00295"></a><span class="lineno">  295</span>&#160;<span class="comment">     * @return True if the GEMM operation is running fixed-format kernel else false.</span></div>
<div class="line"><a name="l00296"></a><span class="lineno">  296</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00297"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ab8326941d60f08c5d1f47ed65514f3ed">  297</a></span>&#160;    <span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ab8326941d60f08c5d1f47ed65514f3ed">fixed_format</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00298"></a><span class="lineno">  298</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00299"></a><span class="lineno">  299</span>&#160;        <span class="keywordflow">return</span> _fixed_format;</div>
<div class="line"><a name="l00300"></a><span class="lineno">  300</span>&#160;    }<span class="comment"></span></div>
<div class="line"><a name="l00301"></a><span class="lineno">  301</span>&#160;<span class="comment">    /** Flag which specifies if GEMM should accumulate the result in destination or not.</span></div>
<div class="line"><a name="l00302"></a><span class="lineno">  302</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00303"></a><span class="lineno">  303</span>&#160;<span class="comment">     * @return True if GEMM is accumulating the result.</span></div>
<div class="line"><a name="l00304"></a><span class="lineno">  304</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00305"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a1e9fdeb262430b47630852f779699d43">  305</a></span>&#160;    <span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a1e9fdeb262430b47630852f779699d43">accumulate</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00306"></a><span class="lineno">  306</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00307"></a><span class="lineno">  307</span>&#160;        <span class="keywordflow">return</span> _accumulate;</div>
<div class="line"><a name="l00308"></a><span class="lineno">  308</span>&#160;    }<span class="comment"></span></div>
<div class="line"><a name="l00309"></a><span class="lineno">  309</span>&#160;<span class="comment">    /** Set fixed-format flag</span></div>
<div class="line"><a name="l00310"></a><span class="lineno">  310</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00311"></a><span class="lineno">  311</span>&#160;<span class="comment">     * @param[in] fixed_format sets whether or not to use fixed-format kernels</span></div>
<div class="line"><a name="l00312"></a><span class="lineno">  312</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00313"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a15aa71f614ff585d24cbb101bfa5232e">  313</a></span>&#160;    <span class="keywordtype">void</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a15aa71f614ff585d24cbb101bfa5232e">set_fixed_format</a>(<span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ab8326941d60f08c5d1f47ed65514f3ed">fixed_format</a>)</div>
<div class="line"><a name="l00314"></a><span class="lineno">  314</span>&#160;    {</div>
<div class="line"><a name="l00315"></a><span class="lineno">  315</span>&#160;        _fixed_format = <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#ab8326941d60f08c5d1f47ed65514f3ed">fixed_format</a>;</div>
<div class="line"><a name="l00316"></a><span class="lineno">  316</span>&#160;    }<span class="comment"></span></div>
<div class="line"><a name="l00317"></a><span class="lineno">  317</span>&#160;<span class="comment">    /** Set accumulate flag</span></div>
<div class="line"><a name="l00318"></a><span class="lineno">  318</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00319"></a><span class="lineno">  319</span>&#160;<span class="comment">     * @param[in] accumulate sets whether or not to use accumulation</span></div>
<div class="line"><a name="l00320"></a><span class="lineno">  320</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00321"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a1ce0c3d620d7b0b2009e93464d142d20">  321</a></span>&#160;    <span class="keywordtype">void</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a1ce0c3d620d7b0b2009e93464d142d20">set_accumulate</a>(<span class="keywordtype">bool</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a1e9fdeb262430b47630852f779699d43">accumulate</a>)</div>
<div class="line"><a name="l00322"></a><span class="lineno">  322</span>&#160;    {</div>
<div class="line"><a name="l00323"></a><span class="lineno">  323</span>&#160;        _accumulate = <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a1e9fdeb262430b47630852f779699d43">accumulate</a>;</div>
<div class="line"><a name="l00324"></a><span class="lineno">  324</span>&#160;    }</div>
<div class="line"><a name="l00325"></a><span class="lineno">  325</span>&#160; </div>
<div class="line"><a name="l00326"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a6d5e7ad98fd0d58a84c837e931464de6">  326</a></span>&#160;    <a class="code" href="namespacearm__compute.xhtml#a23ab0e5c6b5d13e084628686c4f282d5">arm_compute::WeightFormat</a> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a6d5e7ad98fd0d58a84c837e931464de6">weight_format</a>()<span class="keyword"> const</span></div>
<div class="line"><a name="l00327"></a><span class="lineno">  327</span>&#160;<span class="keyword">    </span>{</div>
<div class="line"><a name="l00328"></a><span class="lineno">  328</span>&#160;        <span class="keywordflow">return</span> _weight_format;</div>
<div class="line"><a name="l00329"></a><span class="lineno">  329</span>&#160;    }<span class="comment"></span></div>
<div class="line"><a name="l00330"></a><span class="lineno">  330</span>&#160;<span class="comment">    /** Set weight format to be used</span></div>
<div class="line"><a name="l00331"></a><span class="lineno">  331</span>&#160;<span class="comment">     *</span></div>
<div class="line"><a name="l00332"></a><span class="lineno">  332</span>&#160;<span class="comment">     * @param[in] weight_format arm_compute::WeightFormat enumeration</span></div>
<div class="line"><a name="l00333"></a><span class="lineno">  333</span>&#160;<span class="comment">     */</span></div>
<div class="line"><a name="l00334"></a><span class="lineno"><a class="line" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a9a76aeb3d02431c5dc06ea031c2e49a6">  334</a></span>&#160;    <span class="keywordtype">void</span> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a9a76aeb3d02431c5dc06ea031c2e49a6">set_weight_format</a>(<a class="code" href="namespacearm__compute.xhtml#a23ab0e5c6b5d13e084628686c4f282d5">arm_compute::WeightFormat</a> <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a6d5e7ad98fd0d58a84c837e931464de6">weight_format</a>)</div>
<div class="line"><a name="l00335"></a><span class="lineno">  335</span>&#160;    {</div>
<div class="line"><a name="l00336"></a><span class="lineno">  336</span>&#160;        _weight_format = <a class="code" href="classarm__compute_1_1_g_e_m_m_info.xhtml#a6d5e7ad98fd0d58a84c837e931464de6">weight_format</a>;</div>
<div class="line"><a name="l00337"></a><span class="lineno">  337</span>&#160;    }</div>
<div class="line"><a name="l00338"></a><span class="lineno">  338</span>&#160; </div>
<div class="line"><a name="l00339"></a><span class="lineno">  339</span>&#160;<span class="keyword">private</span>:</div>
<div class="line"><a name="l00340"></a><span class="lineno">  340</span>&#160;    <span class="keywordtype">bool</span>                      _is_a_reshaped;</div>
<div class="line"><a name="l00341"></a><span class="lineno">  341</span>&#160;    <span class="keywordtype">bool</span>                      _is_b_reshaped;</div>
<div class="line"><a name="l00342"></a><span class="lineno">  342</span>&#160;    <span class="keywordtype">bool</span>                      _reshape_b_only_on_first_run;</div>
<div class="line"><a name="l00343"></a><span class="lineno">  343</span>&#160;    <span class="keywordtype">int</span>                       _depth_output_gemm3d;</div>
<div class="line"><a name="l00344"></a><span class="lineno">  344</span>&#160;    <span class="keywordtype">bool</span>                      _reinterpret_input_as_3d;</div>
<div class="line"><a name="l00345"></a><span class="lineno">  345</span>&#160;    <span class="keywordtype">bool</span>                      _retain_internal_weights;</div>
<div class="line"><a name="l00346"></a><span class="lineno">  346</span>&#160;    <a class="code" href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml">GEMMLowpOutputStageInfo</a>   _gemmlowp_output_stage;</div>
<div class="line"><a name="l00347"></a><span class="lineno">  347</span>&#160;    <span class="keywordtype">bool</span>                      _fast_math;</div>
<div class="line"><a name="l00348"></a><span class="lineno">  348</span>&#160;    <span class="keywordtype">bool</span>                      _fp_mixed_precision;</div>
<div class="line"><a name="l00349"></a><span class="lineno">  349</span>&#160;    <span class="keywordtype">bool</span>                      _broadcast_bias;</div>
<div class="line"><a name="l00350"></a><span class="lineno">  350</span>&#160;    <span class="keywordtype">bool</span>                      _pretranspose_A;</div>
<div class="line"><a name="l00351"></a><span class="lineno">  351</span>&#160;    <span class="keywordtype">bool</span>                      _pretranspose_B;</div>
<div class="line"><a name="l00352"></a><span class="lineno">  352</span>&#160;    <a class="code" href="classarm__compute_1_1_activation_layer_info.xhtml">ActivationLayerInfo</a>       _activation_info;</div>
<div class="line"><a name="l00353"></a><span class="lineno">  353</span>&#160;    <span class="keywordtype">bool</span>                      _fixed_format;</div>
<div class="line"><a name="l00354"></a><span class="lineno">  354</span>&#160;    <a class="code" href="namespacearm__compute.xhtml#a23ab0e5c6b5d13e084628686c4f282d5">arm_compute::WeightFormat</a> _weight_format;</div>
<div class="line"><a name="l00355"></a><span class="lineno">  355</span>&#160;    <span class="keywordtype">bool</span>                      _accumulate;</div>
<div class="line"><a name="l00356"></a><span class="lineno">  356</span>&#160;};</div>
<div class="line"><a name="l00357"></a><span class="lineno">  357</span>&#160;} <span class="comment">//namespace arm_compute</span></div>
<div class="line"><a name="l00358"></a><span class="lineno">  358</span>&#160;<span class="preprocessor">#endif // ACL_ARM_COMPUTE_FUNCTION_INFO_GEMMINFO_H</span></div>
</div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a15aa71f614ff585d24cbb101bfa5232e"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a15aa71f614ff585d24cbb101bfa5232e">arm_compute::GEMMInfo::set_fixed_format</a></div><div class="ttdeci">void set_fixed_format(bool fixed_format)</div><div class="ttdoc">Set fixed-format flag.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00313">GEMMInfo.h:313</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a7e1df6b05c99e52dc87aaa3b1aeab3e9"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a7e1df6b05c99e52dc87aaa3b1aeab3e9">arm_compute::GEMMInfo::set_pretranspose_B</a></div><div class="ttdeci">void set_pretranspose_B(bool flag)</div><div class="ttdoc">Set pre-transpose b flag.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00273">GEMMInfo.h:273</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a501521734ce4f9d81d91d0c7e35dcfe9"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a501521734ce4f9d81d91d0c7e35dcfe9">arm_compute::GEMMInfo::fp_mixed_precision</a></div><div class="ttdeci">bool fp_mixed_precision() const</div><div class="ttdoc">Flag which specifies if a wider accumulator should be used.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00215">GEMMInfo.h:215</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a1e9fdeb262430b47630852f779699d43"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a1e9fdeb262430b47630852f779699d43">arm_compute::GEMMInfo::accumulate</a></div><div class="ttdeci">bool accumulate() const</div><div class="ttdoc">Flag which specifies if GEMM should accumulate the result in destination or not.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00305">GEMMInfo.h:305</a></div></div>
<div class="ttc" id="astructarm__compute_1_1_g_e_m_m_lowp_output_stage_info_xhtml_a1cfb92f1c287bf099c3fca0ef0391a2b"><div class="ttname"><a href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a1cfb92f1c287bf099c3fca0ef0391a2b">arm_compute::GEMMLowpOutputStageInfo::gemmlowp_multiplier</a></div><div class="ttdeci">int32_t gemmlowp_multiplier</div><div class="ttdoc">GEMMLowp output stage multiplier used for quantizing to QASYMM8.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00049">GEMMInfo.h:49</a></div></div>
<div class="ttc" id="anamespacearm__compute_xhtml_a5558e2cc22f7f4771653d992c8ad8864aad664ac5008f135e38afeb391e524f9c"><div class="ttname"><a href="namespacearm__compute.xhtml#a5558e2cc22f7f4771653d992c8ad8864aad664ac5008f135e38afeb391e524f9c">arm_compute::GEMMLowpOutputStageType::QUANTIZE_DOWN_FLOAT</a></div><div class="ttdeci">@ QUANTIZE_DOWN_FLOAT</div><div class="ttdoc">Quantize using a floating point multiplication.</div></div>
<div class="ttc" id="astructarm__compute_1_1_g_e_m_m_lowp_output_stage_info_xhtml"><div class="ttname"><a href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml">arm_compute::GEMMLowpOutputStageInfo</a></div><div class="ttdoc">GEMMLowp output stage info.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00045">GEMMInfo.h:45</a></div></div>
<div class="ttc" id="astructarm__compute_1_1_g_e_m_m_lowp_output_stage_info_xhtml_a01934c5087f5193aaf3ea9bf41d1a8dc"><div class="ttname"><a href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a01934c5087f5193aaf3ea9bf41d1a8dc">arm_compute::GEMMLowpOutputStageInfo::gemmlowp_offset</a></div><div class="ttdeci">int32_t gemmlowp_offset</div><div class="ttdoc">GEMMLowp output stage offset used for quantizing to QASYMM8.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00048">GEMMInfo.h:48</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_ad8fc03416b05b11b3b57e6cb17deea6f"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#ad8fc03416b05b11b3b57e6cb17deea6f">arm_compute::GEMMInfo::set_activation_info</a></div><div class="ttdeci">void set_activation_info(const ActivationLayerInfo &amp;activation_info)</div><div class="ttdoc">Set activation layer info.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00289">GEMMInfo.h:289</a></div></div>
<div class="ttc" id="anamespacearm__compute_1_1support_1_1cpp11_xhtml_a73e352c61baaf9c1178da2d30105b04e"><div class="ttname"><a href="namespacearm__compute_1_1support_1_1cpp11.xhtml#a73e352c61baaf9c1178da2d30105b04e">arm_compute::support::cpp11::lowest</a></div><div class="ttdeci">T lowest()</div><div class="ttdef"><b>Definition:</b> <a href="_toolchain_support_8h_source.xhtml#l00278">ToolchainSupport.h:278</a></div></div>
<div class="ttc" id="astructarm__compute_1_1_g_e_m_m_lowp_output_stage_info_xhtml_a6e019ad85979fd73c74f97e5483faf35"><div class="ttname"><a href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a6e019ad85979fd73c74f97e5483faf35">arm_compute::GEMMLowpOutputStageInfo::type</a></div><div class="ttdeci">GEMMLowpOutputStageType type</div><div class="ttdoc">GEMMLowp output stage type.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00047">GEMMInfo.h:47</a></div></div>
<div class="ttc" id="a_activation_layer_info_8h_xhtml"><div class="ttname"><a href="_activation_layer_info_8h.xhtml">ActivationLayerInfo.h</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_aa7e9584d7080ca6442cec62afaff6cad"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#aa7e9584d7080ca6442cec62afaff6cad">arm_compute::GEMMInfo::is_a_reshaped</a></div><div class="ttdeci">bool is_a_reshaped() const</div><div class="ttdoc">Flag which specifies if the matrix A has been reshaped.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00149">GEMMInfo.h:149</a></div></div>
<div class="ttc" id="astructarm__compute_1_1_g_e_m_m_lowp_output_stage_info_xhtml_a94e1801be6c3d9d6645c694d7e280cda"><div class="ttname"><a href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a94e1801be6c3d9d6645c694d7e280cda">arm_compute::GEMMLowpOutputStageInfo::is_quantized_per_channel</a></div><div class="ttdeci">bool is_quantized_per_channel</div><div class="ttdoc">GEMMLowp quantized per-channel flag.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00060">GEMMInfo.h:60</a></div></div>
<div class="ttc" id="astructarm__compute_1_1_g_e_m_m_lowp_output_stage_info_xhtml_ab269b182588a158cd256f9d4bb2a00dd"><div class="ttname"><a href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#ab269b182588a158cd256f9d4bb2a00dd">arm_compute::GEMMLowpOutputStageInfo::gemmlowp_shifts</a></div><div class="ttdeci">std::vector&lt; int32_t &gt; gemmlowp_shifts</div><div class="ttdoc">GEMMLowp output stage multiplier used for quantizing to QASYMM8.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00058">GEMMInfo.h:58</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a72940f70ecb21f935870ca4b61947ce9"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a72940f70ecb21f935870ca4b61947ce9">arm_compute::GEMMInfo::set_pretranspose_A</a></div><div class="ttdeci">void set_pretranspose_A(bool flag)</div><div class="ttdoc">Set pre-transpose A flag.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00255">GEMMInfo.h:255</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_ab8326941d60f08c5d1f47ed65514f3ed"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#ab8326941d60f08c5d1f47ed65514f3ed">arm_compute::GEMMInfo::fixed_format</a></div><div class="ttdeci">bool fixed_format() const</div><div class="ttdoc">Flag which specifies if the GEMM operation is running fixed-format kernels.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00297">GEMMInfo.h:297</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml">arm_compute::GEMMInfo</a></div><div class="ttdoc">GEMM information class.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00069">GEMMInfo.h:69</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a6d5e7ad98fd0d58a84c837e931464de6"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a6d5e7ad98fd0d58a84c837e931464de6">arm_compute::GEMMInfo::weight_format</a></div><div class="ttdeci">arm_compute::WeightFormat weight_format() const</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00326">GEMMInfo.h:326</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_af706055cbc19cf4ee2b6effe3beaccee"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#af706055cbc19cf4ee2b6effe3beaccee">arm_compute::GEMMInfo::set_fast_math</a></div><div class="ttdeci">void set_fast_math(bool fast_math)</div><div class="ttdoc">Set fast math flag.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00231">GEMMInfo.h:231</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_ae2d38726d9f14d748185f77798617c8a"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#ae2d38726d9f14d748185f77798617c8a">arm_compute::GEMMInfo::fast_math</a></div><div class="ttdeci">bool fast_math() const</div><div class="ttdoc">Flag which specifies if a shorter accumulator to be used.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00223">GEMMInfo.h:223</a></div></div>
<div class="ttc" id="astructarm__compute_1_1_g_e_m_m_lowp_output_stage_info_xhtml_a6db94040329f1dedcd348ec7de072e2a"><div class="ttname"><a href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a6db94040329f1dedcd348ec7de072e2a">arm_compute::GEMMLowpOutputStageInfo::gemmlowp_max_bound</a></div><div class="ttdeci">int32_t gemmlowp_max_bound</div><div class="ttdoc">GEMMLowp max value used to saturate down the output result before converting back to QASYMM8.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00054">GEMMInfo.h:54</a></div></div>
<div class="ttc" id="aworking__space_8hpp_xhtml_aa2b9b52a4827eeb41f97f92a3781eee8"><div class="ttname"><a href="working__space_8hpp.xhtml#aa2b9b52a4827eeb41f97f92a3781eee8">output_stage</a></div><div class="ttdeci">const OutputStage &amp; output_stage</div><div class="ttdef"><b>Definition:</b> <a href="working__space_8hpp_source.xhtml#l00107">working_space.hpp:107</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_activation_layer_info_xhtml"><div class="ttname"><a href="classarm__compute_1_1_activation_layer_info.xhtml">arm_compute::ActivationLayerInfo</a></div><div class="ttdoc">Activation Layer Information class.</div><div class="ttdef"><b>Definition:</b> <a href="_activation_layer_info_8h_source.xhtml#l00061">ActivationLayerInfo.h:61</a></div></div>
<div class="ttc" id="anamespacearm__compute_xhtml_a5558e2cc22f7f4771653d992c8ad8864a079e2ddc95b344b5cb0188bed9a80d8b"><div class="ttname"><a href="namespacearm__compute.xhtml#a5558e2cc22f7f4771653d992c8ad8864a079e2ddc95b344b5cb0188bed9a80d8b">arm_compute::GEMMLowpOutputStageType::QUANTIZE_DOWN</a></div><div class="ttdeci">@ QUANTIZE_DOWN</div><div class="ttdoc">Quantize using an integer multiplication.</div></div>
<div class="ttc" id="astructarm__compute_1_1_g_e_m_m_lowp_output_stage_info_xhtml_adc26274f64ea797e55d06e6a813f30f9"><div class="ttname"><a href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#adc26274f64ea797e55d06e6a813f30f9">arm_compute::GEMMLowpOutputStageInfo::gemmlowp_real_multiplier</a></div><div class="ttdeci">float gemmlowp_real_multiplier</div><div class="ttdoc">GEMMLowp output stage real multiplier used for quantizing to QASYMM8.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00059">GEMMInfo.h:59</a></div></div>
<div class="ttc" id="anamespacearm__compute_xhtml_a23ab0e5c6b5d13e084628686c4f282d5"><div class="ttname"><a href="namespacearm__compute.xhtml#a23ab0e5c6b5d13e084628686c4f282d5">arm_compute::WeightFormat</a></div><div class="ttdeci">WeightFormat</div><div class="ttdoc">Memory layouts for the weights tensor.</div><div class="ttdef"><b>Definition:</b> <a href="_core_types_8h_source.xhtml#l00311">CoreTypes.h:311</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_ac6663acffe7ad0fa26750779bec17d82"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#ac6663acffe7ad0fa26750779bec17d82">arm_compute::GEMMInfo::pretranspose_A</a></div><div class="ttdeci">bool pretranspose_A() const</div><div class="ttdoc">Flag which specifies whether A should be pre-transposed if supported.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00247">GEMMInfo.h:247</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a4c8f9fa843de1086c27c86a6b8cf4582"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a4c8f9fa843de1086c27c86a6b8cf4582">arm_compute::GEMMInfo::reshape_b_only_on_first_run</a></div><div class="ttdeci">bool reshape_b_only_on_first_run() const</div><div class="ttdoc">Flag which specifies if the reshape of matrix B should executed only for the first.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00167">GEMMInfo.h:167</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_ae081b85ce44f2ec7cfe875afabcdfcc3"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#ae081b85ce44f2ec7cfe875afabcdfcc3">arm_compute::GEMMInfo::set_gemmlowp_output_stage</a></div><div class="ttdeci">void set_gemmlowp_output_stage(GEMMLowpOutputStageInfo &amp;output_stage)</div><div class="ttdoc">Sets GEMMLowp output stage.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00207">GEMMInfo.h:207</a></div></div>
<div class="ttc" id="a_core_types_8h_xhtml"><div class="ttname"><a href="_core_types_8h.xhtml">CoreTypes.h</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a455a66e83f3ef47942543321d04a0b01"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a455a66e83f3ef47942543321d04a0b01">arm_compute::GEMMInfo::GEMMInfo</a></div><div class="ttdeci">GEMMInfo(bool is_a_reshaped, bool is_b_reshaped, bool reshape_b_only_on_first_run, int depth_output_gemm3d=0, bool reinterpret_input_as_3d=false, bool retain_internal_weights=false, GEMMLowpOutputStageInfo gemmlowp_output_stage=GEMMLowpOutputStageInfo(), bool fp_mixed_precision=false, bool fast_math=false, bool broadcast_bias=false, const ActivationLayerInfo &amp;activation_info=ActivationLayerInfo(), bool fixed_format=false, arm_compute::WeightFormat weight_format=arm_compute::WeightFormat::UNSPECIFIED, bool pretranspose_B=false, bool accumulate=false) noexcept</div><div class="ttdoc">Constructor.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00112">GEMMInfo.h:112</a></div></div>
<div class="ttc" id="astructarm__compute_1_1_g_e_m_m_lowp_output_stage_info_xhtml_ae5bd6bebbc0c7ebd9e7dbfd47d939c2a"><div class="ttname"><a href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#ae5bd6bebbc0c7ebd9e7dbfd47d939c2a">arm_compute::GEMMLowpOutputStageInfo::gemmlowp_multipliers</a></div><div class="ttdeci">std::vector&lt; int32_t &gt; gemmlowp_multipliers</div><div class="ttdoc">GEMMLowp output stage multiplier used for quantizing to QASYMM8.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00057">GEMMInfo.h:57</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_ade745f0d990160b2ad90ded0814a2498"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#ade745f0d990160b2ad90ded0814a2498">arm_compute::GEMMInfo::pretranspose_B</a></div><div class="ttdeci">bool pretranspose_B() const</div><div class="ttdoc">Flag which specifies whether b should be pre-transposed if supported.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00265">GEMMInfo.h:265</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a77964edb8d16bb8ec14ddd8985e03cb0"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a77964edb8d16bb8ec14ddd8985e03cb0">arm_compute::GEMMInfo::is_b_reshaped</a></div><div class="ttdeci">bool is_b_reshaped() const</div><div class="ttdoc">Flag which specifies if the matrix B has been reshaped.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00157">GEMMInfo.h:157</a></div></div>
<div class="ttc" id="anamespacearm__compute_xhtml_a5558e2cc22f7f4771653d992c8ad8864ab50339a10e1de285ac99d4c3990b8693"><div class="ttname"><a href="namespacearm__compute.xhtml#a5558e2cc22f7f4771653d992c8ad8864ab50339a10e1de285ac99d4c3990b8693">arm_compute::GEMMLowpOutputStageType::NONE</a></div><div class="ttdeci">@ NONE</div><div class="ttdoc">No quantization.</div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a9a76aeb3d02431c5dc06ea031c2e49a6"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a9a76aeb3d02431c5dc06ea031c2e49a6">arm_compute::GEMMInfo::set_weight_format</a></div><div class="ttdeci">void set_weight_format(arm_compute::WeightFormat weight_format)</div><div class="ttdoc">Set weight format to be used.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00334">GEMMInfo.h:334</a></div></div>
<div class="ttc" id="anamespacearm__compute_xhtml_a23ab0e5c6b5d13e084628686c4f282d5a1c04cc3823d476c3017238679a0fdf52"><div class="ttname"><a href="namespacearm__compute.xhtml#a23ab0e5c6b5d13e084628686c4f282d5a1c04cc3823d476c3017238679a0fdf52">arm_compute::WeightFormat::UNSPECIFIED</a></div><div class="ttdeci">@ UNSPECIFIED</div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a1ce0c3d620d7b0b2009e93464d142d20"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a1ce0c3d620d7b0b2009e93464d142d20">arm_compute::GEMMInfo::set_accumulate</a></div><div class="ttdeci">void set_accumulate(bool accumulate)</div><div class="ttdoc">Set accumulate flag.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00321">GEMMInfo.h:321</a></div></div>
<div class="ttc" id="anamespacearm__compute_xhtml_a5558e2cc22f7f4771653d992c8ad8864"><div class="ttname"><a href="namespacearm__compute.xhtml#a5558e2cc22f7f4771653d992c8ad8864">arm_compute::GEMMLowpOutputStageType</a></div><div class="ttdeci">GEMMLowpOutputStageType</div><div class="ttdoc">GEMMLowp output stage type.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00036">GEMMInfo.h:36</a></div></div>
<div class="ttc" id="anamespacearm__compute_xhtml"><div class="ttname"><a href="namespacearm__compute.xhtml">arm_compute</a></div><div class="ttdoc">Copyright (c) 2017-2024 Arm Limited.</div><div class="ttdef"><b>Definition:</b> <a href="introduction_8dox_source.xhtml#l00024">introduction.dox:24</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_ae0c4636f1099428785df0463f7151889"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#ae0c4636f1099428785df0463f7151889">arm_compute::GEMMInfo::GEMMInfo</a></div><div class="ttdeci">GEMMInfo() noexcept</div><div class="ttdoc">Default constructor.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00073">GEMMInfo.h:73</a></div></div>
<div class="ttc" id="astructarm__compute_1_1_g_e_m_m_lowp_output_stage_info_xhtml_ab233758aca2751c6e71a2f79baf7b92a"><div class="ttname"><a href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#ab233758aca2751c6e71a2f79baf7b92a">arm_compute::GEMMLowpOutputStageInfo::output_data_type</a></div><div class="ttdeci">DataType output_data_type</div><div class="ttdoc">Output tensor data type to use if the output is not initialized.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00061">GEMMInfo.h:61</a></div></div>
<div class="ttc" id="astructarm__compute_1_1_g_e_m_m_lowp_output_stage_info_xhtml_a3f0613aeb69c326e7d8ffb34b44fae94"><div class="ttname"><a href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a3f0613aeb69c326e7d8ffb34b44fae94">arm_compute::GEMMLowpOutputStageInfo::gemmlowp_shift</a></div><div class="ttdeci">int32_t gemmlowp_shift</div><div class="ttdoc">GEMMLowp output stage shift used for quantizing to uint8.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00050">GEMMInfo.h:50</a></div></div>
<div class="ttc" id="astructarm__compute_1_1_g_e_m_m_lowp_output_stage_info_xhtml_a155d27c75f14a82a74e5039c9657c8eb"><div class="ttname"><a href="structarm__compute_1_1_g_e_m_m_lowp_output_stage_info.xhtml#a155d27c75f14a82a74e5039c9657c8eb">arm_compute::GEMMLowpOutputStageInfo::gemmlowp_min_bound</a></div><div class="ttdeci">int32_t gemmlowp_min_bound</div><div class="ttdoc">GEMMLowp min value used to saturate down the output result before converting back to QASYMM8.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00051">GEMMInfo.h:51</a></div></div>
<div class="ttc" id="anamespacearm__compute_xhtml_ad8ed01ff3ff33333d8e19db4d2818bb6a696b031073e74bf2cb98e5ef201d4aa3"><div class="ttname"><a href="namespacearm__compute.xhtml#ad8ed01ff3ff33333d8e19db4d2818bb6a696b031073e74bf2cb98e5ef201d4aa3">arm_compute::DataType::UNKNOWN</a></div><div class="ttdeci">@ UNKNOWN</div><div class="ttdoc">Unknown data type.</div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a757197ffaf53ef6b284c6ceb24fdb688"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a757197ffaf53ef6b284c6ceb24fdb688">arm_compute::GEMMInfo::broadcast_bias</a></div><div class="ttdeci">bool broadcast_bias() const</div><div class="ttdoc">Flag which specifies whether to broadcast the shape of the bias tensor.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00239">GEMMInfo.h:239</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a39a8dd296461705ee5cb54eacb4b2818"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a39a8dd296461705ee5cb54eacb4b2818">arm_compute::GEMMInfo::activation_info</a></div><div class="ttdeci">ActivationLayerInfo activation_info() const</div><div class="ttdoc">Activation layer to apply after the matrix multiplication.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00281">GEMMInfo.h:281</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_ac9e7f8fc8a688fd119d3fb2892cce6b9"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#ac9e7f8fc8a688fd119d3fb2892cce6b9">arm_compute::GEMMInfo::retain_internal_weights</a></div><div class="ttdeci">bool retain_internal_weights() const</div><div class="ttdoc">Flag which specifies if the weights tensor has to be retained from previous run.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00191">GEMMInfo.h:191</a></div></div>
<div class="ttc" id="anamespacearm__compute_xhtml_a5558e2cc22f7f4771653d992c8ad8864ab300cae200f67712c1eb9234e28158ca"><div class="ttname"><a href="namespacearm__compute.xhtml#a5558e2cc22f7f4771653d992c8ad8864ab300cae200f67712c1eb9234e28158ca">arm_compute::GEMMLowpOutputStageType::QUANTIZE_DOWN_FIXEDPOINT</a></div><div class="ttdeci">@ QUANTIZE_DOWN_FIXEDPOINT</div><div class="ttdoc">Quantize using a fixed point multiplication.</div></div>
<div class="ttc" id="anamespacearm__compute_xhtml_ad8ed01ff3ff33333d8e19db4d2818bb6"><div class="ttname"><a href="namespacearm__compute.xhtml#ad8ed01ff3ff33333d8e19db4d2818bb6">arm_compute::DataType</a></div><div class="ttdeci">DataType</div><div class="ttdoc">Available data types.</div><div class="ttdef"><b>Definition:</b> <a href="_core_types_8h_source.xhtml#l00083">CoreTypes.h:83</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a00330b8913cac3b07029ac0c3350e806"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a00330b8913cac3b07029ac0c3350e806">arm_compute::GEMMInfo::reinterpret_input_as_3d</a></div><div class="ttdeci">bool reinterpret_input_as_3d() const</div><div class="ttdoc">Flag which specifies if the input tensor has to be reinterpreted as 3D.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00183">GEMMInfo.h:183</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_a11d8f855e323a8396fe6944edcef4238"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#a11d8f855e323a8396fe6944edcef4238">arm_compute::GEMMInfo::gemmlowp_output_stage</a></div><div class="ttdeci">GEMMLowpOutputStageInfo gemmlowp_output_stage() const</div><div class="ttdoc">GEMMLowp output stage.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00199">GEMMInfo.h:199</a></div></div>
<div class="ttc" id="aclassarm__compute_1_1_g_e_m_m_info_xhtml_abbd888f118c2209bf7578eb4f8942a07"><div class="ttname"><a href="classarm__compute_1_1_g_e_m_m_info.xhtml#abbd888f118c2209bf7578eb4f8942a07">arm_compute::GEMMInfo::depth_output_gemm3d</a></div><div class="ttdeci">int depth_output_gemm3d() const</div><div class="ttdoc">Depth of the output when GEMM output is reinterpreted as 3D tensor.</div><div class="ttdef"><b>Definition:</b> <a href="_g_e_m_m_info_8h_source.xhtml#l00175">GEMMInfo.h:175</a></div></div>
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="dir_214608ef36d61442cb2b0c1c4e9a7def.xhtml">arm_compute</a></li><li class="navelem"><a class="el" href="dir_c9636d973c877190ddc5121fa1f43c4f.xhtml">function_info</a></li><li class="navelem"><a class="el" href="_g_e_m_m_info_8h.xhtml">GEMMInfo.h</a></li>
    <li class="footer">Generated on Mon Apr 29 2024 10:53:52 for Compute Library by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.17 </li>
  </ul>
</div>
</body>
</html>
