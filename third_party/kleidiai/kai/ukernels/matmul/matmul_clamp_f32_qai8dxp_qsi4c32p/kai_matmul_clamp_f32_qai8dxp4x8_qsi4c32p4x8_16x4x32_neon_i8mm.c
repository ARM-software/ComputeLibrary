//
// SPDX-FileCopyrightText: Copyright 2024 Arm Limited and/or its affiliates <open-source-office@arm.com>
//
// SPDX-License-Identifier: Apache-2.0
//
#if !defined(__ARM_FEATURE_MATMUL_INT8)
#error "I8mm extension required to compile this micro-kernel"
#else
#include "kai_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_16x4x32_neon_i8mm.h"

#include <arm_neon.h>
#include <stddef.h>
#include <stdint.h>

#include "kai/kai_common.h"

static const size_t kai_m_step = 16;
static const size_t kai_n_step = 4;
static const size_t kai_mr = 4;
static const size_t kai_nr = 4;
static const size_t kai_kr = 16;
static const size_t kai_sr = 2;
static const size_t kai_bl_multiple_of = 32;
static const size_t kai_num_bytes_multiplier_lhs = sizeof(float);
static const size_t kai_num_bytes_multiplier_rhs = sizeof(uint16_t);
static const size_t kai_num_bytes_offset_lhs = sizeof(int32_t);
static const size_t kai_num_bytes_sum_rhs = sizeof(float);
static const size_t kai_num_bytes_bias = sizeof(float);

inline static size_t kai_num_blocks_per_row(size_t k, size_t bl) {
    KAI_ASSERT((bl % kai_bl_multiple_of) == 0);
    return kai_roundup(k, bl) / bl;
}

inline static size_t kai_k_roundedup(size_t k) {
    // Since we pack a float and int32 value at the end of the row,
    // we must make sure that k is a multiple of 4 for alignment
    size_t kr_sr_roundedup4 = kai_roundup(kai_kr * kai_sr, 4);
    return kai_roundup(k, kr_sr_roundedup4);
}

inline static size_t kai_lhs_packed_stride(size_t k) {
    const size_t k_internal = kai_k_roundedup(k);

    KAI_ASSERT((k_internal % 2) == 0);

    return kai_mr * (k_internal * sizeof(int8_t) + kai_num_bytes_multiplier_lhs + kai_num_bytes_offset_lhs);
}

inline static size_t kai_rhs_packed_stride(size_t k, size_t bl) {
    KAI_ASSERT((bl % kai_kr) == 0);
    KAI_ASSERT((bl % kai_bl_multiple_of) == 0);

    const size_t num_blocks_per_row = kai_num_blocks_per_row(k, bl);
    const size_t num_bytes_per_block = (bl / 2) + kai_num_bytes_multiplier_rhs;

    return kai_nr * ((num_bytes_per_block * num_blocks_per_row) + kai_num_bytes_sum_rhs + kai_num_bytes_bias);
}

size_t kai_get_m_step_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_16x4x32_neon_i8mm(void) {
    return kai_m_step;
}

size_t kai_get_n_step_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_16x4x32_neon_i8mm(void) {
    return kai_n_step;
}

size_t kai_get_mr_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_16x4x32_neon_i8mm(void) {
    return kai_mr;
}

size_t kai_get_nr_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_16x4x32_neon_i8mm(void) {
    return kai_nr;
}

size_t kai_get_kr_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_16x4x32_neon_i8mm(void) {
    return kai_kr;
}

size_t kai_get_sr_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_16x4x32_neon_i8mm(void) {
    return kai_sr;
}

size_t kai_get_lhs_packed_offset_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_16x4x32_neon_i8mm(size_t m_idx, size_t k) {
    KAI_ASSERT((m_idx % kai_m_step) == 0);

    return (m_idx / kai_m_step) * kai_lhs_packed_stride(k);
}

size_t kai_get_rhs_packed_offset_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_16x4x32_neon_i8mm(
    size_t n_idx, size_t k, size_t bl) {
    KAI_ASSERT((n_idx % kai_n_step) == 0);

    return (n_idx / kai_n_step) * kai_rhs_packed_stride(k, bl);
}

size_t kai_get_dst_offset_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_16x4x32_neon_i8mm(
    size_t m_idx, size_t n_idx, size_t dst_stride) {
    KAI_ASSERT((m_idx % kai_m_step) == 0);
    KAI_ASSERT((n_idx % kai_n_step) == 0);

    return (n_idx * sizeof(float)) + m_idx * dst_stride;
}

size_t kai_get_dst_size_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_16x4x32_neon_i8mm(size_t m, size_t n) {
    return m * n * sizeof(float);
}

void kai_run_matmul_clamp_f32_qai8dxp4x8_qsi4c32p4x8_16x4x32_neon_i8mm(
    size_t m, size_t n, size_t k, size_t bl, const void* lhs_packed, const void* rhs_packed,
    float* dst,  // NOLINT(readability-non-const-parameter)
    size_t dst_stride_row, size_t dst_stride_col, float scalar_min, float scalar_max) {
    KAI_ASSERT((bl % kai_kr) == 0);
    KAI_ASSERT((bl % kai_bl_multiple_of) == 0);
    KAI_ASSERT(dst_stride_col == sizeof(float));

    if (m == 0) {
        return;
    }

    size_t num_subblocks = bl / kai_bl_multiple_of;
    size_t num_blocks = kai_num_blocks_per_row(k, bl);

    float clamp_vals[2] = {scalar_min, scalar_max};

    if (bl == 32) {
        __asm__ __volatile__(
            "mov x13, %x[m]\n"
            "mov x12, #0x80\n"
            "mov x20, #0x20\n"
            "cmp x13, #0x10\n"
            "madd x12, %x[num_blocks], x12, x20\n"
            "blt 14f\n"
            "1:"  // Row loop
            "mov x11, %x[rhs_packed]\n"
            "mov x10, %x[n]\n"
            "add x9, %x[dst], %x[dst_stride_row], LSL #4\n"
            "2:"  // Column loop
            "mov x27, %x[lhs_packed]\n"
            "movi v31.16b, #0x0\n"
            "movi v30.16b, #0x0\n"
            "mov x20, %x[num_blocks]\n"
            "movi v29.16b, #0x0\n"
            "movi v28.16b, #0x0\n"
            "movi v27.16b, #0x0\n"
            "movi v26.16b, #0x0\n"
            "add x23, x27, x12\n"
            "add x22, x23, x12\n"
            "movi v25.16b, #0x0\n"
            "movi v24.16b, #0x0\n"
            "add x21, x22, x12\n"
            "movi v23.16b, #0x0\n"
            "movi v22.16b, #0x0\n"
            "movi v21.16b, #0x0\n"
            "movi v20.16b, #0x0\n"
            "movi v19.16b, #0x0\n"
            "movi v18.16b, #0x0\n"
            "movi v17.16b, #0x0\n"
            "movi v16.16b, #0x0\n"
            "3:"  // Block loop
            "ldr q11, [x11, #0x0]\n"
            "ldr q4, [x11, #0x10]\n"
            "movi v2.4s, #0x0\n"
            "movi v9.4s, #0x0\n"
            "ldr q12, [x27, #0x0]\n"
            "ldr q0, [x27, #0x10]\n"
            "movi v7.4s, #0x0\n"
            "movi v5.4s, #0x0\n"
            "ldr q15, [x11, #0x20]\n"
            "ldr q13, [x11, #0x30]\n"
            "movi v10.16b, #0xf0\n"
            "add x11, x11, #0x40\n"
            "ldr q8, [x27, #0x20]\n"
            "ldr q6, [x27, #0x30]\n"
            "shl v14.16b, v11.16b, #0x4\n"
            "shl v3.16b, v4.16b, #0x4\n"
            "ldr q1, [x27, #0x40]\n"
            "and v11.16b, v11.16b, v10.16b\n"
            "and v4.16b, v4.16b, v10.16b\n"
            ".inst 0x4e8ea582  // smmla v2.4s, v12.16b, v14.16b\n"
            ".inst 0x4e83a589  // smmla v9.4s, v12.16b, v3.16b\n"
            "shl v12.16b, v15.16b, #0x4\n"
            ".inst 0x4e8ea407  // smmla v7.4s, v0.16b, v14.16b\n"
            ".inst 0x4e83a405  // smmla v5.4s, v0.16b, v3.16b\n"
            "shl v0.16b, v13.16b, #0x4\n"
            "and v15.16b, v15.16b, v10.16b\n"
            "and v13.16b, v13.16b, v10.16b\n"
            "ldr q10, [x27, #0x50]\n"
            ".inst 0x4e8ca502  // smmla v2.4s, v8.16b, v12.16b\n"
            ".inst 0x4e80a509  // smmla v9.4s, v8.16b, v0.16b\n"
            "ldr q8, [x27, #0x60]\n"
            ".inst 0x4e8ca4c7  // smmla v7.4s, v6.16b, v12.16b\n"
            ".inst 0x4e80a4c5  // smmla v5.4s, v6.16b, v0.16b\n"
            "ldr q6, [x27, #0x70]\n"
            "add x27, x27, #0x80\n"
            ".inst 0x4e8ba422  // smmla v2.4s, v1.16b, v11.16b\n"
            ".inst 0x4e84a429  // smmla v9.4s, v1.16b, v4.16b\n"
            "ldr d1, [x11, #0x0]\n"
            "add x11, x11, #0x8\n"
            ".inst 0x4e8ba547  // smmla v7.4s, v10.16b, v11.16b\n"
            ".inst 0x4e84a545  // smmla v5.4s, v10.16b, v4.16b\n"
            ".inst 0x4e8fa502  // smmla v2.4s, v8.16b, v15.16b\n"
            "shll v1.4s, v1.4h, #0x10\n"
            ".inst 0x4e8da509  // smmla v9.4s, v8.16b, v13.16b\n"
            ".inst 0x4e8fa4c7  // smmla v7.4s, v6.16b, v15.16b\n"
            ".inst 0x4e8da4c5  // smmla v5.4s, v6.16b, v13.16b\n"
            "uzp1 v6.2d, v2.2d, v9.2d\n"
            "uzp2 v8.2d, v2.2d, v9.2d\n"
            "scvtf v6.4s, v6.4s, #0x4\n"
            "uzp1 v9.2d, v7.2d, v5.2d\n"
            "uzp2 v2.2d, v7.2d, v5.2d\n"
            "scvtf v8.4s, v8.4s, #0x4\n"
            "fmla v31.4s, v6.4s, v1.4s\n"
            "scvtf v9.4s, v9.4s, #0x4\n"
            "scvtf v2.4s, v2.4s, #0x4\n"
            "fmla v30.4s, v8.4s, v1.4s\n"
            "fmla v29.4s, v9.4s, v1.4s\n"
            "fmla v28.4s, v2.4s, v1.4s\n"
            "ldr q9, [x23, #0x0]\n"
            "ldr q7, [x23, #0x10]\n"
            "movi v8.4s, #0x0\n"
            "movi v2.4s, #0x0\n"
            "ldr q5, [x23, #0x20]\n"
            "ldr q10, [x23, #0x30]\n"
            "movi v6.4s, #0x0\n"
            ".inst 0x4e8ea528  // smmla v8.4s, v9.16b, v14.16b\n"
            ".inst 0x4e83a522  // smmla v2.4s, v9.16b, v3.16b\n"
            "ldr q9, [x23, #0x40]\n"
            ".inst 0x4e8ea4e6  // smmla v6.4s, v7.16b, v14.16b\n"
            ".inst 0x4e8ca4a8  // smmla v8.4s, v5.16b, v12.16b\n"
            ".inst 0x4e80a4a2  // smmla v2.4s, v5.16b, v0.16b\n"
            "ldr q5, [x23, #0x50]\n"
            ".inst 0x4e8ca546  // smmla v6.4s, v10.16b, v12.16b\n"
            ".inst 0x4e8ba528  // smmla v8.4s, v9.16b, v11.16b\n"
            ".inst 0x4e84a522  // smmla v2.4s, v9.16b, v4.16b\n"
            "ldr q9, [x23, #0x60]\n"
            ".inst 0x4e8ba4a6  // smmla v6.4s, v5.16b, v11.16b\n"
            ".inst 0x4e8fa528  // smmla v8.4s, v9.16b, v15.16b\n"
            ".inst 0x4e8da522  // smmla v2.4s, v9.16b, v13.16b\n"
            "movi v9.4s, #0x0\n"
            ".inst 0x4e83a4e9  // smmla v9.4s, v7.16b, v3.16b\n"
            "ldr q7, [x23, #0x70]\n"
            "add x23, x23, #0x80\n"
            ".inst 0x4e8fa4e6  // smmla v6.4s, v7.16b, v15.16b\n"
            ".inst 0x4e80a549  // smmla v9.4s, v10.16b, v0.16b\n"
            "uzp1 v10.2d, v8.2d, v2.2d\n"
            "uzp2 v2.2d, v8.2d, v2.2d\n"
            "scvtf v10.4s, v10.4s, #0x4\n"
            ".inst 0x4e84a4a9  // smmla v9.4s, v5.16b, v4.16b\n"
            "scvtf v2.4s, v2.4s, #0x4\n"
            "fmla v27.4s, v10.4s, v1.4s\n"
            ".inst 0x4e8da4e9  // smmla v9.4s, v7.16b, v13.16b\n"
            "fmla v26.4s, v2.4s, v1.4s\n"
            "uzp1 v2.2d, v6.2d, v9.2d\n"
            "uzp2 v10.2d, v6.2d, v9.2d\n"
            "scvtf v2.4s, v2.4s, #0x4\n"
            "scvtf v10.4s, v10.4s, #0x4\n"
            "fmla v25.4s, v2.4s, v1.4s\n"
            "fmla v24.4s, v10.4s, v1.4s\n"
            "ldr q8, [x22, #0x0]\n"
            "ldr q7, [x22, #0x10]\n"
            "movi v9.4s, #0x0\n"
            "movi v6.4s, #0x0\n"
            "ldr q2, [x22, #0x20]\n"
            "ldr q5, [x22, #0x30]\n"
            "movi v10.4s, #0x0\n"
            ".inst 0x4e8ea509  // smmla v9.4s, v8.16b, v14.16b\n"
            ".inst 0x4e83a506  // smmla v6.4s, v8.16b, v3.16b\n"
            "ldr q8, [x22, #0x40]\n"
            ".inst 0x4e8ea4ea  // smmla v10.4s, v7.16b, v14.16b\n"
            ".inst 0x4e8ca449  // smmla v9.4s, v2.16b, v12.16b\n"
            ".inst 0x4e80a446  // smmla v6.4s, v2.16b, v0.16b\n"
            "ldr q2, [x22, #0x50]\n"
            ".inst 0x4e8ca4aa  // smmla v10.4s, v5.16b, v12.16b\n"
            ".inst 0x4e8ba509  // smmla v9.4s, v8.16b, v11.16b\n"
            ".inst 0x4e84a506  // smmla v6.4s, v8.16b, v4.16b\n"
            "ldr q8, [x22, #0x60]\n"
            ".inst 0x4e8ba44a  // smmla v10.4s, v2.16b, v11.16b\n"
            ".inst 0x4e8fa509  // smmla v9.4s, v8.16b, v15.16b\n"
            ".inst 0x4e8da506  // smmla v6.4s, v8.16b, v13.16b\n"
            "movi v8.4s, #0x0\n"
            ".inst 0x4e83a4e8  // smmla v8.4s, v7.16b, v3.16b\n"
            "ldr q7, [x22, #0x70]\n"
            "add x22, x22, #0x80\n"
            ".inst 0x4e8fa4ea  // smmla v10.4s, v7.16b, v15.16b\n"
            ".inst 0x4e80a4a8  // smmla v8.4s, v5.16b, v0.16b\n"
            "uzp1 v5.2d, v9.2d, v6.2d\n"
            "uzp2 v9.2d, v9.2d, v6.2d\n"
            "scvtf v5.4s, v5.4s, #0x4\n"
            ".inst 0x4e84a448  // smmla v8.4s, v2.16b, v4.16b\n"
            "scvtf v9.4s, v9.4s, #0x4\n"
            "fmla v23.4s, v5.4s, v1.4s\n"
            ".inst 0x4e8da4e8  // smmla v8.4s, v7.16b, v13.16b\n"
            "fmla v22.4s, v9.4s, v1.4s\n"
            "uzp1 v2.2d, v10.2d, v8.2d\n"
            "uzp2 v10.2d, v10.2d, v8.2d\n"
            "scvtf v2.4s, v2.4s, #0x4\n"
            "scvtf v10.4s, v10.4s, #0x4\n"
            "fmla v21.4s, v2.4s, v1.4s\n"
            "fmla v20.4s, v10.4s, v1.4s\n"
            "ldr q2, [x21, #0x0]\n"
            "ldr q10, [x21, #0x10]\n"
            "movi v6.4s, #0x0\n"
            "movi v9.4s, #0x0\n"
            "ldr q5, [x21, #0x20]\n"
            "ldr q8, [x21, #0x30]\n"
            "movi v7.4s, #0x0\n"
            ".inst 0x4e8ea446  // smmla v6.4s, v2.16b, v14.16b\n"
            ".inst 0x4e83a449  // smmla v9.4s, v2.16b, v3.16b\n"
            "ldr q2, [x21, #0x40]\n"
            ".inst 0x4e8ea547  // smmla v7.4s, v10.16b, v14.16b\n"
            "ldr q14, [x21, #0x50]\n"
            ".inst 0x4e8ca4a6  // smmla v6.4s, v5.16b, v12.16b\n"
            ".inst 0x4e80a4a9  // smmla v9.4s, v5.16b, v0.16b\n"
            "ldr q5, [x21, #0x60]\n"
            ".inst 0x4e8ca507  // smmla v7.4s, v8.16b, v12.16b\n"
            "ldr q12, [x21, #0x70]\n"
            "add x21, x21, #0x80\n"
            ".inst 0x4e8ba446  // smmla v6.4s, v2.16b, v11.16b\n"
            ".inst 0x4e84a449  // smmla v9.4s, v2.16b, v4.16b\n"
            "movi v2.4s, #0x0\n"
            ".inst 0x4e83a542  // smmla v2.4s, v10.16b, v3.16b\n"
            ".inst 0x4e8ba5c7  // smmla v7.4s, v14.16b, v11.16b\n"
            ".inst 0x4e8fa4a6  // smmla v6.4s, v5.16b, v15.16b\n"
            ".inst 0x4e80a502  // smmla v2.4s, v8.16b, v0.16b\n"
            ".inst 0x4e8da4a9  // smmla v9.4s, v5.16b, v13.16b\n"
            ".inst 0x4e8fa587  // smmla v7.4s, v12.16b, v15.16b\n"
            ".inst 0x4e84a5c2  // smmla v2.4s, v14.16b, v4.16b\n"
            "uzp1 v11.2d, v6.2d, v9.2d\n"
            "uzp2 v14.2d, v6.2d, v9.2d\n"
            "scvtf v11.4s, v11.4s, #0x4\n"
            ".inst 0x4e8da582  // smmla v2.4s, v12.16b, v13.16b\n"
            "scvtf v14.4s, v14.4s, #0x4\n"
            "fmla v19.4s, v11.4s, v1.4s\n"
            "uzp1 v9.2d, v7.2d, v2.2d\n"
            "uzp2 v0.2d, v7.2d, v2.2d\n"
            "fmla v18.4s, v14.4s, v1.4s\n"
            "scvtf v9.4s, v9.4s, #0x4\n"
            "scvtf v0.4s, v0.4s, #0x4\n"
            "fmla v17.4s, v9.4s, v1.4s\n"
            "fmla v16.4s, v0.4s, v1.4s\n"
            "subs x20, x20, #0x1\n"
            "bgt 3b\n"
            "ld1 { v11.4s }, [x27]\n"
            "ld1 { v10.4s }, [x23]\n"
            "add x27, x27, #0x10\n"
            "add x23, x23, #0x10\n"
            "ld1 { v9.4s }, [x22]\n"
            "ld1 { v8.4s }, [x21]\n"
            "add x22, x22, #0x10\n"
            "add x21, x21, #0x10\n"
            "ldr q7, [x11, #0x0]\n"
            "ldr q6, [x27, #0x0]\n"
            "add x20, %x[clamp_vals], #0x4\n"
            "cmp x10, #0x4\n"
            "ldr q5, [x23, #0x0]\n"
            "ldr q4, [x22, #0x0]\n"
            "scvtf v11.4s, v11.4s\n"
            "scvtf v10.4s, v10.4s\n"
            "ldr q3, [x21, #0x0]\n"
            "ldr q2, [x11, #0x10]\n"
            "scvtf v9.4s, v9.4s\n"
            "scvtf v8.4s, v8.4s\n"
            "ld1r { v1.4s }, [%x[clamp_vals]]\n"
            "ld1r { v0.4s }, [x20]\n"
            "add x11, x11, #0x20\n"
            "fmla v31.4s, v7.4s, v11.s[0]\n"
            "fmla v30.4s, v7.4s, v11.s[1]\n"
            "fmla v29.4s, v7.4s, v11.s[2]\n"
            "fmla v28.4s, v7.4s, v11.s[3]\n"
            "fmla v27.4s, v7.4s, v10.s[0]\n"
            "fmla v26.4s, v7.4s, v10.s[1]\n"
            "fmla v25.4s, v7.4s, v10.s[2]\n"
            "fmla v24.4s, v7.4s, v10.s[3]\n"
            "fmla v23.4s, v7.4s, v9.s[0]\n"
            "fmul v31.4s, v31.4s, v6.s[0]\n"
            "fmla v22.4s, v7.4s, v9.s[1]\n"
            "fmla v21.4s, v7.4s, v9.s[2]\n"
            "fmul v30.4s, v30.4s, v6.s[1]\n"
            "fmla v20.4s, v7.4s, v9.s[3]\n"
            "fmla v19.4s, v7.4s, v8.s[0]\n"
            "fmul v29.4s, v29.4s, v6.s[2]\n"
            "fmla v18.4s, v7.4s, v8.s[1]\n"
            "fmla v17.4s, v7.4s, v8.s[2]\n"
            "fmul v28.4s, v28.4s, v6.s[3]\n"
            "fmla v16.4s, v7.4s, v8.s[3]\n"
            "fmul v27.4s, v27.4s, v5.s[0]\n"
            "fmul v26.4s, v26.4s, v5.s[1]\n"
            "fmul v25.4s, v25.4s, v5.s[2]\n"
            "fmul v24.4s, v24.4s, v5.s[3]\n"
            "fmul v23.4s, v23.4s, v4.s[0]\n"
            "fmul v22.4s, v22.4s, v4.s[1]\n"
            "fmul v21.4s, v21.4s, v4.s[2]\n"
            "fmul v20.4s, v20.4s, v4.s[3]\n"
            "fmul v19.4s, v19.4s, v3.s[0]\n"
            "fmul v18.4s, v18.4s, v3.s[1]\n"
            "fmul v17.4s, v17.4s, v3.s[2]\n"
            "fmul v16.4s, v16.4s, v3.s[3]\n"
            "fadd v31.4s, v31.4s, v2.4s\n"
            "fadd v30.4s, v30.4s, v2.4s\n"
            "fadd v29.4s, v29.4s, v2.4s\n"
            "fadd v28.4s, v28.4s, v2.4s\n"
            "fadd v27.4s, v27.4s, v2.4s\n"
            "fadd v26.4s, v26.4s, v2.4s\n"
            "fadd v25.4s, v25.4s, v2.4s\n"
            "fadd v24.4s, v24.4s, v2.4s\n"
            "fadd v23.4s, v23.4s, v2.4s\n"
            "fadd v22.4s, v22.4s, v2.4s\n"
            "fadd v21.4s, v21.4s, v2.4s\n"
            "fadd v20.4s, v20.4s, v2.4s\n"
            "fadd v19.4s, v19.4s, v2.4s\n"
            "fadd v18.4s, v18.4s, v2.4s\n"
            "fadd v17.4s, v17.4s, v2.4s\n"
            "fadd v16.4s, v16.4s, v2.4s\n"
            "fmax v31.4s, v31.4s, v1.4s\n"
            "fmax v30.4s, v30.4s, v1.4s\n"
            "fmax v29.4s, v29.4s, v1.4s\n"
            "fmax v28.4s, v28.4s, v1.4s\n"
            "fmax v27.4s, v27.4s, v1.4s\n"
            "fmax v26.4s, v26.4s, v1.4s\n"
            "fmax v25.4s, v25.4s, v1.4s\n"
            "fmax v24.4s, v24.4s, v1.4s\n"
            "fmax v23.4s, v23.4s, v1.4s\n"
            "fmax v22.4s, v22.4s, v1.4s\n"
            "fmax v21.4s, v21.4s, v1.4s\n"
            "fmax v20.4s, v20.4s, v1.4s\n"
            "fmax v19.4s, v19.4s, v1.4s\n"
            "fmax v18.4s, v18.4s, v1.4s\n"
            "fmax v17.4s, v17.4s, v1.4s\n"
            "fmax v16.4s, v16.4s, v1.4s\n"
            "fmin v31.4s, v31.4s, v0.4s\n"
            "fmin v30.4s, v30.4s, v0.4s\n"
            "fmin v29.4s, v29.4s, v0.4s\n"
            "fmin v28.4s, v28.4s, v0.4s\n"
            "fmin v27.4s, v27.4s, v0.4s\n"
            "fmin v26.4s, v26.4s, v0.4s\n"
            "fmin v25.4s, v25.4s, v0.4s\n"
            "fmin v24.4s, v24.4s, v0.4s\n"
            "fmin v23.4s, v23.4s, v0.4s\n"
            "fmin v22.4s, v22.4s, v0.4s\n"
            "fmin v21.4s, v21.4s, v0.4s\n"
            "fmin v20.4s, v20.4s, v0.4s\n"
            "fmin v19.4s, v19.4s, v0.4s\n"
            "fmin v18.4s, v18.4s, v0.4s\n"
            "fmin v17.4s, v17.4s, v0.4s\n"
            "fmin v16.4s, v16.4s, v0.4s\n"
            "blt 8f\n"
            "mov x20, %x[dst]\n"
            "str q31, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q30, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q29, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q28, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q27, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q26, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q25, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q24, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q23, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q22, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q21, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q20, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q19, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q18, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q17, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q16, [x20, #0x0]\n"
            "b 13f\n"
            "8:"  // Partial output
            "mov x28, %x[dst]\n"
            "add x26, x28, %x[dst_stride_row], LSL #2\n"
            "add x25, x26, %x[dst_stride_row], LSL #1\n"
            "add x24, x26, %x[dst_stride_row]\n"
            "add x23, x25, %x[dst_stride_row]\n"
            "add x22, x28, %x[dst_stride_row], LSL #1\n"
            "add x21, x28, %x[dst_stride_row]\n"
            "add x20, x22, %x[dst_stride_row]\n"
            "add x27, x23, %x[dst_stride_row]\n"
            "tbz x10, #1, 9f\n"
            "st1 { v24.d }[0], [x23], #0x8\n"
            "st1 { v25.d }[0], [x25], #0x8\n"
            "st1 { v26.d }[0], [x24], #0x8\n"
            "st1 { v27.d }[0], [x26], #0x8\n"
            "st1 { v28.d }[0], [x20], #0x8\n"
            "st1 { v29.d }[0], [x22], #0x8\n"
            "st1 { v30.d }[0], [x21], #0x8\n"
            "st1 { v31.d }[0], [x28], #0x8\n"
            "tbz x10, #0, 10f\n"
            "st1 { v24.s }[2], [x23]\n"
            "st1 { v25.s }[2], [x25]\n"
            "st1 { v26.s }[2], [x24]\n"
            "st1 { v27.s }[2], [x26]\n"
            "st1 { v28.s }[2], [x20]\n"
            "st1 { v29.s }[2], [x22]\n"
            "st1 { v30.s }[2], [x21]\n"
            "st1 { v31.s }[2], [x28]\n"
            "b 10f\n"
            "9:"  // Output block 0: partial_1_0
            "st1 { v24.s }[0], [x23]\n"
            "st1 { v25.s }[0], [x25]\n"
            "st1 { v26.s }[0], [x24]\n"
            "st1 { v27.s }[0], [x26]\n"
            "st1 { v28.s }[0], [x20]\n"
            "st1 { v29.s }[0], [x22]\n"
            "st1 { v30.s }[0], [x21]\n"
            "st1 { v31.s }[0], [x28]\n"
            "10:"  // Output block 0: Done
            "add x26, x27, %x[dst_stride_row], LSL #2\n"
            "add x25, x27, %x[dst_stride_row], LSL #1\n"
            "add x24, x26, %x[dst_stride_row], LSL #1\n"
            "add x23, x27, %x[dst_stride_row]\n"
            "add x22, x25, %x[dst_stride_row]\n"
            "add x21, x26, %x[dst_stride_row]\n"
            "add x20, x24, %x[dst_stride_row]\n"
            "tbz x10, #1, 11f\n"
            "st1 { v16.d }[0], [x20], #0x8\n"
            "st1 { v17.d }[0], [x24], #0x8\n"
            "st1 { v18.d }[0], [x21], #0x8\n"
            "st1 { v19.d }[0], [x26], #0x8\n"
            "st1 { v20.d }[0], [x22], #0x8\n"
            "st1 { v21.d }[0], [x25], #0x8\n"
            "st1 { v22.d }[0], [x23], #0x8\n"
            "st1 { v23.d }[0], [x27], #0x8\n"
            "tbz x10, #0, 12f\n"
            "st1 { v16.s }[2], [x20]\n"
            "st1 { v17.s }[2], [x24]\n"
            "st1 { v18.s }[2], [x21]\n"
            "st1 { v19.s }[2], [x26]\n"
            "st1 { v20.s }[2], [x22]\n"
            "st1 { v21.s }[2], [x25]\n"
            "st1 { v22.s }[2], [x23]\n"
            "st1 { v23.s }[2], [x27]\n"
            "b 12f\n"
            "11:"  // Output block 1: partial_1_0
            "st1 { v16.s }[0], [x20]\n"
            "st1 { v17.s }[0], [x24]\n"
            "st1 { v18.s }[0], [x21]\n"
            "st1 { v19.s }[0], [x26]\n"
            "st1 { v20.s }[0], [x22]\n"
            "st1 { v21.s }[0], [x25]\n"
            "st1 { v22.s }[0], [x23]\n"
            "st1 { v23.s }[0], [x27]\n"
            "12:"  // Output block 1: Done
            "13:"  // Output stage exit
            "subs x10, x10, #0x4\n"
            "add %x[dst], %x[dst], #0x10\n"
            "bgt 2b\n"
            "mov x20, #0x4\n"
            "sub x13, x13, #0x10\n"
            "cmp x13, #0x10\n"
            "mov %x[dst], x9\n"
            "madd %x[lhs_packed], x20, x12, %x[lhs_packed]\n"
            "bge 1b\n"
            "14:"  // Row loop skip
            "cbz x13, 23f\n"
            "15:"  // Row tail: Row loop
            "mov x26, %x[rhs_packed]\n"
            "mov x25, %x[n]\n"
            "add x24, %x[dst], %x[dst_stride_row], LSL #2\n"
            "16:"  // Row tail: Column loop
            "movi v31.16b, #0x0\n"
            "movi v30.16b, #0x0\n"
            "mov x27, %x[lhs_packed]\n"
            "mov x20, %x[num_blocks]\n"
            "movi v29.16b, #0x0\n"
            "movi v28.16b, #0x0\n"
            "17:"  // Row tail: Block loop
            "ldr q9, [x26, #0x0]\n"
            "ldr q8, [x26, #0x10]\n"
            "movi v7.4s, #0x0\n"
            "movi v6.4s, #0x0\n"
            "ldr q5, [x27, #0x0]\n"
            "ldr q4, [x27, #0x10]\n"
            "movi v3.4s, #0x0\n"
            "movi v2.4s, #0x0\n"
            "ldr q1, [x26, #0x20]\n"
            "ldr q0, [x26, #0x30]\n"
            "movi v27.16b, #0xf0\n"
            "add x26, x26, #0x40\n"
            "ldr q26, [x27, #0x20]\n"
            "ldr q25, [x27, #0x30]\n"
            "shl v24.16b, v9.16b, #0x4\n"
            "shl v20.16b, v8.16b, #0x4\n"
            "ldr q23, [x27, #0x40]\n"
            "ldr q22, [x27, #0x50]\n"
            "and v9.16b, v9.16b, v27.16b\n"
            "and v8.16b, v8.16b, v27.16b\n"
            "ldr q21, [x27, #0x60]\n"
            "ldr q19, [x27, #0x70]\n"
            "shl v18.16b, v1.16b, #0x4\n"
            "shl v17.16b, v0.16b, #0x4\n"
            "ldr d16, [x26, #0x0]\n"
            ".inst 0x4e98a4a7  // smmla v7.4s, v5.16b, v24.16b\n"
            ".inst 0x4e94a4a6  // smmla v6.4s, v5.16b, v20.16b\n"
            "and v1.16b, v1.16b, v27.16b\n"
            ".inst 0x4e98a483  // smmla v3.4s, v4.16b, v24.16b\n"
            ".inst 0x4e94a482  // smmla v2.4s, v4.16b, v20.16b\n"
            "and v0.16b, v0.16b, v27.16b\n"
            "add x26, x26, #0x8\n"
            "add x27, x27, #0x80\n"
            "shll v20.4s, v16.4h, #0x10\n"
            ".inst 0x4e92a747  // smmla v7.4s, v26.16b, v18.16b\n"
            ".inst 0x4e91a746  // smmla v6.4s, v26.16b, v17.16b\n"
            ".inst 0x4e92a723  // smmla v3.4s, v25.16b, v18.16b\n"
            ".inst 0x4e91a722  // smmla v2.4s, v25.16b, v17.16b\n"
            ".inst 0x4e89a6e7  // smmla v7.4s, v23.16b, v9.16b\n"
            ".inst 0x4e88a6e6  // smmla v6.4s, v23.16b, v8.16b\n"
            ".inst 0x4e89a6c3  // smmla v3.4s, v22.16b, v9.16b\n"
            ".inst 0x4e88a6c2  // smmla v2.4s, v22.16b, v8.16b\n"
            ".inst 0x4e81a6a7  // smmla v7.4s, v21.16b, v1.16b\n"
            ".inst 0x4e80a6a6  // smmla v6.4s, v21.16b, v0.16b\n"
            ".inst 0x4e81a663  // smmla v3.4s, v19.16b, v1.16b\n"
            ".inst 0x4e80a662  // smmla v2.4s, v19.16b, v0.16b\n"
            "uzp1 v19.2d, v7.2d, v6.2d\n"
            "uzp2 v18.2d, v7.2d, v6.2d\n"
            "scvtf v19.4s, v19.4s, #0x4\n"
            "uzp1 v17.2d, v3.2d, v2.2d\n"
            "uzp2 v16.2d, v3.2d, v2.2d\n"
            "scvtf v18.4s, v18.4s, #0x4\n"
            "fmla v31.4s, v19.4s, v20.4s\n"
            "scvtf v17.4s, v17.4s, #0x4\n"
            "scvtf v16.4s, v16.4s, #0x4\n"
            "fmla v30.4s, v18.4s, v20.4s\n"
            "fmla v29.4s, v17.4s, v20.4s\n"
            "fmla v28.4s, v16.4s, v20.4s\n"
            "subs x20, x20, #0x1\n"
            "bgt 17b\n"
            "ld1 { v21.4s }, [x27]\n"
            "ldr q20, [x26, #0x0]\n"
            "add x27, x27, #0x10\n"
            "add x20, %x[clamp_vals], #0x4\n"
            "ldr q19, [x27, #0x0]\n"
            "ldr q18, [x26, #0x10]\n"
            "cmp x25, #0x4\n"
            "add x26, x26, #0x20\n"
            "ld1r { v17.4s }, [%x[clamp_vals]]\n"
            "ld1r { v16.4s }, [x20]\n"
            "scvtf v21.4s, v21.4s\n"
            "fmla v31.4s, v20.4s, v21.s[0]\n"
            "fmla v30.4s, v20.4s, v21.s[1]\n"
            "fmla v29.4s, v20.4s, v21.s[2]\n"
            "fmla v28.4s, v20.4s, v21.s[3]\n"
            "fmul v31.4s, v31.4s, v19.s[0]\n"
            "fmul v30.4s, v30.4s, v19.s[1]\n"
            "fmul v29.4s, v29.4s, v19.s[2]\n"
            "fadd v31.4s, v31.4s, v18.4s\n"
            "fmul v28.4s, v28.4s, v19.s[3]\n"
            "fadd v30.4s, v30.4s, v18.4s\n"
            "fadd v29.4s, v29.4s, v18.4s\n"
            "fadd v28.4s, v28.4s, v18.4s\n"
            "fmax v31.4s, v31.4s, v17.4s\n"
            "fmax v30.4s, v30.4s, v17.4s\n"
            "fmax v29.4s, v29.4s, v17.4s\n"
            "fmax v28.4s, v28.4s, v17.4s\n"
            "fmin v31.4s, v31.4s, v16.4s\n"
            "fmin v30.4s, v30.4s, v16.4s\n"
            "fmin v29.4s, v29.4s, v16.4s\n"
            "fmin v28.4s, v28.4s, v16.4s\n"
            "blt 19f\n"
            "mov x20, %x[dst]\n"
            "cmp x13, #0x1\n"
            "str q31, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "ble 22f\n"
            "cmp x13, #0x2\n"
            "str q30, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "ble 22f\n"
            "cmp x13, #0x3\n"
            "str q29, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "ble 22f\n"
            "str q28, [x20, #0x0]\n"
            "b 22f\n"
            "19:"  // Row tail: Partial output
            "mov x23, %x[dst]\n"
            "cmp x13, #0x1\n"
            "add x22, x23, %x[dst_stride_row]\n"
            "csel x22, x22, x23, GT\n"
            "cmp x13, #0x2\n"
            "add x21, x23, %x[dst_stride_row], LSL #1\n"
            "csel x21, x21, x22, GT\n"
            "cmp x13, #0x3\n"
            "add x20, x21, %x[dst_stride_row]\n"
            "csel x20, x20, x21, GT\n"
            "tbz x25, #1, 20f\n"
            "st1 { v28.d }[0], [x20], #0x8\n"
            "st1 { v29.d }[0], [x21], #0x8\n"
            "st1 { v30.d }[0], [x22], #0x8\n"
            "st1 { v31.d }[0], [x23], #0x8\n"
            "tbz x25, #0, 21f\n"
            "st1 { v28.s }[2], [x20]\n"
            "st1 { v29.s }[2], [x21]\n"
            "st1 { v30.s }[2], [x22]\n"
            "st1 { v31.s }[2], [x23]\n"
            "b 21f\n"
            "20:"  // Row tail: Output block 0: partial_1_0
            "st1 { v28.s }[0], [x20]\n"
            "st1 { v29.s }[0], [x21]\n"
            "st1 { v30.s }[0], [x22]\n"
            "st1 { v31.s }[0], [x23]\n"
            "21:"  // Row tail: Output block 0: Done
            "22:"  // Row tail: Output stage exit
            "subs x25, x25, #0x4\n"
            "add %x[dst], %x[dst], #0x10\n"
            "bgt 16b\n"
            "subs x13, x13, #0x4\n"
            "add %x[lhs_packed], %x[lhs_packed], x12\n"
            "mov %x[dst], x24\n"
            "bgt 15b\n"
            "23:"  // Row tail: Row loop skip
            : [dst] "+&r"(dst), [lhs_packed] "+&r"(lhs_packed)
            : [clamp_vals] "r"(clamp_vals), [dst_stride_row] "r"(dst_stride_row), [m] "r"(m), [n] "r"(n),
              [num_blocks] "r"(num_blocks), [rhs_packed] "r"(rhs_packed)
            : "cc", "memory", "v0", "v1", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v2",
              "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v3", "v30", "v31", "v4", "v5",
              "v6", "v7", "v8", "v9", "x10", "x11", "x12", "x13", "x20", "x21", "x22", "x23", "x24", "x25", "x26",
              "x27", "x28", "x9");
    } else {
        __asm__ __volatile__(
            "mov x13, #0x80\n"
            "mov x12, %x[m]\n"
            "mov x20, #0x20\n"
            "sub SP, SP, #0x100\n"
            "mul x13, %x[num_subblocks], x13\n"
            "cmp x12, #0x10\n"
            "madd x13, %x[num_blocks], x13, x20\n"
            "blt 15f\n"
            "1:"  // Row loop
            "mov x11, %x[rhs_packed]\n"
            "mov x10, %x[n]\n"
            "add x9, %x[dst], %x[dst_stride_row], LSL #4\n"
            "2:"  // Column loop
            "mov x27, %x[lhs_packed]\n"
            "movi v29.4s, #0x0\n"
            "mov x24, %x[num_blocks]\n"
            "str q29, [SP, #0x0]\n"
            "str q29, [SP, #0x10]\n"
            "str q29, [SP, #0x20]\n"
            "add x23, x27, x13\n"
            "add x22, x23, x13\n"
            "str q29, [SP, #0x30]\n"
            "add x21, x22, x13\n"
            "str q29, [SP, #0x40]\n"
            "str q29, [SP, #0x50]\n"
            "str q29, [SP, #0x60]\n"
            "str q29, [SP, #0x70]\n"
            "str q29, [SP, #0x80]\n"
            "str q29, [SP, #0x90]\n"
            "str q29, [SP, #0xa0]\n"
            "str q29, [SP, #0xb0]\n"
            "str q29, [SP, #0xc0]\n"
            "str q29, [SP, #0xd0]\n"
            "str q29, [SP, #0xe0]\n"
            "str q29, [SP, #0xf0]\n"
            "3:"  // Block loop
            "movi v7.4s, #0x0\n"
            "movi v13.4s, #0x0\n"
            "mov x20, %x[num_subblocks]\n"
            "movi v29.4s, #0x0\n"
            "movi v12.4s, #0x0\n"
            "movi v28.4s, #0x0\n"
            "movi v15.4s, #0x0\n"
            "movi v2.4s, #0x0\n"
            "movi v22.4s, #0x0\n"
            "movi v30.4s, #0x0\n"
            "movi v26.4s, #0x0\n"
            "movi v6.4s, #0x0\n"
            "movi v10.4s, #0x0\n"
            "movi v9.4s, #0x0\n"
            "movi v18.4s, #0x0\n"
            "movi v0.4s, #0x0\n"
            "movi v14.4s, #0x0\n"
            "4:"  // Sub block loop
            "ldr q4, [x11, #0x0]\n"
            "ldr q3, [x11, #0x10]\n"
            "movi v31.16b, #0xf0\n"
            "subs x20, x20, #0x1\n"
            "ldr q27, [x27, #0x0]\n"
            "ldr q1, [x27, #0x10]\n"
            "ldr q19, [x23, #0x0]\n"
            "ldr q17, [x23, #0x10]\n"
            "ldr q21, [x22, #0x0]\n"
            "ldr q23, [x22, #0x10]\n"
            "shl v25.16b, v4.16b, #0x4\n"
            "shl v20.16b, v3.16b, #0x4\n"
            "ldr q5, [x21, #0x0]\n"
            "ldr q16, [x21, #0x10]\n"
            "and v4.16b, v4.16b, v31.16b\n"
            "and v3.16b, v3.16b, v31.16b\n"
            "ldr q8, [x11, #0x20]\n"
            "ldr q11, [x11, #0x30]\n"
            "add x11, x11, #0x40\n"
            "ldr q24, [x27, #0x20]\n"
            ".inst 0x4e99a767  // smmla v7.4s, v27.16b, v25.16b\n"
            ".inst 0x4e94a76d  // smmla v13.4s, v27.16b, v20.16b\n"
            "ldr q27, [x27, #0x30]\n"
            ".inst 0x4e99a43d  // smmla v29.4s, v1.16b, v25.16b\n"
            ".inst 0x4e94a42c  // smmla v12.4s, v1.16b, v20.16b\n"
            "ldr q1, [x23, #0x20]\n"
            ".inst 0x4e99a67c  // smmla v28.4s, v19.16b, v25.16b\n"
            ".inst 0x4e94a66f  // smmla v15.4s, v19.16b, v20.16b\n"
            "ldr q19, [x23, #0x30]\n"
            ".inst 0x4e99a622  // smmla v2.4s, v17.16b, v25.16b\n"
            ".inst 0x4e94a636  // smmla v22.4s, v17.16b, v20.16b\n"
            "ldr q17, [x22, #0x20]\n"
            ".inst 0x4e99a6be  // smmla v30.4s, v21.16b, v25.16b\n"
            ".inst 0x4e94a6ba  // smmla v26.4s, v21.16b, v20.16b\n"
            "ldr q21, [x22, #0x30]\n"
            ".inst 0x4e99a6e6  // smmla v6.4s, v23.16b, v25.16b\n"
            ".inst 0x4e94a6ea  // smmla v10.4s, v23.16b, v20.16b\n"
            "ldr q23, [x21, #0x20]\n"
            ".inst 0x4e99a4a9  // smmla v9.4s, v5.16b, v25.16b\n"
            ".inst 0x4e94a4b2  // smmla v18.4s, v5.16b, v20.16b\n"
            "ldr q5, [x21, #0x30]\n"
            ".inst 0x4e99a600  // smmla v0.4s, v16.16b, v25.16b\n"
            "ldr q25, [x27, #0x40]\n"
            ".inst 0x4e94a60e  // smmla v14.4s, v16.16b, v20.16b\n"
            "ldr q16, [x27, #0x50]\n"
            "shl v20.16b, v8.16b, #0x4\n"
            "and v8.16b, v8.16b, v31.16b\n"
            ".inst 0x4e94a707  // smmla v7.4s, v24.16b, v20.16b\n"
            ".inst 0x4e94a77d  // smmla v29.4s, v27.16b, v20.16b\n"
            ".inst 0x4e94a43c  // smmla v28.4s, v1.16b, v20.16b\n"
            ".inst 0x4e94a662  // smmla v2.4s, v19.16b, v20.16b\n"
            ".inst 0x4e94a63e  // smmla v30.4s, v17.16b, v20.16b\n"
            ".inst 0x4e94a6a6  // smmla v6.4s, v21.16b, v20.16b\n"
            ".inst 0x4e94a6e9  // smmla v9.4s, v23.16b, v20.16b\n"
            ".inst 0x4e94a4a0  // smmla v0.4s, v5.16b, v20.16b\n"
            "shl v20.16b, v11.16b, #0x4\n"
            ".inst 0x4e84a727  // smmla v7.4s, v25.16b, v4.16b\n"
            ".inst 0x4e84a61d  // smmla v29.4s, v16.16b, v4.16b\n"
            "and v11.16b, v11.16b, v31.16b\n"
            "ldr q31, [x23, #0x40]\n"
            ".inst 0x4e94a70d  // smmla v13.4s, v24.16b, v20.16b\n"
            "ldr q24, [x23, #0x50]\n"
            ".inst 0x4e94a76c  // smmla v12.4s, v27.16b, v20.16b\n"
            "ldr q27, [x22, #0x40]\n"
            ".inst 0x4e94a42f  // smmla v15.4s, v1.16b, v20.16b\n"
            "ldr q1, [x22, #0x50]\n"
            ".inst 0x4e94a676  // smmla v22.4s, v19.16b, v20.16b\n"
            "ldr q19, [x21, #0x40]\n"
            ".inst 0x4e94a63a  // smmla v26.4s, v17.16b, v20.16b\n"
            "ldr q17, [x21, #0x50]\n"
            ".inst 0x4e94a6aa  // smmla v10.4s, v21.16b, v20.16b\n"
            "ldr q21, [x27, #0x60]\n"
            ".inst 0x4e94a6f2  // smmla v18.4s, v23.16b, v20.16b\n"
            "ldr q23, [x27, #0x70]\n"
            ".inst 0x4e94a4ae  // smmla v14.4s, v5.16b, v20.16b\n"
            "ldr q20, [x23, #0x60]\n"
            ".inst 0x4e83a72d  // smmla v13.4s, v25.16b, v3.16b\n"
            "ldr q5, [x23, #0x70]\n"
            "ldr q25, [x22, #0x60]\n"
            ".inst 0x4e83a60c  // smmla v12.4s, v16.16b, v3.16b\n"
            ".inst 0x4e84a7fc  // smmla v28.4s, v31.16b, v4.16b\n"
            "ldr q16, [x22, #0x70]\n"
            ".inst 0x4e83a7ef  // smmla v15.4s, v31.16b, v3.16b\n"
            "ldr q31, [x21, #0x60]\n"
            ".inst 0x4e84a702  // smmla v2.4s, v24.16b, v4.16b\n"
            ".inst 0x4e83a716  // smmla v22.4s, v24.16b, v3.16b\n"
            "ldr q24, [x21, #0x70]\n"
            ".inst 0x4e84a77e  // smmla v30.4s, v27.16b, v4.16b\n"
            "add x27, x27, #0x80\n"
            ".inst 0x4e83a77a  // smmla v26.4s, v27.16b, v3.16b\n"
            ".inst 0x4e84a426  // smmla v6.4s, v1.16b, v4.16b\n"
            "add x23, x23, #0x80\n"
            "add x22, x22, #0x80\n"
            ".inst 0x4e83a42a  // smmla v10.4s, v1.16b, v3.16b\n"
            ".inst 0x4e84a669  // smmla v9.4s, v19.16b, v4.16b\n"
            "add x21, x21, #0x80\n"
            ".inst 0x4e83a672  // smmla v18.4s, v19.16b, v3.16b\n"
            ".inst 0x4e84a620  // smmla v0.4s, v17.16b, v4.16b\n"
            ".inst 0x4e83a62e  // smmla v14.4s, v17.16b, v3.16b\n"
            ".inst 0x4e88a6a7  // smmla v7.4s, v21.16b, v8.16b\n"
            ".inst 0x4e8ba6ad  // smmla v13.4s, v21.16b, v11.16b\n"
            ".inst 0x4e88a6fd  // smmla v29.4s, v23.16b, v8.16b\n"
            ".inst 0x4e8ba6ec  // smmla v12.4s, v23.16b, v11.16b\n"
            ".inst 0x4e88a69c  // smmla v28.4s, v20.16b, v8.16b\n"
            ".inst 0x4e8ba68f  // smmla v15.4s, v20.16b, v11.16b\n"
            ".inst 0x4e88a4a2  // smmla v2.4s, v5.16b, v8.16b\n"
            ".inst 0x4e8ba4b6  // smmla v22.4s, v5.16b, v11.16b\n"
            ".inst 0x4e88a73e  // smmla v30.4s, v25.16b, v8.16b\n"
            ".inst 0x4e8ba73a  // smmla v26.4s, v25.16b, v11.16b\n"
            ".inst 0x4e88a606  // smmla v6.4s, v16.16b, v8.16b\n"
            ".inst 0x4e8ba60a  // smmla v10.4s, v16.16b, v11.16b\n"
            ".inst 0x4e88a7e9  // smmla v9.4s, v31.16b, v8.16b\n"
            ".inst 0x4e8ba7f2  // smmla v18.4s, v31.16b, v11.16b\n"
            ".inst 0x4e88a700  // smmla v0.4s, v24.16b, v8.16b\n"
            ".inst 0x4e8ba70e  // smmla v14.4s, v24.16b, v11.16b\n"
            "bgt 4b\n"
            "ldr d4, [x11, #0x0]\n"
            "ldr q23, [SP, #0x0]\n"
            "uzp1 v16.2d, v7.2d, v13.2d\n"
            "uzp2 v19.2d, v7.2d, v13.2d\n"
            "uzp1 v20.2d, v29.2d, v12.2d\n"
            "uzp2 v17.2d, v29.2d, v12.2d\n"
            "add x11, x11, #0x8\n"
            "shll v24.4s, v4.4h, #0x10\n"
            "scvtf v16.4s, v16.4s, #0x4\n"
            "scvtf v19.4s, v19.4s, #0x4\n"
            "scvtf v20.4s, v20.4s, #0x4\n"
            "scvtf v17.4s, v17.4s, #0x4\n"
            "fmla v23.4s, v16.4s, v24.4s\n"
            "str q23, [SP, #0x0]\n"
            "ldr q16, [SP, #0x10]\n"
            "fmla v16.4s, v19.4s, v24.4s\n"
            "str q16, [SP, #0x10]\n"
            "ldr q16, [SP, #0x20]\n"
            "fmla v16.4s, v20.4s, v24.4s\n"
            "str q16, [SP, #0x20]\n"
            "ldr q16, [SP, #0x30]\n"
            "fmla v16.4s, v17.4s, v24.4s\n"
            "str q16, [SP, #0x30]\n"
            "ldr q1, [SP, #0x40]\n"
            "uzp1 v16.2d, v28.2d, v15.2d\n"
            "uzp2 v19.2d, v28.2d, v15.2d\n"
            "uzp1 v5.2d, v2.2d, v22.2d\n"
            "uzp2 v17.2d, v2.2d, v22.2d\n"
            "scvtf v16.4s, v16.4s, #0x4\n"
            "scvtf v19.4s, v19.4s, #0x4\n"
            "scvtf v5.4s, v5.4s, #0x4\n"
            "scvtf v17.4s, v17.4s, #0x4\n"
            "fmla v1.4s, v16.4s, v24.4s\n"
            "str q1, [SP, #0x40]\n"
            "ldr q16, [SP, #0x50]\n"
            "fmla v16.4s, v19.4s, v24.4s\n"
            "str q16, [SP, #0x50]\n"
            "ldr q16, [SP, #0x60]\n"
            "fmla v16.4s, v5.4s, v24.4s\n"
            "str q16, [SP, #0x60]\n"
            "ldr q16, [SP, #0x70]\n"
            "fmla v16.4s, v17.4s, v24.4s\n"
            "str q16, [SP, #0x70]\n"
            "ldr q1, [SP, #0x80]\n"
            "uzp1 v16.2d, v30.2d, v26.2d\n"
            "uzp2 v19.2d, v30.2d, v26.2d\n"
            "uzp1 v30.2d, v6.2d, v10.2d\n"
            "uzp2 v17.2d, v6.2d, v10.2d\n"
            "scvtf v16.4s, v16.4s, #0x4\n"
            "scvtf v19.4s, v19.4s, #0x4\n"
            "scvtf v30.4s, v30.4s, #0x4\n"
            "scvtf v17.4s, v17.4s, #0x4\n"
            "fmla v1.4s, v16.4s, v24.4s\n"
            "str q1, [SP, #0x80]\n"
            "ldr q16, [SP, #0x90]\n"
            "fmla v16.4s, v19.4s, v24.4s\n"
            "str q16, [SP, #0x90]\n"
            "ldr q16, [SP, #0xa0]\n"
            "fmla v16.4s, v30.4s, v24.4s\n"
            "str q16, [SP, #0xa0]\n"
            "ldr q16, [SP, #0xb0]\n"
            "fmla v16.4s, v17.4s, v24.4s\n"
            "str q16, [SP, #0xb0]\n"
            "ldr q31, [SP, #0xc0]\n"
            "uzp1 v16.2d, v9.2d, v18.2d\n"
            "uzp2 v19.2d, v9.2d, v18.2d\n"
            "uzp1 v21.2d, v0.2d, v14.2d\n"
            "uzp2 v17.2d, v0.2d, v14.2d\n"
            "scvtf v16.4s, v16.4s, #0x4\n"
            "scvtf v19.4s, v19.4s, #0x4\n"
            "scvtf v21.4s, v21.4s, #0x4\n"
            "scvtf v17.4s, v17.4s, #0x4\n"
            "fmla v31.4s, v16.4s, v24.4s\n"
            "str q31, [SP, #0xc0]\n"
            "ldr q16, [SP, #0xd0]\n"
            "fmla v16.4s, v19.4s, v24.4s\n"
            "str q16, [SP, #0xd0]\n"
            "ldr q16, [SP, #0xe0]\n"
            "fmla v16.4s, v21.4s, v24.4s\n"
            "str q16, [SP, #0xe0]\n"
            "ldr q16, [SP, #0xf0]\n"
            "fmla v16.4s, v17.4s, v24.4s\n"
            "str q16, [SP, #0xf0]\n"
            "subs x24, x24, #0x1\n"
            "bgt 3b\n"
            "ld1 { v11.4s }, [x27]\n"
            "ld1 { v10.4s }, [x23]\n"
            "add x27, x27, #0x10\n"
            "add x23, x23, #0x10\n"
            "ld1 { v9.4s }, [x22]\n"
            "ld1 { v8.4s }, [x21]\n"
            "add x22, x22, #0x10\n"
            "add x21, x21, #0x10\n"
            "ldr q31, [SP, #0x0]\n"
            "ldr q30, [SP, #0x10]\n"
            "add x20, %x[clamp_vals], #0x4\n"
            "cmp x10, #0x4\n"
            "ldr q29, [SP, #0x20]\n"
            "ldr q28, [SP, #0x30]\n"
            "scvtf v11.4s, v11.4s\n"
            "scvtf v10.4s, v10.4s\n"
            "ldr q27, [SP, #0x40]\n"
            "ldr q26, [SP, #0x50]\n"
            "scvtf v9.4s, v9.4s\n"
            "scvtf v8.4s, v8.4s\n"
            "ldr q25, [SP, #0x60]\n"
            "ldr q24, [SP, #0x70]\n"
            "ldr q23, [SP, #0x80]\n"
            "ldr q22, [SP, #0x90]\n"
            "ldr q21, [SP, #0xa0]\n"
            "ldr q20, [SP, #0xb0]\n"
            "ldr q19, [SP, #0xc0]\n"
            "ldr q18, [SP, #0xd0]\n"
            "ldr q17, [SP, #0xe0]\n"
            "ldr q16, [SP, #0xf0]\n"
            "ldr q7, [x11, #0x0]\n"
            "ldr q6, [x27, #0x0]\n"
            "ldr q5, [x23, #0x0]\n"
            "ldr q4, [x22, #0x0]\n"
            "ldr q3, [x21, #0x0]\n"
            "ldr q2, [x11, #0x10]\n"
            "add x11, x11, #0x20\n"
            "ld1r { v1.4s }, [%x[clamp_vals]]\n"
            "ld1r { v0.4s }, [x20]\n"
            "fmla v31.4s, v7.4s, v11.s[0]\n"
            "fmla v30.4s, v7.4s, v11.s[1]\n"
            "fmla v29.4s, v7.4s, v11.s[2]\n"
            "fmla v28.4s, v7.4s, v11.s[3]\n"
            "fmla v27.4s, v7.4s, v10.s[0]\n"
            "fmla v26.4s, v7.4s, v10.s[1]\n"
            "fmla v25.4s, v7.4s, v10.s[2]\n"
            "fmla v24.4s, v7.4s, v10.s[3]\n"
            "fmla v23.4s, v7.4s, v9.s[0]\n"
            "fmla v22.4s, v7.4s, v9.s[1]\n"
            "fmul v31.4s, v31.4s, v6.s[0]\n"
            "fmla v21.4s, v7.4s, v9.s[2]\n"
            "fmla v20.4s, v7.4s, v9.s[3]\n"
            "fmul v30.4s, v30.4s, v6.s[1]\n"
            "fmla v19.4s, v7.4s, v8.s[0]\n"
            "fmla v18.4s, v7.4s, v8.s[1]\n"
            "fmul v29.4s, v29.4s, v6.s[2]\n"
            "fmla v17.4s, v7.4s, v8.s[2]\n"
            "fmla v16.4s, v7.4s, v8.s[3]\n"
            "fmul v28.4s, v28.4s, v6.s[3]\n"
            "fmul v27.4s, v27.4s, v5.s[0]\n"
            "fmul v26.4s, v26.4s, v5.s[1]\n"
            "fmul v25.4s, v25.4s, v5.s[2]\n"
            "fmul v24.4s, v24.4s, v5.s[3]\n"
            "fmul v23.4s, v23.4s, v4.s[0]\n"
            "fmul v22.4s, v22.4s, v4.s[1]\n"
            "fmul v21.4s, v21.4s, v4.s[2]\n"
            "fmul v20.4s, v20.4s, v4.s[3]\n"
            "fmul v19.4s, v19.4s, v3.s[0]\n"
            "fmul v18.4s, v18.4s, v3.s[1]\n"
            "fmul v17.4s, v17.4s, v3.s[2]\n"
            "fmul v16.4s, v16.4s, v3.s[3]\n"
            "fadd v31.4s, v31.4s, v2.4s\n"
            "fadd v30.4s, v30.4s, v2.4s\n"
            "fadd v29.4s, v29.4s, v2.4s\n"
            "fadd v28.4s, v28.4s, v2.4s\n"
            "fadd v27.4s, v27.4s, v2.4s\n"
            "fadd v26.4s, v26.4s, v2.4s\n"
            "fadd v25.4s, v25.4s, v2.4s\n"
            "fadd v24.4s, v24.4s, v2.4s\n"
            "fadd v23.4s, v23.4s, v2.4s\n"
            "fadd v22.4s, v22.4s, v2.4s\n"
            "fadd v21.4s, v21.4s, v2.4s\n"
            "fadd v20.4s, v20.4s, v2.4s\n"
            "fadd v19.4s, v19.4s, v2.4s\n"
            "fadd v18.4s, v18.4s, v2.4s\n"
            "fadd v17.4s, v17.4s, v2.4s\n"
            "fadd v16.4s, v16.4s, v2.4s\n"
            "fmax v31.4s, v31.4s, v1.4s\n"
            "fmax v30.4s, v30.4s, v1.4s\n"
            "fmax v29.4s, v29.4s, v1.4s\n"
            "fmax v28.4s, v28.4s, v1.4s\n"
            "fmax v27.4s, v27.4s, v1.4s\n"
            "fmax v26.4s, v26.4s, v1.4s\n"
            "fmax v25.4s, v25.4s, v1.4s\n"
            "fmax v24.4s, v24.4s, v1.4s\n"
            "fmax v23.4s, v23.4s, v1.4s\n"
            "fmax v22.4s, v22.4s, v1.4s\n"
            "fmax v21.4s, v21.4s, v1.4s\n"
            "fmax v20.4s, v20.4s, v1.4s\n"
            "fmax v19.4s, v19.4s, v1.4s\n"
            "fmax v18.4s, v18.4s, v1.4s\n"
            "fmax v17.4s, v17.4s, v1.4s\n"
            "fmax v16.4s, v16.4s, v1.4s\n"
            "fmin v31.4s, v31.4s, v0.4s\n"
            "fmin v30.4s, v30.4s, v0.4s\n"
            "fmin v29.4s, v29.4s, v0.4s\n"
            "fmin v28.4s, v28.4s, v0.4s\n"
            "fmin v27.4s, v27.4s, v0.4s\n"
            "fmin v26.4s, v26.4s, v0.4s\n"
            "fmin v25.4s, v25.4s, v0.4s\n"
            "fmin v24.4s, v24.4s, v0.4s\n"
            "fmin v23.4s, v23.4s, v0.4s\n"
            "fmin v22.4s, v22.4s, v0.4s\n"
            "fmin v21.4s, v21.4s, v0.4s\n"
            "fmin v20.4s, v20.4s, v0.4s\n"
            "fmin v19.4s, v19.4s, v0.4s\n"
            "fmin v18.4s, v18.4s, v0.4s\n"
            "fmin v17.4s, v17.4s, v0.4s\n"
            "fmin v16.4s, v16.4s, v0.4s\n"
            "blt 9f\n"
            "mov x20, %x[dst]\n"
            "str q31, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q30, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q29, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q28, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q27, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q26, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q25, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q24, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q23, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q22, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q21, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q20, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q19, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q18, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q17, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "str q16, [x20, #0x0]\n"
            "b 14f\n"
            "9:"  // Partial output
            "mov x28, %x[dst]\n"
            "add x26, x28, %x[dst_stride_row], LSL #2\n"
            "add x25, x26, %x[dst_stride_row], LSL #1\n"
            "add x24, x26, %x[dst_stride_row]\n"
            "add x23, x25, %x[dst_stride_row]\n"
            "add x22, x28, %x[dst_stride_row], LSL #1\n"
            "add x21, x28, %x[dst_stride_row]\n"
            "add x20, x22, %x[dst_stride_row]\n"
            "add x27, x23, %x[dst_stride_row]\n"
            "tbz x10, #1, 10f\n"
            "st1 { v24.d }[0], [x23], #0x8\n"
            "st1 { v25.d }[0], [x25], #0x8\n"
            "st1 { v26.d }[0], [x24], #0x8\n"
            "st1 { v27.d }[0], [x26], #0x8\n"
            "st1 { v28.d }[0], [x20], #0x8\n"
            "st1 { v29.d }[0], [x22], #0x8\n"
            "st1 { v30.d }[0], [x21], #0x8\n"
            "st1 { v31.d }[0], [x28], #0x8\n"
            "tbz x10, #0, 11f\n"
            "st1 { v24.s }[2], [x23]\n"
            "st1 { v25.s }[2], [x25]\n"
            "st1 { v26.s }[2], [x24]\n"
            "st1 { v27.s }[2], [x26]\n"
            "st1 { v28.s }[2], [x20]\n"
            "st1 { v29.s }[2], [x22]\n"
            "st1 { v30.s }[2], [x21]\n"
            "st1 { v31.s }[2], [x28]\n"
            "b 11f\n"
            "10:"  // Output block 0: partial_1_0
            "st1 { v24.s }[0], [x23]\n"
            "st1 { v25.s }[0], [x25]\n"
            "st1 { v26.s }[0], [x24]\n"
            "st1 { v27.s }[0], [x26]\n"
            "st1 { v28.s }[0], [x20]\n"
            "st1 { v29.s }[0], [x22]\n"
            "st1 { v30.s }[0], [x21]\n"
            "st1 { v31.s }[0], [x28]\n"
            "11:"  // Output block 0: Done
            "add x26, x27, %x[dst_stride_row], LSL #2\n"
            "add x25, x27, %x[dst_stride_row], LSL #1\n"
            "add x24, x26, %x[dst_stride_row], LSL #1\n"
            "add x23, x27, %x[dst_stride_row]\n"
            "add x22, x25, %x[dst_stride_row]\n"
            "add x21, x26, %x[dst_stride_row]\n"
            "add x20, x24, %x[dst_stride_row]\n"
            "tbz x10, #1, 12f\n"
            "st1 { v16.d }[0], [x20], #0x8\n"
            "st1 { v17.d }[0], [x24], #0x8\n"
            "st1 { v18.d }[0], [x21], #0x8\n"
            "st1 { v19.d }[0], [x26], #0x8\n"
            "st1 { v20.d }[0], [x22], #0x8\n"
            "st1 { v21.d }[0], [x25], #0x8\n"
            "st1 { v22.d }[0], [x23], #0x8\n"
            "st1 { v23.d }[0], [x27], #0x8\n"
            "tbz x10, #0, 13f\n"
            "st1 { v16.s }[2], [x20]\n"
            "st1 { v17.s }[2], [x24]\n"
            "st1 { v18.s }[2], [x21]\n"
            "st1 { v19.s }[2], [x26]\n"
            "st1 { v20.s }[2], [x22]\n"
            "st1 { v21.s }[2], [x25]\n"
            "st1 { v22.s }[2], [x23]\n"
            "st1 { v23.s }[2], [x27]\n"
            "b 13f\n"
            "12:"  // Output block 1: partial_1_0
            "st1 { v16.s }[0], [x20]\n"
            "st1 { v17.s }[0], [x24]\n"
            "st1 { v18.s }[0], [x21]\n"
            "st1 { v19.s }[0], [x26]\n"
            "st1 { v20.s }[0], [x22]\n"
            "st1 { v21.s }[0], [x25]\n"
            "st1 { v22.s }[0], [x23]\n"
            "st1 { v23.s }[0], [x27]\n"
            "13:"  // Output block 1: Done
            "14:"  // Output stage exit
            "subs x10, x10, #0x4\n"
            "add %x[dst], %x[dst], #0x10\n"
            "bgt 2b\n"
            "mov x20, #0x4\n"
            "sub x12, x12, #0x10\n"
            "cmp x12, #0x10\n"
            "mov %x[dst], x9\n"
            "madd %x[lhs_packed], x20, x13, %x[lhs_packed]\n"
            "bge 1b\n"
            "15:"  // Row loop skip
            "cbz x12, 25f\n"
            "16:"  // Row tail: Row loop
            "mov x26, %x[rhs_packed]\n"
            "mov x25, %x[n]\n"
            "add x24, %x[dst], %x[dst_stride_row], LSL #2\n"
            "17:"  // Row tail: Column loop
            "movi v16.4s, #0x0\n"
            "mov x27, %x[lhs_packed]\n"
            "mov x21, %x[num_blocks]\n"
            "str q16, [SP, #0x0]\n"
            "str q16, [SP, #0x10]\n"
            "str q16, [SP, #0x20]\n"
            "str q16, [SP, #0x30]\n"
            "18:"  // Row tail: Block loop
            "movi v7.4s, #0x0\n"
            "movi v13.4s, #0x0\n"
            "mov x20, %x[num_subblocks]\n"
            "movi v29.4s, #0x0\n"
            "movi v12.4s, #0x0\n"
            "19:"  // Row tail: Sub block loop
            "ldr q0, [x26, #0x0]\n"
            "ldr q31, [x26, #0x10]\n"
            "movi v30.16b, #0xf0\n"
            "subs x20, x20, #0x1\n"
            "ldr q18, [x27, #0x0]\n"
            "ldr q28, [x27, #0x10]\n"
            "ldr q27, [x26, #0x20]\n"
            "ldr q26, [x26, #0x30]\n"
            "add x26, x26, #0x40\n"
            "ldr q25, [x27, #0x20]\n"
            "ldr q24, [x27, #0x30]\n"
            "shl v23.16b, v0.16b, #0x4\n"
            "shl v22.16b, v31.16b, #0x4\n"
            "ldr q21, [x27, #0x40]\n"
            "ldr q20, [x27, #0x50]\n"
            "and v0.16b, v0.16b, v30.16b\n"
            "and v31.16b, v31.16b, v30.16b\n"
            "ldr q19, [x27, #0x60]\n"
            "ldr q14, [x27, #0x70]\n"
            "shl v17.16b, v27.16b, #0x4\n"
            "shl v16.16b, v26.16b, #0x4\n"
            ".inst 0x4e97a647  // smmla v7.4s, v18.16b, v23.16b\n"
            ".inst 0x4e96a64d  // smmla v13.4s, v18.16b, v22.16b\n"
            "and v27.16b, v27.16b, v30.16b\n"
            "add x27, x27, #0x80\n"
            ".inst 0x4e97a79d  // smmla v29.4s, v28.16b, v23.16b\n"
            ".inst 0x4e96a78c  // smmla v12.4s, v28.16b, v22.16b\n"
            "and v26.16b, v26.16b, v30.16b\n"
            ".inst 0x4e91a727  // smmla v7.4s, v25.16b, v17.16b\n"
            ".inst 0x4e90a72d  // smmla v13.4s, v25.16b, v16.16b\n"
            ".inst 0x4e91a71d  // smmla v29.4s, v24.16b, v17.16b\n"
            ".inst 0x4e90a70c  // smmla v12.4s, v24.16b, v16.16b\n"
            ".inst 0x4e80a6a7  // smmla v7.4s, v21.16b, v0.16b\n"
            ".inst 0x4e9fa6ad  // smmla v13.4s, v21.16b, v31.16b\n"
            ".inst 0x4e80a69d  // smmla v29.4s, v20.16b, v0.16b\n"
            ".inst 0x4e9fa68c  // smmla v12.4s, v20.16b, v31.16b\n"
            ".inst 0x4e9ba667  // smmla v7.4s, v19.16b, v27.16b\n"
            ".inst 0x4e9aa66d  // smmla v13.4s, v19.16b, v26.16b\n"
            ".inst 0x4e9ba5dd  // smmla v29.4s, v14.16b, v27.16b\n"
            ".inst 0x4e9aa5cc  // smmla v12.4s, v14.16b, v26.16b\n"
            "bgt 19b\n"
            "ldr d17, [x26, #0x0]\n"
            "ldr q21, [SP, #0x0]\n"
            "uzp1 v16.2d, v7.2d, v13.2d\n"
            "uzp2 v20.2d, v7.2d, v13.2d\n"
            "uzp1 v19.2d, v29.2d, v12.2d\n"
            "uzp2 v18.2d, v29.2d, v12.2d\n"
            "add x26, x26, #0x8\n"
            "shll v17.4s, v17.4h, #0x10\n"
            "scvtf v16.4s, v16.4s, #0x4\n"
            "scvtf v20.4s, v20.4s, #0x4\n"
            "scvtf v19.4s, v19.4s, #0x4\n"
            "scvtf v18.4s, v18.4s, #0x4\n"
            "fmla v21.4s, v16.4s, v17.4s\n"
            "str q21, [SP, #0x0]\n"
            "ldr q16, [SP, #0x10]\n"
            "fmla v16.4s, v20.4s, v17.4s\n"
            "str q16, [SP, #0x10]\n"
            "ldr q16, [SP, #0x20]\n"
            "fmla v16.4s, v19.4s, v17.4s\n"
            "str q16, [SP, #0x20]\n"
            "ldr q16, [SP, #0x30]\n"
            "fmla v16.4s, v18.4s, v17.4s\n"
            "str q16, [SP, #0x30]\n"
            "subs x21, x21, #0x1\n"
            "bgt 18b\n"
            "ld1 { v21.4s }, [x27]\n"
            "ldr q31, [SP, #0x0]\n"
            "add x27, x27, #0x10\n"
            "add x20, %x[clamp_vals], #0x4\n"
            "ldr q30, [SP, #0x10]\n"
            "ldr q29, [SP, #0x20]\n"
            "cmp x25, #0x4\n"
            "ldr q28, [SP, #0x30]\n"
            "ldr q20, [x26, #0x0]\n"
            "ldr q19, [x27, #0x0]\n"
            "ldr q18, [x26, #0x10]\n"
            "scvtf v21.4s, v21.4s\n"
            "add x26, x26, #0x20\n"
            "ld1r { v17.4s }, [%x[clamp_vals]]\n"
            "ld1r { v16.4s }, [x20]\n"
            "fmla v31.4s, v20.4s, v21.s[0]\n"
            "fmla v30.4s, v20.4s, v21.s[1]\n"
            "fmla v29.4s, v20.4s, v21.s[2]\n"
            "fmla v28.4s, v20.4s, v21.s[3]\n"
            "fmul v31.4s, v31.4s, v19.s[0]\n"
            "fmul v30.4s, v30.4s, v19.s[1]\n"
            "fadd v31.4s, v31.4s, v18.4s\n"
            "fmul v29.4s, v29.4s, v19.s[2]\n"
            "fmul v28.4s, v28.4s, v19.s[3]\n"
            "fadd v30.4s, v30.4s, v18.4s\n"
            "fmax v31.4s, v31.4s, v17.4s\n"
            "fadd v29.4s, v29.4s, v18.4s\n"
            "fadd v28.4s, v28.4s, v18.4s\n"
            "fmax v30.4s, v30.4s, v17.4s\n"
            "fmin v31.4s, v31.4s, v16.4s\n"
            "fmax v29.4s, v29.4s, v17.4s\n"
            "fmax v28.4s, v28.4s, v17.4s\n"
            "fmin v30.4s, v30.4s, v16.4s\n"
            "fmin v29.4s, v29.4s, v16.4s\n"
            "fmin v28.4s, v28.4s, v16.4s\n"
            "blt 21f\n"
            "mov x20, %x[dst]\n"
            "cmp x12, #0x1\n"
            "str q31, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "ble 24f\n"
            "cmp x12, #0x2\n"
            "str q30, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "ble 24f\n"
            "cmp x12, #0x3\n"
            "str q29, [x20, #0x0]\n"
            "add x20, x20, %x[dst_stride_row]\n"
            "ble 24f\n"
            "str q28, [x20, #0x0]\n"
            "b 24f\n"
            "21:"  // Row tail: Partial output
            "mov x23, %x[dst]\n"
            "cmp x12, #0x1\n"
            "add x22, x23, %x[dst_stride_row]\n"
            "csel x22, x22, x23, GT\n"
            "cmp x12, #0x2\n"
            "add x21, x23, %x[dst_stride_row], LSL #1\n"
            "csel x21, x21, x22, GT\n"
            "cmp x12, #0x3\n"
            "add x20, x21, %x[dst_stride_row]\n"
            "csel x20, x20, x21, GT\n"
            "tbz x25, #1, 22f\n"
            "st1 { v28.d }[0], [x20], #0x8\n"
            "st1 { v29.d }[0], [x21], #0x8\n"
            "st1 { v30.d }[0], [x22], #0x8\n"
            "st1 { v31.d }[0], [x23], #0x8\n"
            "tbz x25, #0, 23f\n"
            "st1 { v28.s }[2], [x20]\n"
            "st1 { v29.s }[2], [x21]\n"
            "st1 { v30.s }[2], [x22]\n"
            "st1 { v31.s }[2], [x23]\n"
            "b 23f\n"
            "22:"  // Row tail: Output block 0: partial_1_0
            "st1 { v28.s }[0], [x20]\n"
            "st1 { v29.s }[0], [x21]\n"
            "st1 { v30.s }[0], [x22]\n"
            "st1 { v31.s }[0], [x23]\n"
            "23:"  // Row tail: Output block 0: Done
            "24:"  // Row tail: Output stage exit
            "subs x25, x25, #0x4\n"
            "add %x[dst], %x[dst], #0x10\n"
            "bgt 17b\n"
            "subs x12, x12, #0x4\n"
            "add %x[lhs_packed], %x[lhs_packed], x13\n"
            "mov %x[dst], x24\n"
            "bgt 16b\n"
            "25:"  // Row tail: Row loop skip
            "add SP, SP, #0x100\n"
            : [dst] "+&r"(dst), [lhs_packed] "+&r"(lhs_packed)
            : [clamp_vals] "r"(clamp_vals), [dst_stride_row] "r"(dst_stride_row), [m] "r"(m), [n] "r"(n),
              [num_blocks] "r"(num_blocks), [num_subblocks] "r"(num_subblocks), [rhs_packed] "r"(rhs_packed)
            : "cc", "memory", "v0", "v1", "v10", "v11", "v12", "v13", "v14", "v15", "v16", "v17", "v18", "v19", "v2",
              "v20", "v21", "v22", "v23", "v24", "v25", "v26", "v27", "v28", "v29", "v3", "v30", "v31", "v4", "v5",
              "v6", "v7", "v8", "v9", "x10", "x11", "x12", "x13", "x20", "x21", "x22", "x23", "x24", "x25", "x26",
              "x27", "x28", "x9");
    }
}
#endif  // Architectural feature check
